{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:03:03.649459600Z",
     "start_time": "2023-12-07T08:03:03.620892100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nGroup members:\\nBohao XU        ETU20211498\\nJijun TAN       ETU20211472\\nYilin Zhang     ETU20211520\\nJunxin Huang    ETU20211420\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Group members:\n",
    "Bohao XU        ETU20211498\n",
    "Jijun TAN       ETU20211472\n",
    "Yilin Zhang     ETU20211520\n",
    "Junxin Huang    ETU20211420\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nREADME!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\\n1. please download the dataset from:\\nhttps://www.kaggle.com/competitions/g-research-crypto-forecasting/data\\n\\n2. please make sure all the files downloaded are placed in the same folder as all the files I submitted to you.\\n\\n3. then you can run the Model_Training.ipynb and Model_Inference.ipynb\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "README!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "1. please download the dataset from:\n",
    "https://www.kaggle.com/competitions/g-research-crypto-forecasting/data\n",
    "\n",
    "2. please make sure all the files downloaded are placed in the same folder as all the files I submitted to you.\n",
    "\n",
    "3. then you can run the Model_Training.ipynb and Model_Inference.ipynb\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:03:03.693261900Z",
     "start_time": "2023-12-07T08:03:03.639450500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "8f8fbc9e-d6c0-48f2-83cd-05c7e26499b3",
    "_uuid": "87495b9f-8db8-4e2e-8cd4-3a71e006318a",
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:20.640745Z",
     "iopub.status.busy": "2023-12-05T15:54:20.640208Z",
     "iopub.status.idle": "2023-12-05T15:54:25.549533Z",
     "shell.execute_reply": "2023-12-05T15:54:25.547827Z",
     "shell.execute_reply.started": "2023-12-05T15:54:20.640705Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:03:05.081816600Z",
     "start_time": "2023-12-07T08:03:03.663260800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gc\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import warnings\n",
    "\n",
    "seed0 = 8586\n",
    "\n",
    "creator='group4'\n",
    "TRAIN_CSV = 'train.csv'\n",
    "SUPPLE_TRAIN_CSV = 'supplemental_train.csv'\n",
    "ASSET_DETAILS_CSV = 'asset_details.csv' \n",
    "ASSET_ID_SELECTED_FOR_INFERENCE = [i for i in range(14)]\n",
    "FEATURES_EXCLUDE = []\n",
    "lags = [60,300,900]\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data_Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_main = pd.read_csv('train.csv', usecols=['timestamp','Asset_ID', 'Close', 'Target','Volume'])\n",
    "df_supple = pd.read_csv('supplemental_train.csv', usecols=['timestamp','Asset_ID', 'Close', 'Target','Volume'])\n",
    "# Merge training data from 2 tables\n",
    "df_train = pd.concat([df_main, df_supple])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:03:44.555537800Z",
     "start_time": "2023-12-07T08:03:05.061815600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:33<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct all training data with timestamps as unique columns, ensuring that each row is only 1 timestamp and contains 14 ids of close,target,volume data\n",
    "train_merged = pd.DataFrame()\n",
    "train_merged[df_train.columns] = 0\n",
    "for id in tqdm( range(14) ):\n",
    "    train_merged = train_merged.merge(df_train.loc[df_train[\"Asset_ID\"] == id, ['timestamp', 'Close','Target','Volume']].copy(), on=\"timestamp\", how='outer',suffixes=['', \"_\"+str(id)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:17.813220600Z",
     "start_time": "2023-12-07T08:03:44.555537800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "          timestamp     Close_0  Target_0     Volume_0       Close_1  \\\n0        1514764860    8.530000 -0.014399    78.380000  13850.176000   \n1        1514764920    8.514500 -0.015875    71.390000  13828.102000   \n2        1514764980    8.484800 -0.015410  1546.820000  13801.314000   \n...             ...         ...       ...          ...           ...   \n2122476  1642982280  384.201667       NaN   496.467537  36276.223333   \n2122477  1642982340  384.002000       NaN   368.684921  36276.623333   \n2122478  1642982400  382.930000       NaN  1490.571077  36221.987143   \n\n         Target_1    Volume_1      Close_2  Target_2    Volume_2   Close_3  \\\n0       -0.014643   31.550062  2374.590000 -0.004218   19.233005       NaN   \n1       -0.015037   31.046432  2372.286667 -0.004079   24.050259       NaN   \n2       -0.010309   55.061820  2372.063333 -0.002892   42.676438       NaN   \n...           ...         ...          ...       ...         ...       ...   \n2122476       NaN   49.867700   302.430000       NaN  198.196485  1.123465   \n2122477       NaN   43.030556   302.562500       NaN   76.954035  1.124831   \n2122478       NaN  110.053151   302.112000       NaN  209.645675  1.121151   \n\n         Target_3       Volume_3   Close_4  Target_4      Volume_4   Close_5  \\\n0             NaN            NaN       NaN       NaN           NaN  7.657600   \n1             NaN            NaN       NaN       NaN           NaN  7.656700   \n2             NaN            NaN       NaN       NaN           NaN  7.651200   \n...           ...            ...       ...       ...           ...       ...   \n2122476       NaN  430719.654610  0.141800       NaN  7.433686e+05  2.272983   \n2122477       NaN  352929.794282  0.141800       NaN  7.287195e+05  2.275100   \n2122478       NaN  249923.229256  0.141391       NaN  1.009688e+06  2.264800   \n\n         Target_5       Volume_5      Close_6  Target_6     Volume_6  \\\n0       -0.013922    6626.713370   738.507500 -0.004809   335.987856   \n1       -0.014534    3277.475494   738.260000 -0.004441   232.793141   \n2       -0.012546    5623.557585   737.502500 -0.004206   174.138031   \n...           ...            ...          ...       ...          ...   \n2122476       NaN   14907.125055  2541.388571       NaN   821.614133   \n2122477       NaN   14559.759824  2541.040000       NaN   749.548301   \n2122478       NaN  202194.015531  2536.151429       NaN  1057.765667   \n\n           Close_7  Target_7     Volume_7   Close_8  Target_8      Volume_8  \\\n0        25.877000 -0.008264   121.087310       NaN       NaN           NaN   \n1        25.897000 -0.029902     1.468019       NaN       NaN           NaN   \n2        26.469500 -0.030832    76.163922       NaN       NaN           NaN   \n...            ...       ...          ...       ...       ...           ...   \n2122476  25.152780       NaN  1334.601131  0.814233       NaN  20511.345658   \n2122477  25.135800       NaN  2410.205214  0.815833       NaN   9064.791667   \n2122478  25.028067       NaN  6709.816310  0.812400       NaN   6019.269834   \n\n            Close_9  Target_9     Volume_9     Close_10  Target_10  Volume_10  \\\n0        225.206667 -0.009791   411.896642          NaN        NaN        NaN   \n1        224.856667 -0.012991  3640.502706          NaN        NaN        NaN   \n2        226.000000 -0.003572   328.350286          NaN        NaN        NaN   \n...             ...       ...          ...          ...        ...        ...   \n2122476  112.258000       NaN   790.632440  1831.573333        NaN   2.142748   \n2122477  112.342400       NaN   331.208442  1833.186000        NaN   3.985169   \n2122478  111.983333       NaN  1243.860266  1833.018333        NaN   4.190290   \n\n         Close_11  Target_11  Volume_11  Close_12  Target_12     Volume_12  \\\n0        329.4600        NaN   6.635710       NaN        NaN           NaN   \n1        329.4500  -0.009690   0.349420       NaN        NaN           NaN   \n2        329.1900   0.006567   1.189553       NaN        NaN           NaN   \n...           ...        ...        ...       ...        ...           ...   \n2122476  156.5125        NaN  21.597886  0.200803        NaN  7.076212e+05   \n2122477  156.8940        NaN  51.115980  0.200968        NaN  1.988516e+05   \n2122478  156.5500        NaN  74.397471  0.200093        NaN  2.983796e+06   \n\n         Close_13  Target_13     Volume_13  \n0             NaN        NaN           NaN  \n1             NaN        NaN           NaN  \n2             NaN        NaN           NaN  \n...           ...        ...           ...  \n2122476  0.057509        NaN  1.301079e+06  \n2122477  0.057521        NaN  9.245588e+05  \n2122478  0.057478        NaN  8.988253e+05  \n\n[2136778 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>Close_0</th>\n      <th>Target_0</th>\n      <th>Volume_0</th>\n      <th>Close_1</th>\n      <th>Target_1</th>\n      <th>Volume_1</th>\n      <th>Close_2</th>\n      <th>Target_2</th>\n      <th>Volume_2</th>\n      <th>Close_3</th>\n      <th>Target_3</th>\n      <th>Volume_3</th>\n      <th>Close_4</th>\n      <th>Target_4</th>\n      <th>Volume_4</th>\n      <th>Close_5</th>\n      <th>Target_5</th>\n      <th>Volume_5</th>\n      <th>Close_6</th>\n      <th>Target_6</th>\n      <th>Volume_6</th>\n      <th>Close_7</th>\n      <th>Target_7</th>\n      <th>Volume_7</th>\n      <th>Close_8</th>\n      <th>Target_8</th>\n      <th>Volume_8</th>\n      <th>Close_9</th>\n      <th>Target_9</th>\n      <th>Volume_9</th>\n      <th>Close_10</th>\n      <th>Target_10</th>\n      <th>Volume_10</th>\n      <th>Close_11</th>\n      <th>Target_11</th>\n      <th>Volume_11</th>\n      <th>Close_12</th>\n      <th>Target_12</th>\n      <th>Volume_12</th>\n      <th>Close_13</th>\n      <th>Target_13</th>\n      <th>Volume_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1514764860</td>\n      <td>8.530000</td>\n      <td>-0.014399</td>\n      <td>78.380000</td>\n      <td>13850.176000</td>\n      <td>-0.014643</td>\n      <td>31.550062</td>\n      <td>2374.590000</td>\n      <td>-0.004218</td>\n      <td>19.233005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.657600</td>\n      <td>-0.013922</td>\n      <td>6626.713370</td>\n      <td>738.507500</td>\n      <td>-0.004809</td>\n      <td>335.987856</td>\n      <td>25.877000</td>\n      <td>-0.008264</td>\n      <td>121.087310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>225.206667</td>\n      <td>-0.009791</td>\n      <td>411.896642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4600</td>\n      <td>NaN</td>\n      <td>6.635710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1514764920</td>\n      <td>8.514500</td>\n      <td>-0.015875</td>\n      <td>71.390000</td>\n      <td>13828.102000</td>\n      <td>-0.015037</td>\n      <td>31.046432</td>\n      <td>2372.286667</td>\n      <td>-0.004079</td>\n      <td>24.050259</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.656700</td>\n      <td>-0.014534</td>\n      <td>3277.475494</td>\n      <td>738.260000</td>\n      <td>-0.004441</td>\n      <td>232.793141</td>\n      <td>25.897000</td>\n      <td>-0.029902</td>\n      <td>1.468019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>224.856667</td>\n      <td>-0.012991</td>\n      <td>3640.502706</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4500</td>\n      <td>-0.009690</td>\n      <td>0.349420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1514764980</td>\n      <td>8.484800</td>\n      <td>-0.015410</td>\n      <td>1546.820000</td>\n      <td>13801.314000</td>\n      <td>-0.010309</td>\n      <td>55.061820</td>\n      <td>2372.063333</td>\n      <td>-0.002892</td>\n      <td>42.676438</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.651200</td>\n      <td>-0.012546</td>\n      <td>5623.557585</td>\n      <td>737.502500</td>\n      <td>-0.004206</td>\n      <td>174.138031</td>\n      <td>26.469500</td>\n      <td>-0.030832</td>\n      <td>76.163922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000000</td>\n      <td>-0.003572</td>\n      <td>328.350286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.1900</td>\n      <td>0.006567</td>\n      <td>1.189553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2122476</th>\n      <td>1642982280</td>\n      <td>384.201667</td>\n      <td>NaN</td>\n      <td>496.467537</td>\n      <td>36276.223333</td>\n      <td>NaN</td>\n      <td>49.867700</td>\n      <td>302.430000</td>\n      <td>NaN</td>\n      <td>198.196485</td>\n      <td>1.123465</td>\n      <td>NaN</td>\n      <td>430719.654610</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.433686e+05</td>\n      <td>2.272983</td>\n      <td>NaN</td>\n      <td>14907.125055</td>\n      <td>2541.388571</td>\n      <td>NaN</td>\n      <td>821.614133</td>\n      <td>25.152780</td>\n      <td>NaN</td>\n      <td>1334.601131</td>\n      <td>0.814233</td>\n      <td>NaN</td>\n      <td>20511.345658</td>\n      <td>112.258000</td>\n      <td>NaN</td>\n      <td>790.632440</td>\n      <td>1831.573333</td>\n      <td>NaN</td>\n      <td>2.142748</td>\n      <td>156.5125</td>\n      <td>NaN</td>\n      <td>21.597886</td>\n      <td>0.200803</td>\n      <td>NaN</td>\n      <td>7.076212e+05</td>\n      <td>0.057509</td>\n      <td>NaN</td>\n      <td>1.301079e+06</td>\n    </tr>\n    <tr>\n      <th>2122477</th>\n      <td>1642982340</td>\n      <td>384.002000</td>\n      <td>NaN</td>\n      <td>368.684921</td>\n      <td>36276.623333</td>\n      <td>NaN</td>\n      <td>43.030556</td>\n      <td>302.562500</td>\n      <td>NaN</td>\n      <td>76.954035</td>\n      <td>1.124831</td>\n      <td>NaN</td>\n      <td>352929.794282</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.287195e+05</td>\n      <td>2.275100</td>\n      <td>NaN</td>\n      <td>14559.759824</td>\n      <td>2541.040000</td>\n      <td>NaN</td>\n      <td>749.548301</td>\n      <td>25.135800</td>\n      <td>NaN</td>\n      <td>2410.205214</td>\n      <td>0.815833</td>\n      <td>NaN</td>\n      <td>9064.791667</td>\n      <td>112.342400</td>\n      <td>NaN</td>\n      <td>331.208442</td>\n      <td>1833.186000</td>\n      <td>NaN</td>\n      <td>3.985169</td>\n      <td>156.8940</td>\n      <td>NaN</td>\n      <td>51.115980</td>\n      <td>0.200968</td>\n      <td>NaN</td>\n      <td>1.988516e+05</td>\n      <td>0.057521</td>\n      <td>NaN</td>\n      <td>9.245588e+05</td>\n    </tr>\n    <tr>\n      <th>2122478</th>\n      <td>1642982400</td>\n      <td>382.930000</td>\n      <td>NaN</td>\n      <td>1490.571077</td>\n      <td>36221.987143</td>\n      <td>NaN</td>\n      <td>110.053151</td>\n      <td>302.112000</td>\n      <td>NaN</td>\n      <td>209.645675</td>\n      <td>1.121151</td>\n      <td>NaN</td>\n      <td>249923.229256</td>\n      <td>0.141391</td>\n      <td>NaN</td>\n      <td>1.009688e+06</td>\n      <td>2.264800</td>\n      <td>NaN</td>\n      <td>202194.015531</td>\n      <td>2536.151429</td>\n      <td>NaN</td>\n      <td>1057.765667</td>\n      <td>25.028067</td>\n      <td>NaN</td>\n      <td>6709.816310</td>\n      <td>0.812400</td>\n      <td>NaN</td>\n      <td>6019.269834</td>\n      <td>111.983333</td>\n      <td>NaN</td>\n      <td>1243.860266</td>\n      <td>1833.018333</td>\n      <td>NaN</td>\n      <td>4.190290</td>\n      <td>156.5500</td>\n      <td>NaN</td>\n      <td>74.397471</td>\n      <td>0.200093</td>\n      <td>NaN</td>\n      <td>2.983796e+06</td>\n      <td>0.057478</td>\n      <td>NaN</td>\n      <td>8.988253e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>2136778 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete columns from the original df before refactoring\n",
    "train_merged = train_merged.drop(df_train.columns.drop(\"timestamp\"), axis=1)\n",
    "train_merged = train_merged.sort_values('timestamp', ascending=True)\n",
    "train_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:18.747036200Z",
     "start_time": "2023-12-07T08:04:17.813220600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "               Close_0  Target_0     Volume_0       Close_1  Target_1  \\\ntimestamp                                                               \n1514764860    8.530000 -0.014399    78.380000  13850.176000 -0.014643   \n1514764920    8.514500 -0.015875    71.390000  13828.102000 -0.015037   \n1514764980    8.484800 -0.015410  1546.820000  13801.314000 -0.010309   \n...                ...       ...          ...           ...       ...   \n1642982280  384.201667       NaN   496.467537  36276.223333       NaN   \n1642982340  384.002000       NaN   368.684921  36276.623333       NaN   \n1642982400  382.930000       NaN  1490.571077  36221.987143       NaN   \n\n              Volume_1      Close_2  Target_2    Volume_2   Close_3  Target_3  \\\ntimestamp                                                                       \n1514764860   31.550062  2374.590000 -0.004218   19.233005       NaN       NaN   \n1514764920   31.046432  2372.286667 -0.004079   24.050259       NaN       NaN   \n1514764980   55.061820  2372.063333 -0.002892   42.676438       NaN       NaN   \n...                ...          ...       ...         ...       ...       ...   \n1642982280   49.867700   302.430000       NaN  198.196485  1.123465       NaN   \n1642982340   43.030556   302.562500       NaN   76.954035  1.124831       NaN   \n1642982400  110.053151   302.112000       NaN  209.645675  1.121151       NaN   \n\n                 Volume_3   Close_4  Target_4      Volume_4   Close_5  \\\ntimestamp                                                               \n1514764860            NaN       NaN       NaN           NaN  7.657600   \n1514764920            NaN       NaN       NaN           NaN  7.656700   \n1514764980            NaN       NaN       NaN           NaN  7.651200   \n...                   ...       ...       ...           ...       ...   \n1642982280  430719.654610  0.141800       NaN  7.433686e+05  2.272983   \n1642982340  352929.794282  0.141800       NaN  7.287195e+05  2.275100   \n1642982400  249923.229256  0.141391       NaN  1.009688e+06  2.264800   \n\n            Target_5       Volume_5      Close_6  Target_6     Volume_6  \\\ntimestamp                                                                 \n1514764860 -0.013922    6626.713370   738.507500 -0.004809   335.987856   \n1514764920 -0.014534    3277.475494   738.260000 -0.004441   232.793141   \n1514764980 -0.012546    5623.557585   737.502500 -0.004206   174.138031   \n...              ...            ...          ...       ...          ...   \n1642982280       NaN   14907.125055  2541.388571       NaN   821.614133   \n1642982340       NaN   14559.759824  2541.040000       NaN   749.548301   \n1642982400       NaN  202194.015531  2536.151429       NaN  1057.765667   \n\n              Close_7  Target_7     Volume_7   Close_8  Target_8  \\\ntimestamp                                                          \n1514764860  25.877000 -0.008264   121.087310       NaN       NaN   \n1514764920  25.897000 -0.029902     1.468019       NaN       NaN   \n1514764980  26.469500 -0.030832    76.163922       NaN       NaN   \n...               ...       ...          ...       ...       ...   \n1642982280  25.152780       NaN  1334.601131  0.814233       NaN   \n1642982340  25.135800       NaN  2410.205214  0.815833       NaN   \n1642982400  25.028067       NaN  6709.816310  0.812400       NaN   \n\n                Volume_8     Close_9  Target_9     Volume_9     Close_10  \\\ntimestamp                                                                  \n1514764860           NaN  225.206667 -0.009791   411.896642          NaN   \n1514764920           NaN  224.856667 -0.012991  3640.502706          NaN   \n1514764980           NaN  226.000000 -0.003572   328.350286          NaN   \n...                  ...         ...       ...          ...          ...   \n1642982280  20511.345658  112.258000       NaN   790.632440  1831.573333   \n1642982340   9064.791667  112.342400       NaN   331.208442  1833.186000   \n1642982400   6019.269834  111.983333       NaN  1243.860266  1833.018333   \n\n            Target_10  Volume_10  Close_11  Target_11  Volume_11  Close_12  \\\ntimestamp                                                                    \n1514764860        NaN        NaN  329.4600        NaN   6.635710       NaN   \n1514764920        NaN        NaN  329.4500  -0.009690   0.349420       NaN   \n1514764980        NaN        NaN  329.1900   0.006567   1.189553       NaN   \n...               ...        ...       ...        ...        ...       ...   \n1642982280        NaN   2.142748  156.5125        NaN  21.597886  0.200803   \n1642982340        NaN   3.985169  156.8940        NaN  51.115980  0.200968   \n1642982400        NaN   4.190290  156.5500        NaN  74.397471  0.200093   \n\n            Target_12     Volume_12  Close_13  Target_13     Volume_13  \ntimestamp                                                               \n1514764860        NaN           NaN       NaN        NaN           NaN  \n1514764920        NaN           NaN       NaN        NaN           NaN  \n1514764980        NaN           NaN       NaN        NaN           NaN  \n...               ...           ...       ...        ...           ...  \n1642982280        NaN  7.076212e+05  0.057509        NaN  1.301079e+06  \n1642982340        NaN  1.988516e+05  0.057521        NaN  9.245588e+05  \n1642982400        NaN  2.983796e+06  0.057478        NaN  8.988253e+05  \n\n[2136778 rows x 42 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Close_0</th>\n      <th>Target_0</th>\n      <th>Volume_0</th>\n      <th>Close_1</th>\n      <th>Target_1</th>\n      <th>Volume_1</th>\n      <th>Close_2</th>\n      <th>Target_2</th>\n      <th>Volume_2</th>\n      <th>Close_3</th>\n      <th>Target_3</th>\n      <th>Volume_3</th>\n      <th>Close_4</th>\n      <th>Target_4</th>\n      <th>Volume_4</th>\n      <th>Close_5</th>\n      <th>Target_5</th>\n      <th>Volume_5</th>\n      <th>Close_6</th>\n      <th>Target_6</th>\n      <th>Volume_6</th>\n      <th>Close_7</th>\n      <th>Target_7</th>\n      <th>Volume_7</th>\n      <th>Close_8</th>\n      <th>Target_8</th>\n      <th>Volume_8</th>\n      <th>Close_9</th>\n      <th>Target_9</th>\n      <th>Volume_9</th>\n      <th>Close_10</th>\n      <th>Target_10</th>\n      <th>Volume_10</th>\n      <th>Close_11</th>\n      <th>Target_11</th>\n      <th>Volume_11</th>\n      <th>Close_12</th>\n      <th>Target_12</th>\n      <th>Volume_12</th>\n      <th>Close_13</th>\n      <th>Target_13</th>\n      <th>Volume_13</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1514764860</th>\n      <td>8.530000</td>\n      <td>-0.014399</td>\n      <td>78.380000</td>\n      <td>13850.176000</td>\n      <td>-0.014643</td>\n      <td>31.550062</td>\n      <td>2374.590000</td>\n      <td>-0.004218</td>\n      <td>19.233005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.657600</td>\n      <td>-0.013922</td>\n      <td>6626.713370</td>\n      <td>738.507500</td>\n      <td>-0.004809</td>\n      <td>335.987856</td>\n      <td>25.877000</td>\n      <td>-0.008264</td>\n      <td>121.087310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>225.206667</td>\n      <td>-0.009791</td>\n      <td>411.896642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4600</td>\n      <td>NaN</td>\n      <td>6.635710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1514764920</th>\n      <td>8.514500</td>\n      <td>-0.015875</td>\n      <td>71.390000</td>\n      <td>13828.102000</td>\n      <td>-0.015037</td>\n      <td>31.046432</td>\n      <td>2372.286667</td>\n      <td>-0.004079</td>\n      <td>24.050259</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.656700</td>\n      <td>-0.014534</td>\n      <td>3277.475494</td>\n      <td>738.260000</td>\n      <td>-0.004441</td>\n      <td>232.793141</td>\n      <td>25.897000</td>\n      <td>-0.029902</td>\n      <td>1.468019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>224.856667</td>\n      <td>-0.012991</td>\n      <td>3640.502706</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4500</td>\n      <td>-0.009690</td>\n      <td>0.349420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1514764980</th>\n      <td>8.484800</td>\n      <td>-0.015410</td>\n      <td>1546.820000</td>\n      <td>13801.314000</td>\n      <td>-0.010309</td>\n      <td>55.061820</td>\n      <td>2372.063333</td>\n      <td>-0.002892</td>\n      <td>42.676438</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.651200</td>\n      <td>-0.012546</td>\n      <td>5623.557585</td>\n      <td>737.502500</td>\n      <td>-0.004206</td>\n      <td>174.138031</td>\n      <td>26.469500</td>\n      <td>-0.030832</td>\n      <td>76.163922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000000</td>\n      <td>-0.003572</td>\n      <td>328.350286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.1900</td>\n      <td>0.006567</td>\n      <td>1.189553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1642982280</th>\n      <td>384.201667</td>\n      <td>NaN</td>\n      <td>496.467537</td>\n      <td>36276.223333</td>\n      <td>NaN</td>\n      <td>49.867700</td>\n      <td>302.430000</td>\n      <td>NaN</td>\n      <td>198.196485</td>\n      <td>1.123465</td>\n      <td>NaN</td>\n      <td>430719.654610</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.433686e+05</td>\n      <td>2.272983</td>\n      <td>NaN</td>\n      <td>14907.125055</td>\n      <td>2541.388571</td>\n      <td>NaN</td>\n      <td>821.614133</td>\n      <td>25.152780</td>\n      <td>NaN</td>\n      <td>1334.601131</td>\n      <td>0.814233</td>\n      <td>NaN</td>\n      <td>20511.345658</td>\n      <td>112.258000</td>\n      <td>NaN</td>\n      <td>790.632440</td>\n      <td>1831.573333</td>\n      <td>NaN</td>\n      <td>2.142748</td>\n      <td>156.5125</td>\n      <td>NaN</td>\n      <td>21.597886</td>\n      <td>0.200803</td>\n      <td>NaN</td>\n      <td>7.076212e+05</td>\n      <td>0.057509</td>\n      <td>NaN</td>\n      <td>1.301079e+06</td>\n    </tr>\n    <tr>\n      <th>1642982340</th>\n      <td>384.002000</td>\n      <td>NaN</td>\n      <td>368.684921</td>\n      <td>36276.623333</td>\n      <td>NaN</td>\n      <td>43.030556</td>\n      <td>302.562500</td>\n      <td>NaN</td>\n      <td>76.954035</td>\n      <td>1.124831</td>\n      <td>NaN</td>\n      <td>352929.794282</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.287195e+05</td>\n      <td>2.275100</td>\n      <td>NaN</td>\n      <td>14559.759824</td>\n      <td>2541.040000</td>\n      <td>NaN</td>\n      <td>749.548301</td>\n      <td>25.135800</td>\n      <td>NaN</td>\n      <td>2410.205214</td>\n      <td>0.815833</td>\n      <td>NaN</td>\n      <td>9064.791667</td>\n      <td>112.342400</td>\n      <td>NaN</td>\n      <td>331.208442</td>\n      <td>1833.186000</td>\n      <td>NaN</td>\n      <td>3.985169</td>\n      <td>156.8940</td>\n      <td>NaN</td>\n      <td>51.115980</td>\n      <td>0.200968</td>\n      <td>NaN</td>\n      <td>1.988516e+05</td>\n      <td>0.057521</td>\n      <td>NaN</td>\n      <td>9.245588e+05</td>\n    </tr>\n    <tr>\n      <th>1642982400</th>\n      <td>382.930000</td>\n      <td>NaN</td>\n      <td>1490.571077</td>\n      <td>36221.987143</td>\n      <td>NaN</td>\n      <td>110.053151</td>\n      <td>302.112000</td>\n      <td>NaN</td>\n      <td>209.645675</td>\n      <td>1.121151</td>\n      <td>NaN</td>\n      <td>249923.229256</td>\n      <td>0.141391</td>\n      <td>NaN</td>\n      <td>1.009688e+06</td>\n      <td>2.264800</td>\n      <td>NaN</td>\n      <td>202194.015531</td>\n      <td>2536.151429</td>\n      <td>NaN</td>\n      <td>1057.765667</td>\n      <td>25.028067</td>\n      <td>NaN</td>\n      <td>6709.816310</td>\n      <td>0.812400</td>\n      <td>NaN</td>\n      <td>6019.269834</td>\n      <td>111.983333</td>\n      <td>NaN</td>\n      <td>1243.860266</td>\n      <td>1833.018333</td>\n      <td>NaN</td>\n      <td>4.190290</td>\n      <td>156.5500</td>\n      <td>NaN</td>\n      <td>74.397471</td>\n      <td>0.200093</td>\n      <td>NaN</td>\n      <td>2.983796e+06</td>\n      <td>0.057478</td>\n      <td>NaN</td>\n      <td>8.988253e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>2136778 rows × 42 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged = train_merged.set_index(\"timestamp\")\n",
    "train_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:19.499726100Z",
     "start_time": "2023-12-07T08:04:18.747036200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "timestamp\n60     2136666\n120         80\n180         10\n        ...   \n360          5\n300          5\n480          1\nName: count, Length: 7, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The time interval between rows before patching, you can see that there are missing values\n",
    "(train_merged.index[1:]-train_merged.index[:-1]).value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:19.594477700Z",
     "start_time": "2023-12-07T08:04:19.499726100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "timestamp\n60    2136959\nName: count, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in missing rows - padding method\n",
    "train_merged = train_merged.reindex(range(train_merged.index[0],train_merged.index[-1]+60,60),method='pad')\n",
    "# after replenishment\n",
    "(train_merged.index[1:]-train_merged.index[:-1]).value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:20.420536100Z",
     "start_time": "2023-12-07T08:04:19.554470800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "          timestamp     Close_0  Target_0     Volume_0       Close_1  \\\n0        1514764860    8.530000 -0.014399    78.380000  13850.176000   \n1        1514764920    8.514500 -0.015875    71.390000  13828.102000   \n2        1514764980    8.484800 -0.015410  1546.820000  13801.314000   \n...             ...         ...       ...          ...           ...   \n2136957  1642982280  384.201667       NaN   496.467537  36276.223333   \n2136958  1642982340  384.002000       NaN   368.684921  36276.623333   \n2136959  1642982400  382.930000       NaN  1490.571077  36221.987143   \n\n         Target_1    Volume_1      Close_2  Target_2    Volume_2   Close_3  \\\n0       -0.014643   31.550062  2374.590000 -0.004218   19.233005       NaN   \n1       -0.015037   31.046432  2372.286667 -0.004079   24.050259       NaN   \n2       -0.010309   55.061820  2372.063333 -0.002892   42.676438       NaN   \n...           ...         ...          ...       ...         ...       ...   \n2136957       NaN   49.867700   302.430000       NaN  198.196485  1.123465   \n2136958       NaN   43.030556   302.562500       NaN   76.954035  1.124831   \n2136959       NaN  110.053151   302.112000       NaN  209.645675  1.121151   \n\n         Target_3       Volume_3   Close_4  Target_4      Volume_4   Close_5  \\\n0             NaN            NaN       NaN       NaN           NaN  7.657600   \n1             NaN            NaN       NaN       NaN           NaN  7.656700   \n2             NaN            NaN       NaN       NaN           NaN  7.651200   \n...           ...            ...       ...       ...           ...       ...   \n2136957       NaN  430719.654610  0.141800       NaN  7.433686e+05  2.272983   \n2136958       NaN  352929.794282  0.141800       NaN  7.287195e+05  2.275100   \n2136959       NaN  249923.229256  0.141391       NaN  1.009688e+06  2.264800   \n\n         Target_5       Volume_5      Close_6  Target_6     Volume_6  \\\n0       -0.013922    6626.713370   738.507500 -0.004809   335.987856   \n1       -0.014534    3277.475494   738.260000 -0.004441   232.793141   \n2       -0.012546    5623.557585   737.502500 -0.004206   174.138031   \n...           ...            ...          ...       ...          ...   \n2136957       NaN   14907.125055  2541.388571       NaN   821.614133   \n2136958       NaN   14559.759824  2541.040000       NaN   749.548301   \n2136959       NaN  202194.015531  2536.151429       NaN  1057.765667   \n\n           Close_7  Target_7     Volume_7   Close_8  Target_8      Volume_8  \\\n0        25.877000 -0.008264   121.087310       NaN       NaN           NaN   \n1        25.897000 -0.029902     1.468019       NaN       NaN           NaN   \n2        26.469500 -0.030832    76.163922       NaN       NaN           NaN   \n...            ...       ...          ...       ...       ...           ...   \n2136957  25.152780       NaN  1334.601131  0.814233       NaN  20511.345658   \n2136958  25.135800       NaN  2410.205214  0.815833       NaN   9064.791667   \n2136959  25.028067       NaN  6709.816310  0.812400       NaN   6019.269834   \n\n            Close_9  Target_9     Volume_9     Close_10  Target_10  Volume_10  \\\n0        225.206667 -0.009791   411.896642          NaN        NaN        NaN   \n1        224.856667 -0.012991  3640.502706          NaN        NaN        NaN   \n2        226.000000 -0.003572   328.350286          NaN        NaN        NaN   \n...             ...       ...          ...          ...        ...        ...   \n2136957  112.258000       NaN   790.632440  1831.573333        NaN   2.142748   \n2136958  112.342400       NaN   331.208442  1833.186000        NaN   3.985169   \n2136959  111.983333       NaN  1243.860266  1833.018333        NaN   4.190290   \n\n         Close_11  Target_11  Volume_11  Close_12  Target_12     Volume_12  \\\n0        329.4600        NaN   6.635710       NaN        NaN           NaN   \n1        329.4500  -0.009690   0.349420       NaN        NaN           NaN   \n2        329.1900   0.006567   1.189553       NaN        NaN           NaN   \n...           ...        ...        ...       ...        ...           ...   \n2136957  156.5125        NaN  21.597886  0.200803        NaN  7.076212e+05   \n2136958  156.8940        NaN  51.115980  0.200968        NaN  1.988516e+05   \n2136959  156.5500        NaN  74.397471  0.200093        NaN  2.983796e+06   \n\n         Close_13  Target_13     Volume_13  \n0             NaN        NaN           NaN  \n1             NaN        NaN           NaN  \n2             NaN        NaN           NaN  \n...           ...        ...           ...  \n2136957  0.057509        NaN  1.301079e+06  \n2136958  0.057521        NaN  9.245588e+05  \n2136959  0.057478        NaN  8.988253e+05  \n\n[2136960 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>Close_0</th>\n      <th>Target_0</th>\n      <th>Volume_0</th>\n      <th>Close_1</th>\n      <th>Target_1</th>\n      <th>Volume_1</th>\n      <th>Close_2</th>\n      <th>Target_2</th>\n      <th>Volume_2</th>\n      <th>Close_3</th>\n      <th>Target_3</th>\n      <th>Volume_3</th>\n      <th>Close_4</th>\n      <th>Target_4</th>\n      <th>Volume_4</th>\n      <th>Close_5</th>\n      <th>Target_5</th>\n      <th>Volume_5</th>\n      <th>Close_6</th>\n      <th>Target_6</th>\n      <th>Volume_6</th>\n      <th>Close_7</th>\n      <th>Target_7</th>\n      <th>Volume_7</th>\n      <th>Close_8</th>\n      <th>Target_8</th>\n      <th>Volume_8</th>\n      <th>Close_9</th>\n      <th>Target_9</th>\n      <th>Volume_9</th>\n      <th>Close_10</th>\n      <th>Target_10</th>\n      <th>Volume_10</th>\n      <th>Close_11</th>\n      <th>Target_11</th>\n      <th>Volume_11</th>\n      <th>Close_12</th>\n      <th>Target_12</th>\n      <th>Volume_12</th>\n      <th>Close_13</th>\n      <th>Target_13</th>\n      <th>Volume_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1514764860</td>\n      <td>8.530000</td>\n      <td>-0.014399</td>\n      <td>78.380000</td>\n      <td>13850.176000</td>\n      <td>-0.014643</td>\n      <td>31.550062</td>\n      <td>2374.590000</td>\n      <td>-0.004218</td>\n      <td>19.233005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.657600</td>\n      <td>-0.013922</td>\n      <td>6626.713370</td>\n      <td>738.507500</td>\n      <td>-0.004809</td>\n      <td>335.987856</td>\n      <td>25.877000</td>\n      <td>-0.008264</td>\n      <td>121.087310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>225.206667</td>\n      <td>-0.009791</td>\n      <td>411.896642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4600</td>\n      <td>NaN</td>\n      <td>6.635710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1514764920</td>\n      <td>8.514500</td>\n      <td>-0.015875</td>\n      <td>71.390000</td>\n      <td>13828.102000</td>\n      <td>-0.015037</td>\n      <td>31.046432</td>\n      <td>2372.286667</td>\n      <td>-0.004079</td>\n      <td>24.050259</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.656700</td>\n      <td>-0.014534</td>\n      <td>3277.475494</td>\n      <td>738.260000</td>\n      <td>-0.004441</td>\n      <td>232.793141</td>\n      <td>25.897000</td>\n      <td>-0.029902</td>\n      <td>1.468019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>224.856667</td>\n      <td>-0.012991</td>\n      <td>3640.502706</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4500</td>\n      <td>-0.009690</td>\n      <td>0.349420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1514764980</td>\n      <td>8.484800</td>\n      <td>-0.015410</td>\n      <td>1546.820000</td>\n      <td>13801.314000</td>\n      <td>-0.010309</td>\n      <td>55.061820</td>\n      <td>2372.063333</td>\n      <td>-0.002892</td>\n      <td>42.676438</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.651200</td>\n      <td>-0.012546</td>\n      <td>5623.557585</td>\n      <td>737.502500</td>\n      <td>-0.004206</td>\n      <td>174.138031</td>\n      <td>26.469500</td>\n      <td>-0.030832</td>\n      <td>76.163922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000000</td>\n      <td>-0.003572</td>\n      <td>328.350286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.1900</td>\n      <td>0.006567</td>\n      <td>1.189553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2136957</th>\n      <td>1642982280</td>\n      <td>384.201667</td>\n      <td>NaN</td>\n      <td>496.467537</td>\n      <td>36276.223333</td>\n      <td>NaN</td>\n      <td>49.867700</td>\n      <td>302.430000</td>\n      <td>NaN</td>\n      <td>198.196485</td>\n      <td>1.123465</td>\n      <td>NaN</td>\n      <td>430719.654610</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.433686e+05</td>\n      <td>2.272983</td>\n      <td>NaN</td>\n      <td>14907.125055</td>\n      <td>2541.388571</td>\n      <td>NaN</td>\n      <td>821.614133</td>\n      <td>25.152780</td>\n      <td>NaN</td>\n      <td>1334.601131</td>\n      <td>0.814233</td>\n      <td>NaN</td>\n      <td>20511.345658</td>\n      <td>112.258000</td>\n      <td>NaN</td>\n      <td>790.632440</td>\n      <td>1831.573333</td>\n      <td>NaN</td>\n      <td>2.142748</td>\n      <td>156.5125</td>\n      <td>NaN</td>\n      <td>21.597886</td>\n      <td>0.200803</td>\n      <td>NaN</td>\n      <td>7.076212e+05</td>\n      <td>0.057509</td>\n      <td>NaN</td>\n      <td>1.301079e+06</td>\n    </tr>\n    <tr>\n      <th>2136958</th>\n      <td>1642982340</td>\n      <td>384.002000</td>\n      <td>NaN</td>\n      <td>368.684921</td>\n      <td>36276.623333</td>\n      <td>NaN</td>\n      <td>43.030556</td>\n      <td>302.562500</td>\n      <td>NaN</td>\n      <td>76.954035</td>\n      <td>1.124831</td>\n      <td>NaN</td>\n      <td>352929.794282</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.287195e+05</td>\n      <td>2.275100</td>\n      <td>NaN</td>\n      <td>14559.759824</td>\n      <td>2541.040000</td>\n      <td>NaN</td>\n      <td>749.548301</td>\n      <td>25.135800</td>\n      <td>NaN</td>\n      <td>2410.205214</td>\n      <td>0.815833</td>\n      <td>NaN</td>\n      <td>9064.791667</td>\n      <td>112.342400</td>\n      <td>NaN</td>\n      <td>331.208442</td>\n      <td>1833.186000</td>\n      <td>NaN</td>\n      <td>3.985169</td>\n      <td>156.8940</td>\n      <td>NaN</td>\n      <td>51.115980</td>\n      <td>0.200968</td>\n      <td>NaN</td>\n      <td>1.988516e+05</td>\n      <td>0.057521</td>\n      <td>NaN</td>\n      <td>9.245588e+05</td>\n    </tr>\n    <tr>\n      <th>2136959</th>\n      <td>1642982400</td>\n      <td>382.930000</td>\n      <td>NaN</td>\n      <td>1490.571077</td>\n      <td>36221.987143</td>\n      <td>NaN</td>\n      <td>110.053151</td>\n      <td>302.112000</td>\n      <td>NaN</td>\n      <td>209.645675</td>\n      <td>1.121151</td>\n      <td>NaN</td>\n      <td>249923.229256</td>\n      <td>0.141391</td>\n      <td>NaN</td>\n      <td>1.009688e+06</td>\n      <td>2.264800</td>\n      <td>NaN</td>\n      <td>202194.015531</td>\n      <td>2536.151429</td>\n      <td>NaN</td>\n      <td>1057.765667</td>\n      <td>25.028067</td>\n      <td>NaN</td>\n      <td>6709.816310</td>\n      <td>0.812400</td>\n      <td>NaN</td>\n      <td>6019.269834</td>\n      <td>111.983333</td>\n      <td>NaN</td>\n      <td>1243.860266</td>\n      <td>1833.018333</td>\n      <td>NaN</td>\n      <td>4.190290</td>\n      <td>156.5500</td>\n      <td>NaN</td>\n      <td>74.397471</td>\n      <td>0.200093</td>\n      <td>NaN</td>\n      <td>2.983796e+06</td>\n      <td>0.057478</td>\n      <td>NaN</td>\n      <td>8.988253e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>2136960 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducing timestamp indexes to normal columns\n",
    "train_merged = train_merged.reset_index()\n",
    "train_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:20.823798200Z",
     "start_time": "2023-12-07T08:04:20.410529900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Fill the null with ffill, but limit it to 100 to prevent the error from being too large\n",
    "for id in range(14):\n",
    "    train_merged[f'Close_{id}'] = train_merged[f'Close_{id}'].ffill(limit=100)\n",
    "    train_merged[f'Volume_{id}'] = train_merged[f'Volume_{id}'].ffill(limit=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.520793200Z",
     "start_time": "2023-12-07T08:04:20.813790900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "          timestamp     Close_0  Target_0     Volume_0       Close_1  \\\n0        1514764860    8.530000 -0.014399    78.380000  13850.176000   \n1        1514764920    8.514500 -0.015875    71.390000  13828.102000   \n2        1514764980    8.484800 -0.015410  1546.820000  13801.314000   \n...             ...         ...       ...          ...           ...   \n2136957  1642982280  384.201667       NaN   496.467537  36276.223333   \n2136958  1642982340  384.002000       NaN   368.684921  36276.623333   \n2136959  1642982400  382.930000       NaN  1490.571077  36221.987143   \n\n         Target_1    Volume_1      Close_2  Target_2    Volume_2   Close_3  \\\n0       -0.014643   31.550062  2374.590000 -0.004218   19.233005       NaN   \n1       -0.015037   31.046432  2372.286667 -0.004079   24.050259       NaN   \n2       -0.010309   55.061820  2372.063333 -0.002892   42.676438       NaN   \n...           ...         ...          ...       ...         ...       ...   \n2136957       NaN   49.867700   302.430000       NaN  198.196485  1.123465   \n2136958       NaN   43.030556   302.562500       NaN   76.954035  1.124831   \n2136959       NaN  110.053151   302.112000       NaN  209.645675  1.121151   \n\n         Target_3       Volume_3   Close_4  Target_4      Volume_4   Close_5  \\\n0             NaN            NaN       NaN       NaN           NaN  7.657600   \n1             NaN            NaN       NaN       NaN           NaN  7.656700   \n2             NaN            NaN       NaN       NaN           NaN  7.651200   \n...           ...            ...       ...       ...           ...       ...   \n2136957       NaN  430719.654610  0.141800       NaN  7.433686e+05  2.272983   \n2136958       NaN  352929.794282  0.141800       NaN  7.287195e+05  2.275100   \n2136959       NaN  249923.229256  0.141391       NaN  1.009688e+06  2.264800   \n\n         Target_5       Volume_5      Close_6  Target_6     Volume_6  \\\n0       -0.013922    6626.713370   738.507500 -0.004809   335.987856   \n1       -0.014534    3277.475494   738.260000 -0.004441   232.793141   \n2       -0.012546    5623.557585   737.502500 -0.004206   174.138031   \n...           ...            ...          ...       ...          ...   \n2136957       NaN   14907.125055  2541.388571       NaN   821.614133   \n2136958       NaN   14559.759824  2541.040000       NaN   749.548301   \n2136959       NaN  202194.015531  2536.151429       NaN  1057.765667   \n\n           Close_7  Target_7     Volume_7   Close_8  Target_8      Volume_8  \\\n0        25.877000 -0.008264   121.087310       NaN       NaN           NaN   \n1        25.897000 -0.029902     1.468019       NaN       NaN           NaN   \n2        26.469500 -0.030832    76.163922       NaN       NaN           NaN   \n...            ...       ...          ...       ...       ...           ...   \n2136957  25.152780       NaN  1334.601131  0.814233       NaN  20511.345658   \n2136958  25.135800       NaN  2410.205214  0.815833       NaN   9064.791667   \n2136959  25.028067       NaN  6709.816310  0.812400       NaN   6019.269834   \n\n            Close_9  Target_9     Volume_9     Close_10  Target_10  Volume_10  \\\n0        225.206667 -0.009791   411.896642          NaN        NaN        NaN   \n1        224.856667 -0.012991  3640.502706          NaN        NaN        NaN   \n2        226.000000 -0.003572   328.350286          NaN        NaN        NaN   \n...             ...       ...          ...          ...        ...        ...   \n2136957  112.258000       NaN   790.632440  1831.573333        NaN   2.142748   \n2136958  112.342400       NaN   331.208442  1833.186000        NaN   3.985169   \n2136959  111.983333       NaN  1243.860266  1833.018333        NaN   4.190290   \n\n         Close_11  Target_11  Volume_11  Close_12  Target_12     Volume_12  \\\n0        329.4600        NaN   6.635710       NaN        NaN           NaN   \n1        329.4500  -0.009690   0.349420       NaN        NaN           NaN   \n2        329.1900   0.006567   1.189553       NaN        NaN           NaN   \n...           ...        ...        ...       ...        ...           ...   \n2136957  156.5125        NaN  21.597886  0.200803        NaN  7.076212e+05   \n2136958  156.8940        NaN  51.115980  0.200968        NaN  1.988516e+05   \n2136959  156.5500        NaN  74.397471  0.200093        NaN  2.983796e+06   \n\n         Close_13  Target_13     Volume_13  \n0             NaN        NaN           NaN  \n1             NaN        NaN           NaN  \n2             NaN        NaN           NaN  \n...           ...        ...           ...  \n2136957  0.057509        NaN  1.301079e+06  \n2136958  0.057521        NaN  9.245588e+05  \n2136959  0.057478        NaN  8.988253e+05  \n\n[2136960 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>Close_0</th>\n      <th>Target_0</th>\n      <th>Volume_0</th>\n      <th>Close_1</th>\n      <th>Target_1</th>\n      <th>Volume_1</th>\n      <th>Close_2</th>\n      <th>Target_2</th>\n      <th>Volume_2</th>\n      <th>Close_3</th>\n      <th>Target_3</th>\n      <th>Volume_3</th>\n      <th>Close_4</th>\n      <th>Target_4</th>\n      <th>Volume_4</th>\n      <th>Close_5</th>\n      <th>Target_5</th>\n      <th>Volume_5</th>\n      <th>Close_6</th>\n      <th>Target_6</th>\n      <th>Volume_6</th>\n      <th>Close_7</th>\n      <th>Target_7</th>\n      <th>Volume_7</th>\n      <th>Close_8</th>\n      <th>Target_8</th>\n      <th>Volume_8</th>\n      <th>Close_9</th>\n      <th>Target_9</th>\n      <th>Volume_9</th>\n      <th>Close_10</th>\n      <th>Target_10</th>\n      <th>Volume_10</th>\n      <th>Close_11</th>\n      <th>Target_11</th>\n      <th>Volume_11</th>\n      <th>Close_12</th>\n      <th>Target_12</th>\n      <th>Volume_12</th>\n      <th>Close_13</th>\n      <th>Target_13</th>\n      <th>Volume_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1514764860</td>\n      <td>8.530000</td>\n      <td>-0.014399</td>\n      <td>78.380000</td>\n      <td>13850.176000</td>\n      <td>-0.014643</td>\n      <td>31.550062</td>\n      <td>2374.590000</td>\n      <td>-0.004218</td>\n      <td>19.233005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.657600</td>\n      <td>-0.013922</td>\n      <td>6626.713370</td>\n      <td>738.507500</td>\n      <td>-0.004809</td>\n      <td>335.987856</td>\n      <td>25.877000</td>\n      <td>-0.008264</td>\n      <td>121.087310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>225.206667</td>\n      <td>-0.009791</td>\n      <td>411.896642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4600</td>\n      <td>NaN</td>\n      <td>6.635710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1514764920</td>\n      <td>8.514500</td>\n      <td>-0.015875</td>\n      <td>71.390000</td>\n      <td>13828.102000</td>\n      <td>-0.015037</td>\n      <td>31.046432</td>\n      <td>2372.286667</td>\n      <td>-0.004079</td>\n      <td>24.050259</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.656700</td>\n      <td>-0.014534</td>\n      <td>3277.475494</td>\n      <td>738.260000</td>\n      <td>-0.004441</td>\n      <td>232.793141</td>\n      <td>25.897000</td>\n      <td>-0.029902</td>\n      <td>1.468019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>224.856667</td>\n      <td>-0.012991</td>\n      <td>3640.502706</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4500</td>\n      <td>-0.009690</td>\n      <td>0.349420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1514764980</td>\n      <td>8.484800</td>\n      <td>-0.015410</td>\n      <td>1546.820000</td>\n      <td>13801.314000</td>\n      <td>-0.010309</td>\n      <td>55.061820</td>\n      <td>2372.063333</td>\n      <td>-0.002892</td>\n      <td>42.676438</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.651200</td>\n      <td>-0.012546</td>\n      <td>5623.557585</td>\n      <td>737.502500</td>\n      <td>-0.004206</td>\n      <td>174.138031</td>\n      <td>26.469500</td>\n      <td>-0.030832</td>\n      <td>76.163922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000000</td>\n      <td>-0.003572</td>\n      <td>328.350286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.1900</td>\n      <td>0.006567</td>\n      <td>1.189553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2136957</th>\n      <td>1642982280</td>\n      <td>384.201667</td>\n      <td>NaN</td>\n      <td>496.467537</td>\n      <td>36276.223333</td>\n      <td>NaN</td>\n      <td>49.867700</td>\n      <td>302.430000</td>\n      <td>NaN</td>\n      <td>198.196485</td>\n      <td>1.123465</td>\n      <td>NaN</td>\n      <td>430719.654610</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.433686e+05</td>\n      <td>2.272983</td>\n      <td>NaN</td>\n      <td>14907.125055</td>\n      <td>2541.388571</td>\n      <td>NaN</td>\n      <td>821.614133</td>\n      <td>25.152780</td>\n      <td>NaN</td>\n      <td>1334.601131</td>\n      <td>0.814233</td>\n      <td>NaN</td>\n      <td>20511.345658</td>\n      <td>112.258000</td>\n      <td>NaN</td>\n      <td>790.632440</td>\n      <td>1831.573333</td>\n      <td>NaN</td>\n      <td>2.142748</td>\n      <td>156.5125</td>\n      <td>NaN</td>\n      <td>21.597886</td>\n      <td>0.200803</td>\n      <td>NaN</td>\n      <td>7.076212e+05</td>\n      <td>0.057509</td>\n      <td>NaN</td>\n      <td>1.301079e+06</td>\n    </tr>\n    <tr>\n      <th>2136958</th>\n      <td>1642982340</td>\n      <td>384.002000</td>\n      <td>NaN</td>\n      <td>368.684921</td>\n      <td>36276.623333</td>\n      <td>NaN</td>\n      <td>43.030556</td>\n      <td>302.562500</td>\n      <td>NaN</td>\n      <td>76.954035</td>\n      <td>1.124831</td>\n      <td>NaN</td>\n      <td>352929.794282</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.287195e+05</td>\n      <td>2.275100</td>\n      <td>NaN</td>\n      <td>14559.759824</td>\n      <td>2541.040000</td>\n      <td>NaN</td>\n      <td>749.548301</td>\n      <td>25.135800</td>\n      <td>NaN</td>\n      <td>2410.205214</td>\n      <td>0.815833</td>\n      <td>NaN</td>\n      <td>9064.791667</td>\n      <td>112.342400</td>\n      <td>NaN</td>\n      <td>331.208442</td>\n      <td>1833.186000</td>\n      <td>NaN</td>\n      <td>3.985169</td>\n      <td>156.8940</td>\n      <td>NaN</td>\n      <td>51.115980</td>\n      <td>0.200968</td>\n      <td>NaN</td>\n      <td>1.988516e+05</td>\n      <td>0.057521</td>\n      <td>NaN</td>\n      <td>9.245588e+05</td>\n    </tr>\n    <tr>\n      <th>2136959</th>\n      <td>1642982400</td>\n      <td>382.930000</td>\n      <td>NaN</td>\n      <td>1490.571077</td>\n      <td>36221.987143</td>\n      <td>NaN</td>\n      <td>110.053151</td>\n      <td>302.112000</td>\n      <td>NaN</td>\n      <td>209.645675</td>\n      <td>1.121151</td>\n      <td>NaN</td>\n      <td>249923.229256</td>\n      <td>0.141391</td>\n      <td>NaN</td>\n      <td>1.009688e+06</td>\n      <td>2.264800</td>\n      <td>NaN</td>\n      <td>202194.015531</td>\n      <td>2536.151429</td>\n      <td>NaN</td>\n      <td>1057.765667</td>\n      <td>25.028067</td>\n      <td>NaN</td>\n      <td>6709.816310</td>\n      <td>0.812400</td>\n      <td>NaN</td>\n      <td>6019.269834</td>\n      <td>111.983333</td>\n      <td>NaN</td>\n      <td>1243.860266</td>\n      <td>1833.018333</td>\n      <td>NaN</td>\n      <td>4.190290</td>\n      <td>156.5500</td>\n      <td>NaN</td>\n      <td>74.397471</td>\n      <td>0.200093</td>\n      <td>NaN</td>\n      <td>2.983796e+06</td>\n      <td>0.057478</td>\n      <td>NaN</td>\n      <td>8.988253e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>2136960 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.606526200Z",
     "start_time": "2023-12-07T08:04:21.540799700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# test_df is for training\n",
    "test_df = train_merged"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.616533300Z",
     "start_time": "2023-12-07T08:04:21.598749800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature_Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.565816Z",
     "iopub.status.busy": "2023-12-05T15:54:25.564642Z",
     "iopub.status.idle": "2023-12-05T15:54:25.602795Z",
     "shell.execute_reply": "2023-12-05T15:54:25.600877Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.565757Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.641523600Z",
     "start_time": "2023-12-07T08:04:21.620062500Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.605809Z",
     "iopub.status.busy": "2023-12-05T15:54:25.605025Z",
     "iopub.status.idle": "2023-12-05T15:54:25.620078Z",
     "shell.execute_reply": "2023-12-05T15:54:25.618797Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.605745Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.667881700Z",
     "start_time": "2023-12-07T08:04:21.635224700Z"
    }
   },
   "outputs": [],
   "source": [
    "def MACD(df, short_term=12 * 30, long_term=26 * 30):\n",
    "    '''Pass in a df and return the df with the 'MACD_id' columns'''\n",
    "    tmp_col = ['EMA_short', 'EMA_long', 'DIF', 'DEA']\n",
    "    for id in range(14):\n",
    "        cols_to_select = [f'Close_{id}', 'timestamp', f'Target_{id}']\n",
    "        df_selected = df[cols_to_select].sort_values('timestamp')\n",
    "\n",
    "        # Calculate short-term (fast) moving averages（EMA12）\n",
    "        df_selected['EMA_short'] = df_selected[f'Close_{id}'].ewm(span=short_term, adjust=False).mean()\n",
    "\n",
    "        # Calculating long-term (slow) moving averages（EMA26）\n",
    "        df_selected['EMA_long'] = df_selected[f'Close_{id}'].ewm(span=long_term, adjust=False).mean()\n",
    "\n",
    "        # Calculation of deviation（DIF）\n",
    "        df_selected['DIF'] = df_selected['EMA_short'] - df_selected['EMA_long']\n",
    "\n",
    "        # Calculating the DEA using a 9-day exponential moving average\n",
    "        df_selected['DEA'] = df_selected['DIF'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "        # Calculating the MACD line\n",
    "        df_selected[f'MACD_id{id}'] = df_selected['DIF'] - df_selected['DEA']\n",
    "\n",
    "        # Merge back into the original DataFrame\n",
    "        df = df.merge(df_selected[['timestamp', f'MACD_id{id}']], on='timestamp', how='left')\n",
    "\n",
    "    # Check and delete temporary columns\n",
    "    df.drop(columns=tmp_col, errors='ignore', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.622786Z",
     "iopub.status.busy": "2023-12-05T15:54:25.622312Z",
     "iopub.status.idle": "2023-12-05T15:54:25.635036Z",
     "shell.execute_reply": "2023-12-05T15:54:25.633815Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.622730Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.686465200Z",
     "start_time": "2023-12-07T08:04:21.652645900Z"
    }
   },
   "outputs": [],
   "source": [
    "def VolumeRatio(df, mean_term=300):\n",
    "    '''\n",
    "    Pass in a df and return the df with the column 'Volume_Ratio_id'\n",
    "    '''\n",
    "    tmp_col = ['Volume_Mean']\n",
    "    \n",
    "    for id in range(14):\n",
    "        # Calculate the 'Volume Mean' column, which represents the volume mean for the previous n days\n",
    "        df[f'Volume_Mean'] = df[f'Volume_{id}'].rolling(window=mean_term).mean()\n",
    "        \n",
    "        # Calculate the 'Volume Ratio' column\n",
    "        df[f'Volume_Ratio_id{id}'] = df[f'Volume_{id}'] / df[f'Volume_Mean']\n",
    "    \n",
    "    # Check and delete temporary columns\n",
    "    df.drop(columns=tmp_col, errors='ignore', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.637625Z",
     "iopub.status.busy": "2023-12-05T15:54:25.636707Z",
     "iopub.status.idle": "2023-12-05T15:54:25.649171Z",
     "shell.execute_reply": "2023-12-05T15:54:25.647799Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.637586Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.715385900Z",
     "start_time": "2023-12-07T08:04:21.653155200Z"
    }
   },
   "outputs": [],
   "source": [
    "def RSI(test_df, window_size=300):\n",
    "    '''\n",
    "    Pass in a df, return the df with the 'RSI_id' columns\n",
    "    '''\n",
    "    tmp_col = ['Price_Change', 'Gain', 'Loss', 'Avg_Gain', 'Avg_Loss', 'RS']\n",
    "    \n",
    "    for id in range(14):\n",
    "        col_name = f'Close_{id}'\n",
    "    \n",
    "        # Calculate daily price changes\n",
    "        test_df['Price_Change'] = test_df[col_name].diff()\n",
    "    \n",
    "        # Categorising price changes into upward and downward\n",
    "        test_df['Gain'] = test_df['Price_Change'].apply(lambda x: x if x > 0 else 0)\n",
    "        test_df['Loss'] = test_df['Price_Change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    \n",
    "        # Calculation of the average of increase and decrease\n",
    "        test_df['Avg_Gain'] = test_df['Gain'].rolling(window=window_size).mean()\n",
    "        test_df['Avg_Loss'] = test_df['Loss'].rolling(window=window_size).mean()\n",
    "    \n",
    "        # Calculation of the relative strength index（RSI）\n",
    "        test_df['RS'] = test_df['Avg_Gain'] / test_df['Avg_Loss']\n",
    "        test_df[f'RSI_id{id}'] = 100 - (100 / (1 + test_df['RS']))\n",
    "    \n",
    "    test_df.drop(tmp_col, axis=1, inplace=True)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.651943Z",
     "iopub.status.busy": "2023-12-05T15:54:25.650833Z",
     "iopub.status.idle": "2023-12-05T15:54:25.666648Z",
     "shell.execute_reply": "2023-12-05T15:54:25.664957Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.651901Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.725391700Z",
     "start_time": "2023-12-07T08:04:21.685957100Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df, train=True):   \n",
    "    for id in range(14):    # 14\n",
    "        for lag in lags: # 3\n",
    "            # feature 1\n",
    "            # Convolutional (smoothing) processing + fill in the vacant bits with 1 + roll rolls 1 to the top\n",
    "            # This feature represents, for example, the logarithm of the price of asset 1 on day 60 versus the average price from day 1 - day 60\n",
    "            df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
    "            # feature 2\n",
    "            # Logarithmic returns at lagged lag period for asset with id=id\n",
    "            # This feature represents, for example, the logarithm of the ratio of the price of asset 1 on day 60 to the price on day 1\n",
    "            df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
    "    for lag in lags: #3\n",
    "        # Feature A\n",
    "        # This feature represents, for example, the mean of the 60th order lag of feature 1 for all assets. \n",
    "        df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
    "        # Feature B\n",
    "        # This feature represents, for example, the mean of the 60th order lag of feature 2 for all assets. \n",
    "        df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
    "        for id in range(14):\n",
    "            # feature 5\n",
    "            # This feature represents, for example, the difference between feature 1 and feature A of asset 1\n",
    "            df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
    "            # feature NO.6\n",
    "            # This feature represents, for example, the difference between feature 2 and feature B for asset 1\n",
    "            df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
    "\n",
    "    for id in range(14):\n",
    "        df = df.drop([f'Close_{id}'], axis=1)\n",
    "        df = df.drop([f'Volume_{id}'],axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.669902Z",
     "iopub.status.busy": "2023-12-05T15:54:25.668989Z",
     "iopub.status.idle": "2023-12-05T15:54:25.681544Z",
     "shell.execute_reply": "2023-12-05T15:54:25.679945Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.669847Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.725391700Z",
     "start_time": "2023-12-07T08:04:21.686465200Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_below_first_non_empty_900_nan(column):\n",
    "    first_non_empty_index = column.first_valid_index()\n",
    "    \n",
    "    if first_non_empty_index is not None:\n",
    "        column.loc[first_non_empty_index:first_non_empty_index+899] = np.nan\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.737239Z",
     "iopub.status.busy": "2023-12-05T15:54:25.734602Z",
     "iopub.status.idle": "2023-12-05T15:54:25.749113Z",
     "shell.execute_reply": "2023-12-05T15:54:25.747878Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.737197Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.725391700Z",
     "start_time": "2023-12-07T08:04:21.695688800Z"
    }
   },
   "outputs": [],
   "source": [
    "# For CV score calculation\n",
    "def corr_score(pred, valid):\n",
    "    len_data = len(pred)\n",
    "    mean_pred = np.sum(pred) / len_data\n",
    "    mean_valid = np.sum(valid) / len_data\n",
    "    var_pred = np.sum(np.square(pred - mean_pred)) / len_data\n",
    "    var_valid = np.sum(np.square(valid - mean_valid)) / len_data\n",
    "\n",
    "    cov = np.sum((pred * valid)) / len_data - mean_pred * mean_valid\n",
    "    corr = cov / np.sqrt(var_pred * var_valid)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:54:25.774030Z",
     "iopub.status.busy": "2023-12-05T15:54:25.773615Z",
     "iopub.status.idle": "2023-12-05T15:54:25.783696Z",
     "shell.execute_reply": "2023-12-05T15:54:25.782278Z",
     "shell.execute_reply.started": "2023-12-05T15:54:25.773993Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.735392800Z",
     "start_time": "2023-12-07T08:04:21.705953Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_scorer(y_true, y_pred):\n",
    "    correlation, _ = np.corrcoef(y_true, y_pred)\n",
    "    return correlation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:55:02.510148Z",
     "iopub.status.busy": "2023-12-05T15:55:02.509858Z",
     "iopub.status.idle": "2023-12-05T15:55:02.554841Z",
     "shell.execute_reply": "2023-12-05T15:55:02.553844Z",
     "shell.execute_reply.started": "2023-12-05T15:55:02.510122Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:04:21.780974800Z",
     "start_time": "2023-12-07T08:04:21.715385900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          timestamp     Close_0  Target_0     Volume_0       Close_1  \\\n0        1514764860    8.530000 -0.014399    78.380000  13850.176000   \n1        1514764920    8.514500 -0.015875    71.390000  13828.102000   \n2        1514764980    8.484800 -0.015410  1546.820000  13801.314000   \n...             ...         ...       ...          ...           ...   \n2136957  1642982280  384.201667       NaN   496.467537  36276.223333   \n2136958  1642982340  384.002000       NaN   368.684921  36276.623333   \n2136959  1642982400  382.930000       NaN  1490.571077  36221.987143   \n\n         Target_1    Volume_1      Close_2  Target_2    Volume_2   Close_3  \\\n0       -0.014643   31.550062  2374.590000 -0.004218   19.233005       NaN   \n1       -0.015037   31.046432  2372.286667 -0.004079   24.050259       NaN   \n2       -0.010309   55.061820  2372.063333 -0.002892   42.676438       NaN   \n...           ...         ...          ...       ...         ...       ...   \n2136957       NaN   49.867700   302.430000       NaN  198.196485  1.123465   \n2136958       NaN   43.030556   302.562500       NaN   76.954035  1.124831   \n2136959       NaN  110.053151   302.112000       NaN  209.645675  1.121151   \n\n         Target_3       Volume_3   Close_4  Target_4      Volume_4   Close_5  \\\n0             NaN            NaN       NaN       NaN           NaN  7.657600   \n1             NaN            NaN       NaN       NaN           NaN  7.656700   \n2             NaN            NaN       NaN       NaN           NaN  7.651200   \n...           ...            ...       ...       ...           ...       ...   \n2136957       NaN  430719.654610  0.141800       NaN  7.433686e+05  2.272983   \n2136958       NaN  352929.794282  0.141800       NaN  7.287195e+05  2.275100   \n2136959       NaN  249923.229256  0.141391       NaN  1.009688e+06  2.264800   \n\n         Target_5       Volume_5      Close_6  Target_6     Volume_6  \\\n0       -0.013922    6626.713370   738.507500 -0.004809   335.987856   \n1       -0.014534    3277.475494   738.260000 -0.004441   232.793141   \n2       -0.012546    5623.557585   737.502500 -0.004206   174.138031   \n...           ...            ...          ...       ...          ...   \n2136957       NaN   14907.125055  2541.388571       NaN   821.614133   \n2136958       NaN   14559.759824  2541.040000       NaN   749.548301   \n2136959       NaN  202194.015531  2536.151429       NaN  1057.765667   \n\n           Close_7  Target_7     Volume_7   Close_8  Target_8      Volume_8  \\\n0        25.877000 -0.008264   121.087310       NaN       NaN           NaN   \n1        25.897000 -0.029902     1.468019       NaN       NaN           NaN   \n2        26.469500 -0.030832    76.163922       NaN       NaN           NaN   \n...            ...       ...          ...       ...       ...           ...   \n2136957  25.152780       NaN  1334.601131  0.814233       NaN  20511.345658   \n2136958  25.135800       NaN  2410.205214  0.815833       NaN   9064.791667   \n2136959  25.028067       NaN  6709.816310  0.812400       NaN   6019.269834   \n\n            Close_9  Target_9     Volume_9     Close_10  Target_10  Volume_10  \\\n0        225.206667 -0.009791   411.896642          NaN        NaN        NaN   \n1        224.856667 -0.012991  3640.502706          NaN        NaN        NaN   \n2        226.000000 -0.003572   328.350286          NaN        NaN        NaN   \n...             ...       ...          ...          ...        ...        ...   \n2136957  112.258000       NaN   790.632440  1831.573333        NaN   2.142748   \n2136958  112.342400       NaN   331.208442  1833.186000        NaN   3.985169   \n2136959  111.983333       NaN  1243.860266  1833.018333        NaN   4.190290   \n\n         Close_11  Target_11  Volume_11  Close_12  Target_12     Volume_12  \\\n0        329.4600        NaN   6.635710       NaN        NaN           NaN   \n1        329.4500  -0.009690   0.349420       NaN        NaN           NaN   \n2        329.1900   0.006567   1.189553       NaN        NaN           NaN   \n...           ...        ...        ...       ...        ...           ...   \n2136957  156.5125        NaN  21.597886  0.200803        NaN  7.076212e+05   \n2136958  156.8940        NaN  51.115980  0.200968        NaN  1.988516e+05   \n2136959  156.5500        NaN  74.397471  0.200093        NaN  2.983796e+06   \n\n         Close_13  Target_13     Volume_13  \n0             NaN        NaN           NaN  \n1             NaN        NaN           NaN  \n2             NaN        NaN           NaN  \n...           ...        ...           ...  \n2136957  0.057509        NaN  1.301079e+06  \n2136958  0.057521        NaN  9.245588e+05  \n2136959  0.057478        NaN  8.988253e+05  \n\n[2136960 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>Close_0</th>\n      <th>Target_0</th>\n      <th>Volume_0</th>\n      <th>Close_1</th>\n      <th>Target_1</th>\n      <th>Volume_1</th>\n      <th>Close_2</th>\n      <th>Target_2</th>\n      <th>Volume_2</th>\n      <th>Close_3</th>\n      <th>Target_3</th>\n      <th>Volume_3</th>\n      <th>Close_4</th>\n      <th>Target_4</th>\n      <th>Volume_4</th>\n      <th>Close_5</th>\n      <th>Target_5</th>\n      <th>Volume_5</th>\n      <th>Close_6</th>\n      <th>Target_6</th>\n      <th>Volume_6</th>\n      <th>Close_7</th>\n      <th>Target_7</th>\n      <th>Volume_7</th>\n      <th>Close_8</th>\n      <th>Target_8</th>\n      <th>Volume_8</th>\n      <th>Close_9</th>\n      <th>Target_9</th>\n      <th>Volume_9</th>\n      <th>Close_10</th>\n      <th>Target_10</th>\n      <th>Volume_10</th>\n      <th>Close_11</th>\n      <th>Target_11</th>\n      <th>Volume_11</th>\n      <th>Close_12</th>\n      <th>Target_12</th>\n      <th>Volume_12</th>\n      <th>Close_13</th>\n      <th>Target_13</th>\n      <th>Volume_13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1514764860</td>\n      <td>8.530000</td>\n      <td>-0.014399</td>\n      <td>78.380000</td>\n      <td>13850.176000</td>\n      <td>-0.014643</td>\n      <td>31.550062</td>\n      <td>2374.590000</td>\n      <td>-0.004218</td>\n      <td>19.233005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.657600</td>\n      <td>-0.013922</td>\n      <td>6626.713370</td>\n      <td>738.507500</td>\n      <td>-0.004809</td>\n      <td>335.987856</td>\n      <td>25.877000</td>\n      <td>-0.008264</td>\n      <td>121.087310</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>225.206667</td>\n      <td>-0.009791</td>\n      <td>411.896642</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4600</td>\n      <td>NaN</td>\n      <td>6.635710</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1514764920</td>\n      <td>8.514500</td>\n      <td>-0.015875</td>\n      <td>71.390000</td>\n      <td>13828.102000</td>\n      <td>-0.015037</td>\n      <td>31.046432</td>\n      <td>2372.286667</td>\n      <td>-0.004079</td>\n      <td>24.050259</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.656700</td>\n      <td>-0.014534</td>\n      <td>3277.475494</td>\n      <td>738.260000</td>\n      <td>-0.004441</td>\n      <td>232.793141</td>\n      <td>25.897000</td>\n      <td>-0.029902</td>\n      <td>1.468019</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>224.856667</td>\n      <td>-0.012991</td>\n      <td>3640.502706</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.4500</td>\n      <td>-0.009690</td>\n      <td>0.349420</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1514764980</td>\n      <td>8.484800</td>\n      <td>-0.015410</td>\n      <td>1546.820000</td>\n      <td>13801.314000</td>\n      <td>-0.010309</td>\n      <td>55.061820</td>\n      <td>2372.063333</td>\n      <td>-0.002892</td>\n      <td>42.676438</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.651200</td>\n      <td>-0.012546</td>\n      <td>5623.557585</td>\n      <td>737.502500</td>\n      <td>-0.004206</td>\n      <td>174.138031</td>\n      <td>26.469500</td>\n      <td>-0.030832</td>\n      <td>76.163922</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>226.000000</td>\n      <td>-0.003572</td>\n      <td>328.350286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>329.1900</td>\n      <td>0.006567</td>\n      <td>1.189553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2136957</th>\n      <td>1642982280</td>\n      <td>384.201667</td>\n      <td>NaN</td>\n      <td>496.467537</td>\n      <td>36276.223333</td>\n      <td>NaN</td>\n      <td>49.867700</td>\n      <td>302.430000</td>\n      <td>NaN</td>\n      <td>198.196485</td>\n      <td>1.123465</td>\n      <td>NaN</td>\n      <td>430719.654610</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.433686e+05</td>\n      <td>2.272983</td>\n      <td>NaN</td>\n      <td>14907.125055</td>\n      <td>2541.388571</td>\n      <td>NaN</td>\n      <td>821.614133</td>\n      <td>25.152780</td>\n      <td>NaN</td>\n      <td>1334.601131</td>\n      <td>0.814233</td>\n      <td>NaN</td>\n      <td>20511.345658</td>\n      <td>112.258000</td>\n      <td>NaN</td>\n      <td>790.632440</td>\n      <td>1831.573333</td>\n      <td>NaN</td>\n      <td>2.142748</td>\n      <td>156.5125</td>\n      <td>NaN</td>\n      <td>21.597886</td>\n      <td>0.200803</td>\n      <td>NaN</td>\n      <td>7.076212e+05</td>\n      <td>0.057509</td>\n      <td>NaN</td>\n      <td>1.301079e+06</td>\n    </tr>\n    <tr>\n      <th>2136958</th>\n      <td>1642982340</td>\n      <td>384.002000</td>\n      <td>NaN</td>\n      <td>368.684921</td>\n      <td>36276.623333</td>\n      <td>NaN</td>\n      <td>43.030556</td>\n      <td>302.562500</td>\n      <td>NaN</td>\n      <td>76.954035</td>\n      <td>1.124831</td>\n      <td>NaN</td>\n      <td>352929.794282</td>\n      <td>0.141800</td>\n      <td>NaN</td>\n      <td>7.287195e+05</td>\n      <td>2.275100</td>\n      <td>NaN</td>\n      <td>14559.759824</td>\n      <td>2541.040000</td>\n      <td>NaN</td>\n      <td>749.548301</td>\n      <td>25.135800</td>\n      <td>NaN</td>\n      <td>2410.205214</td>\n      <td>0.815833</td>\n      <td>NaN</td>\n      <td>9064.791667</td>\n      <td>112.342400</td>\n      <td>NaN</td>\n      <td>331.208442</td>\n      <td>1833.186000</td>\n      <td>NaN</td>\n      <td>3.985169</td>\n      <td>156.8940</td>\n      <td>NaN</td>\n      <td>51.115980</td>\n      <td>0.200968</td>\n      <td>NaN</td>\n      <td>1.988516e+05</td>\n      <td>0.057521</td>\n      <td>NaN</td>\n      <td>9.245588e+05</td>\n    </tr>\n    <tr>\n      <th>2136959</th>\n      <td>1642982400</td>\n      <td>382.930000</td>\n      <td>NaN</td>\n      <td>1490.571077</td>\n      <td>36221.987143</td>\n      <td>NaN</td>\n      <td>110.053151</td>\n      <td>302.112000</td>\n      <td>NaN</td>\n      <td>209.645675</td>\n      <td>1.121151</td>\n      <td>NaN</td>\n      <td>249923.229256</td>\n      <td>0.141391</td>\n      <td>NaN</td>\n      <td>1.009688e+06</td>\n      <td>2.264800</td>\n      <td>NaN</td>\n      <td>202194.015531</td>\n      <td>2536.151429</td>\n      <td>NaN</td>\n      <td>1057.765667</td>\n      <td>25.028067</td>\n      <td>NaN</td>\n      <td>6709.816310</td>\n      <td>0.812400</td>\n      <td>NaN</td>\n      <td>6019.269834</td>\n      <td>111.983333</td>\n      <td>NaN</td>\n      <td>1243.860266</td>\n      <td>1833.018333</td>\n      <td>NaN</td>\n      <td>4.190290</td>\n      <td>156.5500</td>\n      <td>NaN</td>\n      <td>74.397471</td>\n      <td>0.200093</td>\n      <td>NaN</td>\n      <td>2.983796e+06</td>\n      <td>0.057478</td>\n      <td>NaN</td>\n      <td>8.988253e+05</td>\n    </tr>\n  </tbody>\n</table>\n<p>2136960 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:55:02.600438Z",
     "iopub.status.busy": "2023-12-05T15:55:02.600081Z",
     "iopub.status.idle": "2023-12-05T15:56:37.344642Z",
     "shell.execute_reply": "2023-12-05T15:56:37.341998Z",
     "shell.execute_reply.started": "2023-12-05T15:55:02.600408Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:16.004415900Z",
     "start_time": "2023-12-07T08:04:21.777541300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 701.06 MB\n",
      "Memory usage after optimization is: 224.18 MB\n",
      "Decreased by 68.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_6696\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2718.64 MB\n",
      "Memory usage after optimization is: 945.62 MB\n",
      "Decreased by 65.2%\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "test_df=MACD(test_df)\n",
    "test_df=VolumeRatio(test_df)\n",
    "test_df = RSI(test_df)\n",
    "test_df=get_features(test_df)\n",
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:37.347620Z",
     "iopub.status.busy": "2023-12-05T15:56:37.347088Z",
     "iopub.status.idle": "2023-12-05T15:56:41.945028Z",
     "shell.execute_reply": "2023-12-05T15:56:41.943528Z",
     "shell.execute_reply.started": "2023-12-05T15:56:37.347570Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:25.551957500Z",
     "start_time": "2023-12-07T08:09:16.119206200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the first 900 non-null values (eliminating rows affected by the maximum lag order)\n",
    "test_df.iloc[:, 15:] = test_df.iloc[:, 15:].apply(replace_below_first_non_empty_900_nan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:41.946857Z",
     "iopub.status.busy": "2023-12-05T15:56:41.946508Z",
     "iopub.status.idle": "2023-12-05T15:56:41.953336Z",
     "shell.execute_reply": "2023-12-05T15:56:41.951914Z",
     "shell.execute_reply.started": "2023-12-05T15:56:41.946827Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:25.567062800Z",
     "start_time": "2023-12-07T08:09:25.562661700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the first 900 rows (because the targets in the first 900 rows can't be used for training because their corresponding feature values are all NaN)\n",
    "test_df = test_df.iloc[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:42.161287Z",
     "iopub.status.busy": "2023-12-05T15:56:42.161015Z",
     "iopub.status.idle": "2023-12-05T15:56:42.905292Z",
     "shell.execute_reply": "2023-12-05T15:56:42.903627Z",
     "shell.execute_reply.started": "2023-12-05T15:56:42.161261Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:27.461915700Z",
     "start_time": "2023-12-07T08:09:25.577070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 59.20 MB\n",
      "Memory usage after optimization is: 59.20 MB\n",
      "Decreased by 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Selecting the data from 2021-10-25 to 2022-1-24 as being inference\n",
    "test_df = test_df.loc[test_df['timestamp']>=1635091200,]\n",
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:42.907422Z",
     "iopub.status.busy": "2023-12-05T15:56:42.907011Z",
     "iopub.status.idle": "2023-12-05T15:56:43.103662Z",
     "shell.execute_reply": "2023-12-05T15:56:43.102084Z",
     "shell.execute_reply.started": "2023-12-05T15:56:42.907390Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:27.883386800Z",
     "start_time": "2023-12-07T08:09:27.461915700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          timestamp  Target_0  Target_1  Target_2  Target_3  Target_4  \\\n2005439  1635091200 -0.002430  0.002048  0.003521  0.001439 -0.002289   \n2005440  1635091260 -0.000086  0.001321  0.002192  0.001559 -0.000407   \n2005441  1635091320  0.001995  0.001098  0.001890  0.001281  0.001850   \n...             ...       ...       ...       ...       ...       ...   \n2136957  1642982280       NaN       NaN       NaN       NaN       NaN   \n2136958  1642982340       NaN       NaN       NaN       NaN       NaN   \n2136959  1642982400       NaN       NaN       NaN       NaN       NaN   \n\n         Target_5  Target_6  Target_7  Target_8  Target_9  Target_10  \\\n2005439 -0.000250  0.000073  0.001575 -0.009354 -0.008080  -0.002573   \n2005440 -0.000880  0.000226  0.000977 -0.005646 -0.006458  -0.001164   \n2005441 -0.000157 -0.000675  0.000768 -0.003485 -0.006683   0.001768   \n...           ...       ...       ...       ...       ...        ...   \n2136957       NaN       NaN       NaN       NaN       NaN        NaN   \n2136958       NaN       NaN       NaN       NaN       NaN        NaN   \n2136959       NaN       NaN       NaN       NaN       NaN        NaN   \n\n         Target_11  Target_12  Target_13  MACD_id0  MACD_id1  MACD_id2  \\\n2005439  -0.002966  -0.001269  -0.003735 -0.015854 -2.898438 -0.072815   \n2005440  -0.000839  -0.002779  -0.001337 -0.017502 -3.115234 -0.076782   \n2005441   0.002991  -0.003222   0.001167 -0.019943 -3.337891 -0.079834   \n...            ...        ...        ...       ...       ...       ...   \n2136957        NaN        NaN        NaN  0.135498  8.132812  0.113953   \n2136958        NaN        NaN        NaN  0.134888  8.062500  0.114197   \n2136959        NaN        NaN        NaN  0.131714  7.855469  0.112976   \n\n         MACD_id3  MACD_id4  MACD_id5  MACD_id6  MACD_id7  MACD_id8  MACD_id9  \\\n2005439 -0.000112 -0.000053 -0.001646 -0.255371 -0.003613 -0.000191 -0.029114   \n2005440 -0.000120 -0.000054 -0.001637 -0.279541 -0.003744 -0.000199 -0.029541   \n2005441 -0.000131 -0.000056 -0.001637 -0.307373 -0.003990 -0.000211 -0.031311   \n...           ...       ...       ...       ...       ...       ...       ...   \n2136957  0.000429  0.000032  0.000668  0.979004  0.008644  0.000309  0.035126   \n2136958  0.000434  0.000032  0.000672  0.982910  0.008675  0.000312  0.035278   \n2136959  0.000428  0.000031  0.000650  0.969727  0.008430  0.000305  0.034637   \n\n         MACD_id10  MACD_id11  MACD_id12  MACD_id13  Volume_Ratio_id0  \\\n2005439  -0.342041  -0.066162  -0.000055  -0.000010          1.072266   \n2005440  -0.357422  -0.068237  -0.000058  -0.000011          0.937988   \n2005441  -0.373535  -0.070435  -0.000060  -0.000011          2.011719   \n...            ...        ...        ...        ...               ...   \n2136957   0.418213   0.054779   0.000068   0.000012          0.530762   \n2136958   0.417480   0.055115   0.000068   0.000012          0.394531   \n2136959   0.416504   0.054413   0.000066   0.000012          1.596680   \n\n         Volume_Ratio_id1  Volume_Ratio_id2  Volume_Ratio_id3  \\\n2005439          1.370117          4.578125          1.272461   \n2005440          0.925781          0.517090          0.931152   \n2005441          0.998535          3.535156          4.335938   \n...                   ...               ...               ...   \n2136957          0.644043          0.851562          1.186523   \n2136958          0.555176          0.331055          0.971680   \n2136959          1.416992          0.914551          0.688965   \n\n         Volume_Ratio_id4  Volume_Ratio_id5  Volume_Ratio_id6  \\\n2005439          0.949219          5.480469          1.961914   \n2005440          0.403564          1.964844          0.814453   \n2005441          0.708984          1.347656          2.224609   \n...                   ...               ...               ...   \n2136957          0.598145          0.450439          0.832031   \n2136958          0.585938          0.440186          0.757812   \n2136959          0.812500          6.011719          1.068359   \n\n         Volume_Ratio_id7  Volume_Ratio_id8  Volume_Ratio_id9  \\\n2005439          1.569336          0.624512          2.337891   \n2005440          1.745117          0.664062          1.288086   \n2005441          3.621094          1.430664          6.351562   \n...                   ...               ...               ...   \n2136957          0.422607          1.555664          0.582031   \n2136958          0.761719          0.686035          0.243896   \n2136959          2.173828          0.456543          0.916992   \n\n         Volume_Ratio_id10  Volume_Ratio_id11  Volume_Ratio_id12  \\\n2005439           1.936523           0.794922           2.705078   \n2005440           0.237183           0.933105           1.598633   \n2005441           0.474609           0.955566           1.542969   \n...                    ...                ...                ...   \n2136957           0.348633           0.171143           2.171875   \n2136958           0.648438           0.405029           0.609863   \n2136959           0.686523           0.591309           8.945312   \n\n         Volume_Ratio_id13   RSI_id0   RSI_id1   RSI_id2   RSI_id3   RSI_id4  \\\n2005439           2.023438  45.71875  46.09375  42.15625  45.31250  51.37500   \n2005440           2.083984  45.71875  46.03125  41.78125  45.06250  51.56250   \n2005441           1.884766  45.46875  46.03125  41.93750  44.84375  51.43750   \n...                    ...       ...       ...       ...       ...       ...   \n2136957           0.782715  55.65625  56.84375  56.75000  55.06250  53.84375   \n2136958           0.556152  55.75000  56.96875  57.00000  55.43750  54.06250   \n2136959           0.541992  54.84375  56.31250  56.06250  54.53125  53.18750   \n\n          RSI_id5   RSI_id6  RSI_id7   RSI_id8   RSI_id9  RSI_id10  RSI_id11  \\\n2005439  42.18750  45.59375  45.0625  45.56250  42.40625  41.37500  46.12500   \n2005440  42.31250  45.50000  45.3125  47.71875  42.56250  41.37500  45.65625   \n2005441  42.31250  45.50000  45.1875  45.06250  42.21875  41.37500  45.65625   \n...           ...       ...      ...       ...       ...       ...       ...   \n2136957  54.62500  56.87500  55.5625  53.09375  55.81250  52.62500  53.34375   \n2136958  54.84375  57.00000  55.5625  53.37500  55.93750  52.71875  53.71875   \n2136959  53.71875  56.34375  54.1250  52.56250  55.09375  52.31250  53.06250   \n\n         RSI_id12  RSI_id13  log_close/mean_60_id0  log_return_60_id0  \\\n2005439  44.12500  41.46875              -0.002672          -0.004894   \n2005440  43.87500  41.46875              -0.002602          -0.004406   \n2005441  44.00000  41.18750              -0.003565          -0.005878   \n...           ...       ...                    ...                ...   \n2136957  55.53125  54.21875               0.007030           0.035492   \n2136958  55.75000  54.40625               0.005821           0.033600   \n2136959  54.75000  54.06250               0.002691           0.031708   \n\n         log_close/mean_300_id0  log_return_300_id0  log_close/mean_900_id0  \\\n2005439               -0.007069           -0.015251               -0.016434   \n2005440               -0.007015           -0.015251               -0.016403   \n2005441               -0.008026           -0.016251               -0.017426   \n...                         ...                 ...                     ...   \n2136957                0.036987            0.040192                0.020447   \n2136958                0.036224            0.041138                0.019791   \n2136959                0.033478            0.034546                0.017197   \n\n         log_return_900_id0  log_close/mean_60_id1  log_return_60_id1  \\\n2005439           -0.027725              -0.001974          -0.004112   \n2005440           -0.027222              -0.002298          -0.004620   \n2005441           -0.028229              -0.002623          -0.005299   \n...                     ...                    ...                ...   \n2136957            0.001951               0.002705           0.020645   \n2136958           -0.002445               0.002371           0.020798   \n2136959           -0.004406               0.000519           0.020889   \n\n         log_close/mean_300_id1  log_return_300_id1  log_close/mean_900_id1  \\\n2005439               -0.007874           -0.017975               -0.014359   \n2005440               -0.008209           -0.018341               -0.014740   \n2005441               -0.008560           -0.018417               -0.015129   \n...                         ...                 ...                     ...   \n2136957                0.024490            0.035980                0.021210   \n2136958                0.024384            0.036499                0.021210   \n2136959                0.022766            0.033142                0.019684   \n\n         log_return_900_id1  log_close/mean_60_id2  log_return_60_id2  \\\n2005439           -0.020294              -0.005310          -0.010803   \n2005440           -0.020386              -0.005939          -0.011292   \n2005441           -0.020966              -0.005733          -0.012283   \n...                     ...                    ...                ...   \n2136957            0.015366               0.008522           0.038330   \n2136958            0.011810               0.007881           0.038330   \n2136959            0.011513               0.005604           0.038330   \n\n         log_close/mean_300_id2  log_return_300_id2  log_close/mean_900_id2  \\\n2005439               -0.016815           -0.035278               -0.025009   \n2005440               -0.017517           -0.037323               -0.025803   \n2005441               -0.017395           -0.036285               -0.025757   \n...                         ...                 ...                     ...   \n2136957                0.038910            0.050476                0.034485   \n2136958                0.038727            0.052307                0.034485   \n2136959                0.036926            0.044891                0.032806   \n\n         log_return_900_id2  log_close/mean_60_id3  log_return_60_id3  \\\n2005439           -0.033264              -0.003073          -0.004406   \n2005440           -0.033752              -0.003906          -0.005386   \n2005441           -0.034790              -0.004715          -0.007351   \n...                     ...                    ...                ...   \n2136957            0.021255               0.006191           0.037384   \n2136958            0.017426               0.007259           0.041138   \n2136959            0.016464               0.003155           0.038330   \n\n         log_close/mean_300_id3  log_return_300_id3  log_close/mean_900_id3  \\\n2005439               -0.008156           -0.016739               -0.016357   \n2005440               -0.009026           -0.017731               -0.017258   \n2005441               -0.009895           -0.018234               -0.018158   \n...                         ...                 ...                     ...   \n2136957                0.040100            0.047668                0.018402   \n2136958                0.041687            0.051392                0.020142   \n2136959                0.038055            0.042999                0.016663   \n\n         log_return_900_id3  log_close/mean_60_id4  log_return_60_id4  \\\n2005439           -0.025711              -0.011513          -0.029739   \n2005440           -0.026718              -0.008171          -0.026718   \n2005441           -0.027725              -0.010544          -0.031250   \n...                     ...                    ...                ...   \n2136957            0.001951               0.003750           0.022217   \n2136958           -0.001955               0.003363           0.023163   \n2136959           -0.003424              -0.000446           0.022217   \n\n         log_close/mean_300_id4  log_return_300_id4  log_close/mean_900_id4  \\\n2005439               -0.021225            0.021255                0.002699   \n2005440               -0.018402            0.024124                0.005581   \n2005441               -0.021378            0.022217                0.002680   \n...                         ...                 ...                     ...   \n2136957                0.026413            0.034546                0.022949   \n2136958                0.026291            0.035492                0.022949   \n2136959                0.022751            0.027924                0.019485   \n\n         log_return_900_id4  log_close/mean_60_id5  log_return_60_id5  \\\n2005439            0.006813              -0.009712          -0.023712   \n2005440            0.010681              -0.008507          -0.022720   \n2005441            0.005844              -0.008934          -0.023712   \n...                     ...                    ...                ...   \n2136957            0.013580               0.005184           0.029831   \n2136958            0.010681               0.005550           0.029831   \n2136959            0.008751               0.000762           0.029831   \n\n         log_close/mean_300_id5  log_return_300_id5  log_close/mean_900_id5  \\\n2005439               -0.030365           -0.056763               -0.026169   \n2005440               -0.029358           -0.055725               -0.025345   \n2005441               -0.029999           -0.055725               -0.026169   \n...                         ...                 ...                     ...   \n2136957                0.029572            0.035492                0.020981   \n2136958                0.030304            0.037384                0.021835   \n2136959                0.025894            0.028870                0.017532   \n\n         log_return_900_id5  log_close/mean_60_id6  log_return_60_id6  \\\n2005439            0.002926              -0.004341          -0.006367   \n2005440            0.003899              -0.004726          -0.006859   \n2005441            0.002926              -0.005577          -0.008827   \n...                     ...                    ...                ...   \n2136957            0.003899               0.008240           0.037384   \n2136958            0.000976               0.007607           0.038330   \n2136959           -0.001955               0.004623           0.037384   \n\n         log_close/mean_300_id6  log_return_300_id6  log_close/mean_900_id6  \\\n2005439               -0.010208           -0.023224               -0.022064   \n2005440               -0.010628           -0.023712               -0.022522   \n2005441               -0.011551           -0.023712               -0.023483   \n...                         ...                 ...                     ...   \n2136957                0.041626            0.050476                0.031494   \n2136958                0.041443            0.050476                0.031494   \n2136959                0.038940            0.045807                0.029114   \n\n         log_return_900_id6  log_close/mean_60_id7  log_return_60_id7  \\\n2005439           -0.035797              -0.004593          -0.009323   \n2005440           -0.036285              -0.003857          -0.009323   \n2005441           -0.037323              -0.004833          -0.010803   \n...                     ...                    ...                ...   \n2136957            0.015503               0.007595           0.034546   \n2136958            0.008751               0.006382           0.036438   \n2136959            0.008751               0.001489           0.032654   \n\n         log_close/mean_300_id7  log_return_300_id7  log_close/mean_900_id7  \\\n2005439               -0.010254           -0.022217               -0.019638   \n2005440               -0.009605           -0.021225               -0.019012   \n2005441               -0.010689           -0.021713               -0.020142   \n...                         ...                 ...                     ...   \n2136957                0.037842            0.053253                0.031769   \n2136958                0.037048            0.053253                0.031128   \n2136959                0.032562            0.039246                0.026749   \n\n         log_return_900_id7  log_close/mean_60_id8  log_return_60_id8  \\\n2005439           -0.031738              -0.007305          -0.015251   \n2005440           -0.031250              -0.007812          -0.016251   \n2005441           -0.032745              -0.009834          -0.019226   \n...                     ...                    ...                ...   \n2136957            0.019348               0.012558           0.042084   \n2136958            0.013580               0.013618           0.044891   \n2136959            0.011650               0.008713           0.042999   \n\n         log_close/mean_300_id8  log_return_300_id8  log_close/mean_900_id8  \\\n2005439               -0.017853           -0.070312               -0.027740   \n2005440               -0.018509           -0.035278               -0.028458   \n2005441               -0.020615           -0.071838               -0.030762   \n...                         ...                 ...                     ...   \n2136957                0.034271            0.028870                0.016159   \n2136958                0.035980            0.030777                0.017975   \n2136959                0.031708            0.024124                0.013802   \n\n         log_return_900_id8  log_close/mean_60_id9  log_return_60_id9  \\\n2005439           -0.027725              -0.007298          -0.015251   \n2005440           -0.061951              -0.006397          -0.014259   \n2005441           -0.028732              -0.009399          -0.018234   \n...                     ...                    ...                ...   \n2136957           -0.012283               0.007431           0.033600   \n2136958           -0.014755               0.007412           0.035492   \n2136959           -0.016739               0.004063           0.033600   \n\n         log_close/mean_300_id9  log_return_300_id9  log_close/mean_900_id9  \\\n2005439               -0.018951           -0.036804               -0.031372   \n2005440               -0.018173           -0.036285               -0.030670   \n2005441               -0.021347           -0.038330               -0.033936   \n...                         ...                 ...                     ...   \n2136957                0.032318            0.048615                0.031097   \n2136958                0.032715            0.049530                0.031647   \n2136959                0.029785            0.042084                0.028824   \n\n         log_return_900_id9  log_close/mean_60_id10  log_return_60_id10  \\\n2005439           -0.035797               -0.007019           -0.013763   \n2005440           -0.035278               -0.007614           -0.013763   \n2005441           -0.038818               -0.008194           -0.014755   \n...                     ...                     ...                 ...   \n2136957            0.024124                0.000755            0.025070   \n2136958            0.019348                0.000828            0.028870   \n2136959            0.018387                0.000373            0.027924   \n\n         log_close/mean_300_id10  log_return_300_id10  \\\n2005439                -0.019409            -0.034790   \n2005440                -0.020126            -0.035278   \n2005441                -0.020828            -0.035278   \n...                          ...                  ...   \n2136957                 0.019608             0.021255   \n2136958                 0.020081             0.021255   \n2136959                 0.020020             0.018387   \n\n         log_close/mean_900_id10  log_return_900_id10  log_close/mean_60_id11  \\\n2005439                -0.032257            -0.043915               -0.005642   \n2005440                -0.033051            -0.045441               -0.007214   \n2005441                -0.033813            -0.045441               -0.007858   \n...                          ...                  ...                     ...   \n2136957                -0.004524            -0.031738                0.008331   \n2136958                -0.003941            -0.033752                0.010132   \n2136959                -0.003902            -0.034790                0.007202   \n\n         log_return_60_id11  log_close/mean_300_id11  log_return_300_id11  \\\n2005439           -0.013763                -0.019226            -0.021713   \n2005440           -0.015251                -0.020966            -0.024216   \n2005441           -0.016251                -0.021805            -0.024719   \n...                     ...                      ...                  ...   \n2136957            0.030777                 0.033112             0.027924   \n2136958            0.035492                 0.035370             0.031708   \n2136959            0.032654                 0.032898             0.026031   \n\n         log_close/mean_900_id11  log_return_900_id11  log_close/mean_60_id12  \\\n2005439                -0.011093             0.022217               -0.005791   \n2005440                -0.012947             0.021255               -0.006859   \n2005441                -0.013885             0.019348               -0.006611   \n...                          ...                  ...                     ...   \n2136957                 0.022232            -0.001466                0.008842   \n2136958                 0.024628            -0.003424                0.008858   \n2136959                 0.022232            -0.003914                0.004044   \n\n         log_return_60_id12  log_close/mean_300_id12  log_return_300_id12  \\\n2005439           -0.012779                -0.014549            -0.027222   \n2005440           -0.014259                -0.015762            -0.028229   \n2005441           -0.014755                -0.015671            -0.027725   \n...                     ...                      ...                  ...   \n2136957            0.035492                 0.035797             0.043945   \n2136958            0.035492                 0.036255             0.045807   \n2136959            0.033600                 0.031860             0.038330   \n\n         log_close/mean_900_id12  log_return_900_id12  log_close/mean_60_id13  \\\n2005439                -0.019928            -0.012779               -0.004208   \n2005440                -0.021210            -0.014755               -0.004044   \n2005441                -0.021194            -0.014755               -0.005089   \n...                          ...                  ...                     ...   \n2136957                 0.032471             0.025070                0.000345   \n2136958                 0.033051             0.021255                0.000504   \n2136959                 0.028778             0.019348               -0.000911   \n\n         log_return_60_id13  log_close/mean_300_id13  log_return_300_id13  \\\n2005439           -0.010307                -0.013206            -0.029739   \n2005440           -0.009811                -0.013107            -0.029739   \n2005441           -0.011787                -0.014244            -0.031250   \n...                     ...                      ...                  ...   \n2136957            0.021255                 0.019852             0.026031   \n2136958            0.022217                 0.020294             0.026978   \n2136959            0.021255                 0.019150             0.025070   \n\n         log_close/mean_900_id13  log_return_900_id13  mean_close/mean_60  \\\n2005439                -0.016266            -0.019730           -0.005745   \n2005440                -0.016235            -0.020218           -0.005711   \n2005441                -0.017456            -0.021713           -0.006680   \n...                          ...                  ...                 ...   \n2136957                 0.011200            -0.009323            0.006248   \n2136958                 0.011749            -0.012283            0.006256   \n2136959                 0.010696            -0.011787            0.002991   \n\n         mean_log_returns_60  log_close/mean_60-mean_close/mean_60_id0  \\\n2005439            -0.012459                                  0.003073   \n2005440            -0.012497                                  0.003109   \n2005441            -0.014313                                  0.003115   \n...                      ...                                       ...   \n2136957             0.031708                                  0.000781   \n2136958             0.033142                                 -0.000434   \n2136959             0.031677                                 -0.000301   \n\n         log_return_60-mean_log_returns_60_id0  \\\n2005439                               0.007568   \n2005440                               0.008087   \n2005441                               0.008438   \n...                                        ...   \n2136957                               0.003771   \n2136958                               0.000451   \n2136959                               0.000038   \n\n         log_close/mean_60-mean_close/mean_60_id1  \\\n2005439                                  0.003771   \n2005440                                  0.003412   \n2005441                                  0.004059   \n...                                           ...   \n2136957                                 -0.003544   \n2136958                                 -0.003885   \n2136959                                 -0.002472   \n\n         log_return_60-mean_log_returns_60_id1  \\\n2005439                               0.008354   \n2005440                               0.007874   \n2005441                               0.009018   \n...                                        ...   \n2136957                              -0.011078   \n2136958                              -0.012344   \n2136959                              -0.010773   \n\n         log_close/mean_60-mean_close/mean_60_id2  \\\n2005439                                  0.000437   \n2005440                                 -0.000228   \n2005441                                  0.000945   \n...                                           ...   \n2136957                                  0.002272   \n2136958                                  0.001626   \n2136959                                  0.002611   \n\n         log_return_60-mean_log_returns_60_id2  \\\n2005439                               0.001659   \n2005440                               0.001203   \n2005441                               0.002031   \n...                                        ...   \n2136957                               0.006607   \n2136958                               0.005180   \n2136959                               0.006660   \n\n         log_close/mean_60-mean_close/mean_60_id3  \\\n2005439                                  0.002674   \n2005440                                  0.001803   \n2005441                                  0.001966   \n...                                           ...   \n2136957                                 -0.000056   \n2136958                                  0.001004   \n2136959                                  0.000164   \n\n         log_return_60-mean_log_returns_60_id3  \\\n2005439                               0.008057   \n2005440                               0.007107   \n2005441                               0.006966   \n...                                        ...   \n2136957                               0.005661   \n2136958                               0.007988   \n2136959                               0.006660   \n\n         log_close/mean_60-mean_close/mean_60_id4  \\\n2005439                                 -0.005764   \n2005440                                 -0.002460   \n2005441                                 -0.003860   \n...                                           ...   \n2136957                                 -0.002499   \n2136958                                 -0.002893   \n2136959                                 -0.003437   \n\n         log_return_60-mean_log_returns_60_id4  \\\n2005439                              -0.017273   \n2005440                              -0.014221   \n2005441                              -0.016937   \n...                                        ...   \n2136957                              -0.009506   \n2136958                              -0.009987   \n2136959                              -0.009453   \n\n         log_close/mean_60-mean_close/mean_60_id5  \\\n2005439                                 -0.003963   \n2005440                                 -0.002798   \n2005441                                 -0.002256   \n...                                           ...   \n2136957                                 -0.001066   \n2136958                                 -0.000704   \n2136959                                 -0.002230   \n\n         log_return_60-mean_log_returns_60_id5  \\\n2005439                              -0.011253   \n2005440                              -0.010223   \n2005441                              -0.009399   \n...                                        ...   \n2136957                              -0.001891   \n2136958                              -0.003319   \n2136959                              -0.001839   \n\n         log_close/mean_60-mean_close/mean_60_id6  \\\n2005439                                  0.001403   \n2005440                                  0.000984   \n2005441                                  0.001101   \n...                                           ...   \n2136957                                  0.001993   \n2136958                                  0.001352   \n2136959                                  0.001633   \n\n         log_return_60-mean_log_returns_60_id6  \\\n2005439                               0.006096   \n2005440                               0.005634   \n2005441                               0.005489   \n...                                        ...   \n2136957                               0.005661   \n2136958                               0.005180   \n2136959                               0.005714   \n\n         log_close/mean_60-mean_close/mean_60_id7  \\\n2005439                                  0.001155   \n2005440                                  0.001854   \n2005441                                  0.001845   \n...                                           ...   \n2136957                                  0.001348   \n2136958                                  0.000124   \n2136959                                 -0.001503   \n\n         log_return_60-mean_log_returns_60_id7  \\\n2005439                               0.003139   \n2005440                               0.003172   \n2005441                               0.003511   \n...                                        ...   \n2136957                               0.002825   \n2136958                               0.003288   \n2136959                               0.000984   \n\n         log_close/mean_60-mean_close/mean_60_id8  \\\n2005439                                 -0.001559   \n2005440                                 -0.002104   \n2005441                                 -0.003157   \n...                                           ...   \n2136957                                  0.006310   \n2136958                                  0.007362   \n2136959                                  0.005722   \n\n         log_return_60-mean_log_returns_60_id8  \\\n2005439                              -0.002789   \n2005440                              -0.003756   \n2005441                              -0.004910   \n...                                        ...   \n2136957                               0.010361   \n2136958                               0.011742   \n2136959                               0.011330   \n\n         log_close/mean_60-mean_close/mean_60_id9  \\\n2005439                                 -0.001553   \n2005440                                 -0.000686   \n2005441                                 -0.002720   \n...                                           ...   \n2136957                                  0.001184   \n2136958                                  0.001154   \n2136959                                  0.001072   \n\n         log_return_60-mean_log_returns_60_id9  \\\n2005439                              -0.002789   \n2005440                              -0.001765   \n2005441                              -0.003918   \n...                                        ...   \n2136957                               0.001878   \n2136958                               0.002342   \n2136959                               0.001930   \n\n         log_close/mean_60-mean_close/mean_60_id10  \\\n2005439                                  -0.001271   \n2005440                                  -0.001902   \n2005441                                  -0.001516   \n...                                            ...   \n2136957                                  -0.005493   \n2136958                                  -0.005428   \n2136959                                  -0.002619   \n\n         log_return_60-mean_log_returns_60_id10  \\\n2005439                               -0.001301   \n2005440                               -0.001269   \n2005441                               -0.000440   \n...                                         ...   \n2136957                               -0.006653   \n2136958                               -0.004280   \n2136959                               -0.003746   \n\n         log_close/mean_60-mean_close/mean_60_id11  \\\n2005439                                   0.000105   \n2005440                                  -0.001504   \n2005441                                  -0.001179   \n...                                            ...   \n2136957                                   0.002079   \n2136958                                   0.003874   \n2136959                                   0.004211   \n\n         log_return_60-mean_log_returns_60_id11  \\\n2005439                               -0.001301   \n2005440                               -0.002756   \n2005441                               -0.001936   \n...                                         ...   \n2136957                               -0.000945   \n2136958                                0.002342   \n2136959                                0.000984   \n\n         log_close/mean_60-mean_close/mean_60_id12  \\\n2005439                                  -0.000043   \n2005440                                  -0.001148   \n2005441                                   0.000070   \n...                                            ...   \n2136957                                   0.002594   \n2136958                                   0.002602   \n2136959                                   0.001054   \n\n         log_return_60-mean_log_returns_60_id12  \\\n2005439                               -0.000317   \n2005440                               -0.001765   \n2005441                               -0.000440   \n...                                         ...   \n2136957                                0.003771   \n2136958                                0.002342   \n2136959                                0.001930   \n\n         log_close/mean_60-mean_close/mean_60_id13  \\\n2005439                                   0.001536   \n2005440                                   0.001666   \n2005441                                   0.001589   \n...                                            ...   \n2136957                                  -0.005905   \n2136958                                  -0.005753   \n2136959                                  -0.003902   \n\n         log_return_60-mean_log_returns_60_id13  mean_close/mean_300  \\\n2005439                                0.002155            -0.015366   \n2005440                                0.002684            -0.015457   \n2005441                                0.002527            -0.016571   \n...                                         ...                  ...   \n2136957                               -0.010468             0.032196   \n2136958                               -0.010933             0.032623   \n2136959                               -0.010414             0.029770   \n\n         mean_log_returns_300  log_close/mean_300-mean_close/mean_300_id0  \\\n2005439             -0.027634                                    0.008301   \n2005440             -0.025299                                    0.008438   \n2005441             -0.028381                                    0.008553   \n...                       ...                                         ...   \n2136957              0.038910                                    0.004791   \n2136958              0.040283                                    0.003582   \n2136959              0.033661                                    0.003716   \n\n         log_return_300-mean_log_returns_300_id0  \\\n2005439                                 0.012375   \n2005440                                 0.010048   \n2005441                                 0.012123   \n...                                          ...   \n2136957                                 0.001282   \n2136958                                 0.000852   \n2136959                                 0.000871   \n\n         log_close/mean_300-mean_close/mean_300_id1  \\\n2005439                                    0.007496   \n2005440                                    0.007248   \n2005441                                    0.008011   \n...                                             ...   \n2136957                                   -0.007713   \n2136958                                   -0.008240   \n2136959                                   -0.007004   \n\n         log_return_300-mean_log_returns_300_id1  \\\n2005439                                 0.009651   \n2005440                                 0.006958   \n2005441                                 0.009949   \n...                                          ...   \n2136957                                -0.002918   \n2136958                                -0.003782   \n2136959                                -0.000531   \n\n         log_close/mean_300-mean_close/mean_300_id2  \\\n2005439                                   -0.001447   \n2005440                                   -0.002054   \n2005441                                   -0.000819   \n...                                             ...   \n2136957                                    0.006699   \n2136958                                    0.006104   \n2136959                                    0.007156   \n\n         log_return_300-mean_log_returns_300_id2  \\\n2005439                                -0.007652   \n2005440                                -0.012024   \n2005441                                -0.007912   \n...                                          ...   \n2136957                                 0.011566   \n2136958                                 0.012024   \n2136959                                 0.011215   \n\n         log_close/mean_300-mean_close/mean_300_id3  \\\n2005439                                    0.007210   \n2005440                                    0.006432   \n2005441                                    0.006676   \n...                                             ...   \n2136957                                    0.007904   \n2136958                                    0.009048   \n2136959                                    0.008286   \n\n         log_return_300-mean_log_returns_300_id3  \\\n2005439                                 0.010887   \n2005440                                 0.007568   \n2005441                                 0.010139   \n...                                          ...   \n2136957                                 0.008759   \n2136958                                 0.011108   \n2136959                                 0.009323   \n\n         log_close/mean_300-mean_close/mean_300_id4  \\\n2005439                                   -0.005852   \n2005440                                   -0.002945   \n2005441                                   -0.004799   \n...                                             ...   \n2136957                                   -0.005798   \n2136958                                   -0.006336   \n2136959                                   -0.007023   \n\n         log_return_300-mean_log_returns_300_id4  \\\n2005439                                 0.048889   \n2005440                                 0.049438   \n2005441                                 0.050598   \n...                                          ...   \n2136957                                -0.004364   \n2136958                                -0.004795   \n2136959                                -0.005753   \n\n         log_close/mean_300-mean_close/mean_300_id5  \\\n2005439                                   -0.015007   \n2005440                                   -0.013908   \n2005441                                   -0.013428   \n...                                             ...   \n2136957                                   -0.002642   \n2136958                                   -0.002329   \n2136959                                   -0.003868   \n\n         log_return_300-mean_log_returns_300_id5  \\\n2005439                                -0.029129   \n2005440                                -0.030426   \n2005441                                -0.027344   \n...                                          ...   \n2136957                                -0.003418   \n2136958                                -0.002901   \n2136959                                -0.004807   \n\n         log_close/mean_300-mean_close/mean_300_id6  \\\n2005439                                    0.005161   \n2005440                                    0.004829   \n2005441                                    0.005020   \n...                                             ...   \n2136957                                    0.009407   \n2136958                                    0.008820   \n2136959                                    0.009155   \n\n         log_return_300-mean_log_returns_300_id6  \\\n2005439                                 0.004402   \n2005440                                 0.001589   \n2005441                                 0.004665   \n...                                          ...   \n2136957                                 0.011566   \n2136958                                 0.010193   \n2136959                                 0.012131   \n\n         log_close/mean_300-mean_close/mean_300_id7  \\\n2005439                                    0.005112   \n2005440                                    0.005856   \n2005441                                    0.005882   \n...                                             ...   \n2136957                                    0.005646   \n2136958                                    0.004429   \n2136959                                    0.002794   \n\n         log_return_300-mean_log_returns_300_id7  \\\n2005439                                 0.005409   \n2005440                                 0.004074   \n2005441                                 0.006664   \n...                                          ...   \n2136957                                 0.014343   \n2136958                                 0.012970   \n2136959                                 0.005569   \n\n         log_close/mean_300-mean_close/mean_300_id8  \\\n2005439                                   -0.002478   \n2005440                                   -0.003050   \n2005441                                   -0.004040   \n...                                             ...   \n2136957                                    0.002073   \n2136958                                    0.003347   \n2136959                                    0.001926   \n\n         log_return_300-mean_log_returns_300_id8  \\\n2005439                                -0.042694   \n2005440                                -0.009979   \n2005441                                -0.043457   \n...                                          ...   \n2136957                                -0.010040   \n2136958                                -0.009506   \n2136959                                -0.009552   \n\n         log_close/mean_300-mean_close/mean_300_id9  \\\n2005439                                   -0.003588   \n2005440                                   -0.002716   \n2005441                                   -0.004784   \n...                                             ...   \n2136957                                    0.000113   \n2136958                                    0.000084   \n2136959                                    0.000014   \n\n         log_return_300-mean_log_returns_300_id9  \\\n2005439                                -0.009178   \n2005440                                -0.010986   \n2005441                                -0.009956   \n...                                          ...   \n2136957                                 0.009705   \n2136958                                 0.009247   \n2136959                                 0.008408   \n\n         log_close/mean_300-mean_close/mean_300_id10  \\\n2005439                                    -0.004040   \n2005440                                    -0.004662   \n2005441                                    -0.004261   \n...                                              ...   \n2136957                                    -0.012604   \n2136958                                    -0.012550   \n2136959                                    -0.009750   \n\n         log_return_300-mean_log_returns_300_id10  \\\n2005439                                 -0.007164   \n2005440                                 -0.009979   \n2005441                                 -0.006901   \n...                                           ...   \n2136957                                 -0.017654   \n2136958                                 -0.019028   \n2136959                                 -0.015289   \n\n         log_close/mean_300-mean_close/mean_300_id11  \\\n2005439                                    -0.003855   \n2005440                                    -0.005516   \n2005441                                    -0.005238   \n...                                              ...   \n2136957                                     0.000888   \n2136958                                     0.002756   \n2136959                                     0.003132   \n\n         log_return_300-mean_log_returns_300_id11  \\\n2005439                                  0.005913   \n2005440                                  0.001085   \n2005441                                  0.003656   \n...                                           ...   \n2136957                                 -0.010986   \n2136958                                 -0.008575   \n2136959                                 -0.007645   \n\n         log_close/mean_300-mean_close/mean_300_id12  \\\n2005439                                     0.000816   \n2005440                                    -0.000307   \n2005441                                     0.000899   \n...                                              ...   \n2136957                                     0.003588   \n2136958                                     0.003622   \n2136959                                     0.002090   \n\n         log_return_300-mean_log_returns_300_id12  \\\n2005439                                  0.000405   \n2005440                                 -0.002928   \n2005441                                  0.000651   \n...                                           ...   \n2136957                                  0.005035   \n2136958                                  0.005520   \n2136959                                  0.004654   \n\n         log_close/mean_300-mean_close/mean_300_id13  \\\n2005439                                     0.002161   \n2005440                                     0.002352   \n2005441                                     0.002329   \n...                                              ...   \n2136957                                    -0.012352   \n2136958                                    -0.012329   \n2136959                                    -0.010620   \n\n         log_return_300-mean_log_returns_300_id13  mean_close/mean_900  \\\n2005439                                 -0.002113            -0.019714   \n2005440                                 -0.004440            -0.019867   \n2005441                                 -0.002874            -0.021042   \n...                                           ...                  ...   \n2136957                                 -0.012878             0.022171   \n2136958                                 -0.013306             0.022720   \n2136959                                 -0.008606             0.019974   \n\n         mean_log_returns_900  log_close/mean_900-mean_close/mean_900_id0  \\\n2005439             -0.020172                                    0.003283   \n2005440             -0.022675                                    0.003462   \n2005441             -0.021652                                    0.003618   \n...                       ...                                         ...   \n2136957              0.006233                                   -0.001726   \n2136958              0.002516                                   -0.002926   \n2136959              0.001275                                   -0.002783   \n\n         log_return_900-mean_log_returns_900_id0  \\\n2005439                                -0.007545   \n2005440                                -0.004547   \n2005441                                -0.006577   \n...                                          ...   \n2136957                                -0.004280   \n2136958                                -0.004959   \n2136959                                -0.005680   \n\n         log_close/mean_900-mean_close/mean_900_id1  \\\n2005439                                    0.005352   \n2005440                                    0.005123   \n2005441                                    0.005920   \n...                                             ...   \n2136957                                   -0.000965   \n2136958                                   -0.001519   \n2136959                                   -0.000294   \n\n         log_return_900-mean_log_returns_900_id1  \\\n2005439                                -0.000108   \n2005440                                 0.002281   \n2005441                                 0.000686   \n...                                          ...   \n2136957                                 0.009140   \n2136958                                 0.009300   \n2136959                                 0.010239   \n\n         log_close/mean_900-mean_close/mean_900_id2  \\\n2005439                                   -0.005302   \n2005440                                   -0.005936   \n2005441                                   -0.004715   \n...                                             ...   \n2136957                                    0.012321   \n2136958                                    0.011749   \n2136959                                    0.012825   \n\n         log_return_900-mean_log_returns_900_id2  \\\n2005439                                -0.013084   \n2005440                                -0.011078   \n2005441                                -0.013138   \n...                                          ...   \n2136957                                 0.015022   \n2136958                                 0.014908   \n2136959                                 0.015190   \n\n         log_close/mean_900-mean_close/mean_900_id3  \\\n2005439                                    0.003355   \n2005440                                    0.002605   \n2005441                                    0.002892   \n...                                             ...   \n2136957                                   -0.003769   \n2136958                                   -0.002583   \n2136959                                   -0.003311   \n\n         log_return_900-mean_log_returns_900_id3  \\\n2005439                                -0.005531   \n2005440                                -0.004044   \n2005441                                -0.006073   \n...                                          ...   \n2136957                                -0.004280   \n2136958                                -0.004471   \n2136959                                -0.004700   \n\n         log_close/mean_900-mean_close/mean_900_id4  \\\n2005439                                    0.022415   \n2005440                                    0.025452   \n2005441                                    0.023727   \n...                                             ...   \n2136957                                    0.000784   \n2136958                                    0.000221   \n2136959                                   -0.000492   \n\n         log_return_900-mean_log_returns_900_id4  \\\n2005439                                 0.026993   \n2005440                                 0.033356   \n2005441                                 0.027496   \n...                                          ...   \n2136957                                 0.007347   \n2136958                                 0.008163   \n2136959                                 0.007477   \n\n         log_close/mean_900-mean_close/mean_900_id5  \\\n2005439                                   -0.006451   \n2005440                                   -0.005486   \n2005441                                   -0.005123   \n...                                             ...   \n2136957                                   -0.001193   \n2136958                                   -0.000887   \n2136959                                   -0.002441   \n\n         log_return_900-mean_log_returns_900_id5  \\\n2005439                                 0.023102   \n2005440                                 0.026566   \n2005441                                 0.024582   \n...                                          ...   \n2136957                                -0.002333   \n2136958                                -0.001539   \n2136959                                -0.003229   \n\n         log_close/mean_900-mean_close/mean_900_id6  \\\n2005439                                   -0.002350   \n2005440                                   -0.002661   \n2005441                                   -0.002438   \n...                                             ...   \n2136957                                    0.009331   \n2136958                                    0.008766   \n2136959                                    0.009140   \n\n         log_return_900-mean_log_returns_900_id6  \\\n2005439                                -0.015617   \n2005440                                -0.013611   \n2005441                                -0.015671   \n...                                          ...   \n2136957                                 0.009270   \n2136958                                 0.006237   \n2136959                                 0.007477   \n\n         log_close/mean_900-mean_close/mean_900_id7  \\\n2005439                                    0.000082   \n2005440                                    0.000846   \n2005441                                    0.000907   \n...                                             ...   \n2136957                                    0.009590   \n2136958                                    0.008400   \n2136959                                    0.006771   \n\n         log_return_900-mean_log_returns_900_id7  \\\n2005439                                -0.011559   \n2005440                                -0.008575   \n2005441                                -0.011093   \n...                                          ...   \n2136957                                 0.013115   \n2136958                                 0.011063   \n2136959                                 0.010376   \n\n         log_close/mean_900-mean_close/mean_900_id8  \\\n2005439                                   -0.008026   \n2005440                                   -0.008591   \n2005441                                   -0.009720   \n...                                             ...   \n2136957                                   -0.006004   \n2136958                                   -0.004742   \n2136959                                   -0.006176   \n\n         log_return_900-mean_log_returns_900_id8  \\\n2005439                                -0.007545   \n2005440                                -0.039276   \n2005441                                -0.007080   \n...                                          ...   \n2136957                                -0.018509   \n2136958                                -0.017273   \n2136959                                -0.018021   \n\n         log_close/mean_900-mean_close/mean_900_id9  \\\n2005439                                   -0.011658   \n2005440                                   -0.010811   \n2005441                                   -0.012894   \n...                                             ...   \n2136957                                    0.008934   \n2136958                                    0.008911   \n2136959                                    0.008850   \n\n         log_return_900-mean_log_returns_900_id9  \\\n2005439                                -0.015617   \n2005440                                -0.012604   \n2005441                                -0.017166   \n...                                          ...   \n2136957                                 0.017899   \n2136958                                 0.016830   \n2136959                                 0.017105   \n\n         log_close/mean_900-mean_close/mean_900_id10  \\\n2005439                                    -0.012550   \n2005440                                    -0.013184   \n2005441                                    -0.012779   \n...                                              ...   \n2136957                                    -0.026703   \n2136958                                    -0.026657   \n2136959                                    -0.023880   \n\n         log_return_900-mean_log_returns_900_id10  \\\n2005439                                 -0.023743   \n2005440                                 -0.022766   \n2005441                                 -0.023788   \n...                                           ...   \n2136957                                 -0.037964   \n2136958                                 -0.036255   \n2136959                                 -0.036072   \n\n         log_close/mean_900-mean_close/mean_900_id11  \\\n2005439                                     0.008621   \n2005440                                     0.006912   \n2005441                                     0.007160   \n...                                              ...   \n2136957                                     0.000061   \n2136958                                     0.001905   \n2136959                                     0.002262   \n\n         log_return_900-mean_log_returns_900_id11  \\\n2005439                                  0.042389   \n2005440                                  0.043915   \n2005441                                  0.040985   \n...                                           ...   \n2136957                                 -0.007698   \n2136958                                 -0.005939   \n2136959                                 -0.005188   \n\n         log_close/mean_900-mean_close/mean_900_id12  \\\n2005439                                    -0.000211   \n2005440                                    -0.001353   \n2005441                                    -0.000152   \n...                                              ...   \n2136957                                     0.010307   \n2136958                                     0.010338   \n2136959                                     0.008804   \n\n         log_return_900-mean_log_returns_900_id12  \\\n2005439                                  0.007401   \n2005440                                  0.007919   \n2005441                                  0.006897   \n...                                           ...   \n2136957                                  0.018845   \n2136958                                  0.018738   \n2136959                                  0.018066   \n\n         log_close/mean_900-mean_close/mean_900_id13  \\\n2005439                                     0.003452   \n2005440                                     0.003624   \n2005441                                     0.003590   \n...                                              ...   \n2136957                                    -0.010971   \n2136958                                    -0.010979   \n2136959                                    -0.009277   \n\n         log_return_900-mean_log_returns_900_id13  \n2005439                                  0.000450  \n2005440                                  0.002455  \n2005441                                 -0.000062  \n...                                           ...  \n2136957                                 -0.015556  \n2136958                                 -0.014801  \n2136959                                 -0.013062  \n\n[131521 rows x 231 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>Target_0</th>\n      <th>Target_1</th>\n      <th>Target_2</th>\n      <th>Target_3</th>\n      <th>Target_4</th>\n      <th>Target_5</th>\n      <th>Target_6</th>\n      <th>Target_7</th>\n      <th>Target_8</th>\n      <th>Target_9</th>\n      <th>Target_10</th>\n      <th>Target_11</th>\n      <th>Target_12</th>\n      <th>Target_13</th>\n      <th>MACD_id0</th>\n      <th>MACD_id1</th>\n      <th>MACD_id2</th>\n      <th>MACD_id3</th>\n      <th>MACD_id4</th>\n      <th>MACD_id5</th>\n      <th>MACD_id6</th>\n      <th>MACD_id7</th>\n      <th>MACD_id8</th>\n      <th>MACD_id9</th>\n      <th>MACD_id10</th>\n      <th>MACD_id11</th>\n      <th>MACD_id12</th>\n      <th>MACD_id13</th>\n      <th>Volume_Ratio_id0</th>\n      <th>Volume_Ratio_id1</th>\n      <th>Volume_Ratio_id2</th>\n      <th>Volume_Ratio_id3</th>\n      <th>Volume_Ratio_id4</th>\n      <th>Volume_Ratio_id5</th>\n      <th>Volume_Ratio_id6</th>\n      <th>Volume_Ratio_id7</th>\n      <th>Volume_Ratio_id8</th>\n      <th>Volume_Ratio_id9</th>\n      <th>Volume_Ratio_id10</th>\n      <th>Volume_Ratio_id11</th>\n      <th>Volume_Ratio_id12</th>\n      <th>Volume_Ratio_id13</th>\n      <th>RSI_id0</th>\n      <th>RSI_id1</th>\n      <th>RSI_id2</th>\n      <th>RSI_id3</th>\n      <th>RSI_id4</th>\n      <th>RSI_id5</th>\n      <th>RSI_id6</th>\n      <th>RSI_id7</th>\n      <th>RSI_id8</th>\n      <th>RSI_id9</th>\n      <th>RSI_id10</th>\n      <th>RSI_id11</th>\n      <th>RSI_id12</th>\n      <th>RSI_id13</th>\n      <th>log_close/mean_60_id0</th>\n      <th>log_return_60_id0</th>\n      <th>log_close/mean_300_id0</th>\n      <th>log_return_300_id0</th>\n      <th>log_close/mean_900_id0</th>\n      <th>log_return_900_id0</th>\n      <th>log_close/mean_60_id1</th>\n      <th>log_return_60_id1</th>\n      <th>log_close/mean_300_id1</th>\n      <th>log_return_300_id1</th>\n      <th>log_close/mean_900_id1</th>\n      <th>log_return_900_id1</th>\n      <th>log_close/mean_60_id2</th>\n      <th>log_return_60_id2</th>\n      <th>log_close/mean_300_id2</th>\n      <th>log_return_300_id2</th>\n      <th>log_close/mean_900_id2</th>\n      <th>log_return_900_id2</th>\n      <th>log_close/mean_60_id3</th>\n      <th>log_return_60_id3</th>\n      <th>log_close/mean_300_id3</th>\n      <th>log_return_300_id3</th>\n      <th>log_close/mean_900_id3</th>\n      <th>log_return_900_id3</th>\n      <th>log_close/mean_60_id4</th>\n      <th>log_return_60_id4</th>\n      <th>log_close/mean_300_id4</th>\n      <th>log_return_300_id4</th>\n      <th>log_close/mean_900_id4</th>\n      <th>log_return_900_id4</th>\n      <th>log_close/mean_60_id5</th>\n      <th>log_return_60_id5</th>\n      <th>log_close/mean_300_id5</th>\n      <th>log_return_300_id5</th>\n      <th>log_close/mean_900_id5</th>\n      <th>log_return_900_id5</th>\n      <th>log_close/mean_60_id6</th>\n      <th>log_return_60_id6</th>\n      <th>log_close/mean_300_id6</th>\n      <th>log_return_300_id6</th>\n      <th>log_close/mean_900_id6</th>\n      <th>log_return_900_id6</th>\n      <th>log_close/mean_60_id7</th>\n      <th>log_return_60_id7</th>\n      <th>log_close/mean_300_id7</th>\n      <th>log_return_300_id7</th>\n      <th>log_close/mean_900_id7</th>\n      <th>log_return_900_id7</th>\n      <th>log_close/mean_60_id8</th>\n      <th>log_return_60_id8</th>\n      <th>log_close/mean_300_id8</th>\n      <th>log_return_300_id8</th>\n      <th>log_close/mean_900_id8</th>\n      <th>log_return_900_id8</th>\n      <th>log_close/mean_60_id9</th>\n      <th>log_return_60_id9</th>\n      <th>log_close/mean_300_id9</th>\n      <th>log_return_300_id9</th>\n      <th>log_close/mean_900_id9</th>\n      <th>log_return_900_id9</th>\n      <th>log_close/mean_60_id10</th>\n      <th>log_return_60_id10</th>\n      <th>log_close/mean_300_id10</th>\n      <th>log_return_300_id10</th>\n      <th>log_close/mean_900_id10</th>\n      <th>log_return_900_id10</th>\n      <th>log_close/mean_60_id11</th>\n      <th>log_return_60_id11</th>\n      <th>log_close/mean_300_id11</th>\n      <th>log_return_300_id11</th>\n      <th>log_close/mean_900_id11</th>\n      <th>log_return_900_id11</th>\n      <th>log_close/mean_60_id12</th>\n      <th>log_return_60_id12</th>\n      <th>log_close/mean_300_id12</th>\n      <th>log_return_300_id12</th>\n      <th>log_close/mean_900_id12</th>\n      <th>log_return_900_id12</th>\n      <th>log_close/mean_60_id13</th>\n      <th>log_return_60_id13</th>\n      <th>log_close/mean_300_id13</th>\n      <th>log_return_300_id13</th>\n      <th>log_close/mean_900_id13</th>\n      <th>log_return_900_id13</th>\n      <th>mean_close/mean_60</th>\n      <th>mean_log_returns_60</th>\n      <th>log_close/mean_60-mean_close/mean_60_id0</th>\n      <th>log_return_60-mean_log_returns_60_id0</th>\n      <th>log_close/mean_60-mean_close/mean_60_id1</th>\n      <th>log_return_60-mean_log_returns_60_id1</th>\n      <th>log_close/mean_60-mean_close/mean_60_id2</th>\n      <th>log_return_60-mean_log_returns_60_id2</th>\n      <th>log_close/mean_60-mean_close/mean_60_id3</th>\n      <th>log_return_60-mean_log_returns_60_id3</th>\n      <th>log_close/mean_60-mean_close/mean_60_id4</th>\n      <th>log_return_60-mean_log_returns_60_id4</th>\n      <th>log_close/mean_60-mean_close/mean_60_id5</th>\n      <th>log_return_60-mean_log_returns_60_id5</th>\n      <th>log_close/mean_60-mean_close/mean_60_id6</th>\n      <th>log_return_60-mean_log_returns_60_id6</th>\n      <th>log_close/mean_60-mean_close/mean_60_id7</th>\n      <th>log_return_60-mean_log_returns_60_id7</th>\n      <th>log_close/mean_60-mean_close/mean_60_id8</th>\n      <th>log_return_60-mean_log_returns_60_id8</th>\n      <th>log_close/mean_60-mean_close/mean_60_id9</th>\n      <th>log_return_60-mean_log_returns_60_id9</th>\n      <th>log_close/mean_60-mean_close/mean_60_id10</th>\n      <th>log_return_60-mean_log_returns_60_id10</th>\n      <th>log_close/mean_60-mean_close/mean_60_id11</th>\n      <th>log_return_60-mean_log_returns_60_id11</th>\n      <th>log_close/mean_60-mean_close/mean_60_id12</th>\n      <th>log_return_60-mean_log_returns_60_id12</th>\n      <th>log_close/mean_60-mean_close/mean_60_id13</th>\n      <th>log_return_60-mean_log_returns_60_id13</th>\n      <th>mean_close/mean_300</th>\n      <th>mean_log_returns_300</th>\n      <th>log_close/mean_300-mean_close/mean_300_id0</th>\n      <th>log_return_300-mean_log_returns_300_id0</th>\n      <th>log_close/mean_300-mean_close/mean_300_id1</th>\n      <th>log_return_300-mean_log_returns_300_id1</th>\n      <th>log_close/mean_300-mean_close/mean_300_id2</th>\n      <th>log_return_300-mean_log_returns_300_id2</th>\n      <th>log_close/mean_300-mean_close/mean_300_id3</th>\n      <th>log_return_300-mean_log_returns_300_id3</th>\n      <th>log_close/mean_300-mean_close/mean_300_id4</th>\n      <th>log_return_300-mean_log_returns_300_id4</th>\n      <th>log_close/mean_300-mean_close/mean_300_id5</th>\n      <th>log_return_300-mean_log_returns_300_id5</th>\n      <th>log_close/mean_300-mean_close/mean_300_id6</th>\n      <th>log_return_300-mean_log_returns_300_id6</th>\n      <th>log_close/mean_300-mean_close/mean_300_id7</th>\n      <th>log_return_300-mean_log_returns_300_id7</th>\n      <th>log_close/mean_300-mean_close/mean_300_id8</th>\n      <th>log_return_300-mean_log_returns_300_id8</th>\n      <th>log_close/mean_300-mean_close/mean_300_id9</th>\n      <th>log_return_300-mean_log_returns_300_id9</th>\n      <th>log_close/mean_300-mean_close/mean_300_id10</th>\n      <th>log_return_300-mean_log_returns_300_id10</th>\n      <th>log_close/mean_300-mean_close/mean_300_id11</th>\n      <th>log_return_300-mean_log_returns_300_id11</th>\n      <th>log_close/mean_300-mean_close/mean_300_id12</th>\n      <th>log_return_300-mean_log_returns_300_id12</th>\n      <th>log_close/mean_300-mean_close/mean_300_id13</th>\n      <th>log_return_300-mean_log_returns_300_id13</th>\n      <th>mean_close/mean_900</th>\n      <th>mean_log_returns_900</th>\n      <th>log_close/mean_900-mean_close/mean_900_id0</th>\n      <th>log_return_900-mean_log_returns_900_id0</th>\n      <th>log_close/mean_900-mean_close/mean_900_id1</th>\n      <th>log_return_900-mean_log_returns_900_id1</th>\n      <th>log_close/mean_900-mean_close/mean_900_id2</th>\n      <th>log_return_900-mean_log_returns_900_id2</th>\n      <th>log_close/mean_900-mean_close/mean_900_id3</th>\n      <th>log_return_900-mean_log_returns_900_id3</th>\n      <th>log_close/mean_900-mean_close/mean_900_id4</th>\n      <th>log_return_900-mean_log_returns_900_id4</th>\n      <th>log_close/mean_900-mean_close/mean_900_id5</th>\n      <th>log_return_900-mean_log_returns_900_id5</th>\n      <th>log_close/mean_900-mean_close/mean_900_id6</th>\n      <th>log_return_900-mean_log_returns_900_id6</th>\n      <th>log_close/mean_900-mean_close/mean_900_id7</th>\n      <th>log_return_900-mean_log_returns_900_id7</th>\n      <th>log_close/mean_900-mean_close/mean_900_id8</th>\n      <th>log_return_900-mean_log_returns_900_id8</th>\n      <th>log_close/mean_900-mean_close/mean_900_id9</th>\n      <th>log_return_900-mean_log_returns_900_id9</th>\n      <th>log_close/mean_900-mean_close/mean_900_id10</th>\n      <th>log_return_900-mean_log_returns_900_id10</th>\n      <th>log_close/mean_900-mean_close/mean_900_id11</th>\n      <th>log_return_900-mean_log_returns_900_id11</th>\n      <th>log_close/mean_900-mean_close/mean_900_id12</th>\n      <th>log_return_900-mean_log_returns_900_id12</th>\n      <th>log_close/mean_900-mean_close/mean_900_id13</th>\n      <th>log_return_900-mean_log_returns_900_id13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2005439</th>\n      <td>1635091200</td>\n      <td>-0.002430</td>\n      <td>0.002048</td>\n      <td>0.003521</td>\n      <td>0.001439</td>\n      <td>-0.002289</td>\n      <td>-0.000250</td>\n      <td>0.000073</td>\n      <td>0.001575</td>\n      <td>-0.009354</td>\n      <td>-0.008080</td>\n      <td>-0.002573</td>\n      <td>-0.002966</td>\n      <td>-0.001269</td>\n      <td>-0.003735</td>\n      <td>-0.015854</td>\n      <td>-2.898438</td>\n      <td>-0.072815</td>\n      <td>-0.000112</td>\n      <td>-0.000053</td>\n      <td>-0.001646</td>\n      <td>-0.255371</td>\n      <td>-0.003613</td>\n      <td>-0.000191</td>\n      <td>-0.029114</td>\n      <td>-0.342041</td>\n      <td>-0.066162</td>\n      <td>-0.000055</td>\n      <td>-0.000010</td>\n      <td>1.072266</td>\n      <td>1.370117</td>\n      <td>4.578125</td>\n      <td>1.272461</td>\n      <td>0.949219</td>\n      <td>5.480469</td>\n      <td>1.961914</td>\n      <td>1.569336</td>\n      <td>0.624512</td>\n      <td>2.337891</td>\n      <td>1.936523</td>\n      <td>0.794922</td>\n      <td>2.705078</td>\n      <td>2.023438</td>\n      <td>45.71875</td>\n      <td>46.09375</td>\n      <td>42.15625</td>\n      <td>45.31250</td>\n      <td>51.37500</td>\n      <td>42.18750</td>\n      <td>45.59375</td>\n      <td>45.0625</td>\n      <td>45.56250</td>\n      <td>42.40625</td>\n      <td>41.37500</td>\n      <td>46.12500</td>\n      <td>44.12500</td>\n      <td>41.46875</td>\n      <td>-0.002672</td>\n      <td>-0.004894</td>\n      <td>-0.007069</td>\n      <td>-0.015251</td>\n      <td>-0.016434</td>\n      <td>-0.027725</td>\n      <td>-0.001974</td>\n      <td>-0.004112</td>\n      <td>-0.007874</td>\n      <td>-0.017975</td>\n      <td>-0.014359</td>\n      <td>-0.020294</td>\n      <td>-0.005310</td>\n      <td>-0.010803</td>\n      <td>-0.016815</td>\n      <td>-0.035278</td>\n      <td>-0.025009</td>\n      <td>-0.033264</td>\n      <td>-0.003073</td>\n      <td>-0.004406</td>\n      <td>-0.008156</td>\n      <td>-0.016739</td>\n      <td>-0.016357</td>\n      <td>-0.025711</td>\n      <td>-0.011513</td>\n      <td>-0.029739</td>\n      <td>-0.021225</td>\n      <td>0.021255</td>\n      <td>0.002699</td>\n      <td>0.006813</td>\n      <td>-0.009712</td>\n      <td>-0.023712</td>\n      <td>-0.030365</td>\n      <td>-0.056763</td>\n      <td>-0.026169</td>\n      <td>0.002926</td>\n      <td>-0.004341</td>\n      <td>-0.006367</td>\n      <td>-0.010208</td>\n      <td>-0.023224</td>\n      <td>-0.022064</td>\n      <td>-0.035797</td>\n      <td>-0.004593</td>\n      <td>-0.009323</td>\n      <td>-0.010254</td>\n      <td>-0.022217</td>\n      <td>-0.019638</td>\n      <td>-0.031738</td>\n      <td>-0.007305</td>\n      <td>-0.015251</td>\n      <td>-0.017853</td>\n      <td>-0.070312</td>\n      <td>-0.027740</td>\n      <td>-0.027725</td>\n      <td>-0.007298</td>\n      <td>-0.015251</td>\n      <td>-0.018951</td>\n      <td>-0.036804</td>\n      <td>-0.031372</td>\n      <td>-0.035797</td>\n      <td>-0.007019</td>\n      <td>-0.013763</td>\n      <td>-0.019409</td>\n      <td>-0.034790</td>\n      <td>-0.032257</td>\n      <td>-0.043915</td>\n      <td>-0.005642</td>\n      <td>-0.013763</td>\n      <td>-0.019226</td>\n      <td>-0.021713</td>\n      <td>-0.011093</td>\n      <td>0.022217</td>\n      <td>-0.005791</td>\n      <td>-0.012779</td>\n      <td>-0.014549</td>\n      <td>-0.027222</td>\n      <td>-0.019928</td>\n      <td>-0.012779</td>\n      <td>-0.004208</td>\n      <td>-0.010307</td>\n      <td>-0.013206</td>\n      <td>-0.029739</td>\n      <td>-0.016266</td>\n      <td>-0.019730</td>\n      <td>-0.005745</td>\n      <td>-0.012459</td>\n      <td>0.003073</td>\n      <td>0.007568</td>\n      <td>0.003771</td>\n      <td>0.008354</td>\n      <td>0.000437</td>\n      <td>0.001659</td>\n      <td>0.002674</td>\n      <td>0.008057</td>\n      <td>-0.005764</td>\n      <td>-0.017273</td>\n      <td>-0.003963</td>\n      <td>-0.011253</td>\n      <td>0.001403</td>\n      <td>0.006096</td>\n      <td>0.001155</td>\n      <td>0.003139</td>\n      <td>-0.001559</td>\n      <td>-0.002789</td>\n      <td>-0.001553</td>\n      <td>-0.002789</td>\n      <td>-0.001271</td>\n      <td>-0.001301</td>\n      <td>0.000105</td>\n      <td>-0.001301</td>\n      <td>-0.000043</td>\n      <td>-0.000317</td>\n      <td>0.001536</td>\n      <td>0.002155</td>\n      <td>-0.015366</td>\n      <td>-0.027634</td>\n      <td>0.008301</td>\n      <td>0.012375</td>\n      <td>0.007496</td>\n      <td>0.009651</td>\n      <td>-0.001447</td>\n      <td>-0.007652</td>\n      <td>0.007210</td>\n      <td>0.010887</td>\n      <td>-0.005852</td>\n      <td>0.048889</td>\n      <td>-0.015007</td>\n      <td>-0.029129</td>\n      <td>0.005161</td>\n      <td>0.004402</td>\n      <td>0.005112</td>\n      <td>0.005409</td>\n      <td>-0.002478</td>\n      <td>-0.042694</td>\n      <td>-0.003588</td>\n      <td>-0.009178</td>\n      <td>-0.004040</td>\n      <td>-0.007164</td>\n      <td>-0.003855</td>\n      <td>0.005913</td>\n      <td>0.000816</td>\n      <td>0.000405</td>\n      <td>0.002161</td>\n      <td>-0.002113</td>\n      <td>-0.019714</td>\n      <td>-0.020172</td>\n      <td>0.003283</td>\n      <td>-0.007545</td>\n      <td>0.005352</td>\n      <td>-0.000108</td>\n      <td>-0.005302</td>\n      <td>-0.013084</td>\n      <td>0.003355</td>\n      <td>-0.005531</td>\n      <td>0.022415</td>\n      <td>0.026993</td>\n      <td>-0.006451</td>\n      <td>0.023102</td>\n      <td>-0.002350</td>\n      <td>-0.015617</td>\n      <td>0.000082</td>\n      <td>-0.011559</td>\n      <td>-0.008026</td>\n      <td>-0.007545</td>\n      <td>-0.011658</td>\n      <td>-0.015617</td>\n      <td>-0.012550</td>\n      <td>-0.023743</td>\n      <td>0.008621</td>\n      <td>0.042389</td>\n      <td>-0.000211</td>\n      <td>0.007401</td>\n      <td>0.003452</td>\n      <td>0.000450</td>\n    </tr>\n    <tr>\n      <th>2005440</th>\n      <td>1635091260</td>\n      <td>-0.000086</td>\n      <td>0.001321</td>\n      <td>0.002192</td>\n      <td>0.001559</td>\n      <td>-0.000407</td>\n      <td>-0.000880</td>\n      <td>0.000226</td>\n      <td>0.000977</td>\n      <td>-0.005646</td>\n      <td>-0.006458</td>\n      <td>-0.001164</td>\n      <td>-0.000839</td>\n      <td>-0.002779</td>\n      <td>-0.001337</td>\n      <td>-0.017502</td>\n      <td>-3.115234</td>\n      <td>-0.076782</td>\n      <td>-0.000120</td>\n      <td>-0.000054</td>\n      <td>-0.001637</td>\n      <td>-0.279541</td>\n      <td>-0.003744</td>\n      <td>-0.000199</td>\n      <td>-0.029541</td>\n      <td>-0.357422</td>\n      <td>-0.068237</td>\n      <td>-0.000058</td>\n      <td>-0.000011</td>\n      <td>0.937988</td>\n      <td>0.925781</td>\n      <td>0.517090</td>\n      <td>0.931152</td>\n      <td>0.403564</td>\n      <td>1.964844</td>\n      <td>0.814453</td>\n      <td>1.745117</td>\n      <td>0.664062</td>\n      <td>1.288086</td>\n      <td>0.237183</td>\n      <td>0.933105</td>\n      <td>1.598633</td>\n      <td>2.083984</td>\n      <td>45.71875</td>\n      <td>46.03125</td>\n      <td>41.78125</td>\n      <td>45.06250</td>\n      <td>51.56250</td>\n      <td>42.31250</td>\n      <td>45.50000</td>\n      <td>45.3125</td>\n      <td>47.71875</td>\n      <td>42.56250</td>\n      <td>41.37500</td>\n      <td>45.65625</td>\n      <td>43.87500</td>\n      <td>41.46875</td>\n      <td>-0.002602</td>\n      <td>-0.004406</td>\n      <td>-0.007015</td>\n      <td>-0.015251</td>\n      <td>-0.016403</td>\n      <td>-0.027222</td>\n      <td>-0.002298</td>\n      <td>-0.004620</td>\n      <td>-0.008209</td>\n      <td>-0.018341</td>\n      <td>-0.014740</td>\n      <td>-0.020386</td>\n      <td>-0.005939</td>\n      <td>-0.011292</td>\n      <td>-0.017517</td>\n      <td>-0.037323</td>\n      <td>-0.025803</td>\n      <td>-0.033752</td>\n      <td>-0.003906</td>\n      <td>-0.005386</td>\n      <td>-0.009026</td>\n      <td>-0.017731</td>\n      <td>-0.017258</td>\n      <td>-0.026718</td>\n      <td>-0.008171</td>\n      <td>-0.026718</td>\n      <td>-0.018402</td>\n      <td>0.024124</td>\n      <td>0.005581</td>\n      <td>0.010681</td>\n      <td>-0.008507</td>\n      <td>-0.022720</td>\n      <td>-0.029358</td>\n      <td>-0.055725</td>\n      <td>-0.025345</td>\n      <td>0.003899</td>\n      <td>-0.004726</td>\n      <td>-0.006859</td>\n      <td>-0.010628</td>\n      <td>-0.023712</td>\n      <td>-0.022522</td>\n      <td>-0.036285</td>\n      <td>-0.003857</td>\n      <td>-0.009323</td>\n      <td>-0.009605</td>\n      <td>-0.021225</td>\n      <td>-0.019012</td>\n      <td>-0.031250</td>\n      <td>-0.007812</td>\n      <td>-0.016251</td>\n      <td>-0.018509</td>\n      <td>-0.035278</td>\n      <td>-0.028458</td>\n      <td>-0.061951</td>\n      <td>-0.006397</td>\n      <td>-0.014259</td>\n      <td>-0.018173</td>\n      <td>-0.036285</td>\n      <td>-0.030670</td>\n      <td>-0.035278</td>\n      <td>-0.007614</td>\n      <td>-0.013763</td>\n      <td>-0.020126</td>\n      <td>-0.035278</td>\n      <td>-0.033051</td>\n      <td>-0.045441</td>\n      <td>-0.007214</td>\n      <td>-0.015251</td>\n      <td>-0.020966</td>\n      <td>-0.024216</td>\n      <td>-0.012947</td>\n      <td>0.021255</td>\n      <td>-0.006859</td>\n      <td>-0.014259</td>\n      <td>-0.015762</td>\n      <td>-0.028229</td>\n      <td>-0.021210</td>\n      <td>-0.014755</td>\n      <td>-0.004044</td>\n      <td>-0.009811</td>\n      <td>-0.013107</td>\n      <td>-0.029739</td>\n      <td>-0.016235</td>\n      <td>-0.020218</td>\n      <td>-0.005711</td>\n      <td>-0.012497</td>\n      <td>0.003109</td>\n      <td>0.008087</td>\n      <td>0.003412</td>\n      <td>0.007874</td>\n      <td>-0.000228</td>\n      <td>0.001203</td>\n      <td>0.001803</td>\n      <td>0.007107</td>\n      <td>-0.002460</td>\n      <td>-0.014221</td>\n      <td>-0.002798</td>\n      <td>-0.010223</td>\n      <td>0.000984</td>\n      <td>0.005634</td>\n      <td>0.001854</td>\n      <td>0.003172</td>\n      <td>-0.002104</td>\n      <td>-0.003756</td>\n      <td>-0.000686</td>\n      <td>-0.001765</td>\n      <td>-0.001902</td>\n      <td>-0.001269</td>\n      <td>-0.001504</td>\n      <td>-0.002756</td>\n      <td>-0.001148</td>\n      <td>-0.001765</td>\n      <td>0.001666</td>\n      <td>0.002684</td>\n      <td>-0.015457</td>\n      <td>-0.025299</td>\n      <td>0.008438</td>\n      <td>0.010048</td>\n      <td>0.007248</td>\n      <td>0.006958</td>\n      <td>-0.002054</td>\n      <td>-0.012024</td>\n      <td>0.006432</td>\n      <td>0.007568</td>\n      <td>-0.002945</td>\n      <td>0.049438</td>\n      <td>-0.013908</td>\n      <td>-0.030426</td>\n      <td>0.004829</td>\n      <td>0.001589</td>\n      <td>0.005856</td>\n      <td>0.004074</td>\n      <td>-0.003050</td>\n      <td>-0.009979</td>\n      <td>-0.002716</td>\n      <td>-0.010986</td>\n      <td>-0.004662</td>\n      <td>-0.009979</td>\n      <td>-0.005516</td>\n      <td>0.001085</td>\n      <td>-0.000307</td>\n      <td>-0.002928</td>\n      <td>0.002352</td>\n      <td>-0.004440</td>\n      <td>-0.019867</td>\n      <td>-0.022675</td>\n      <td>0.003462</td>\n      <td>-0.004547</td>\n      <td>0.005123</td>\n      <td>0.002281</td>\n      <td>-0.005936</td>\n      <td>-0.011078</td>\n      <td>0.002605</td>\n      <td>-0.004044</td>\n      <td>0.025452</td>\n      <td>0.033356</td>\n      <td>-0.005486</td>\n      <td>0.026566</td>\n      <td>-0.002661</td>\n      <td>-0.013611</td>\n      <td>0.000846</td>\n      <td>-0.008575</td>\n      <td>-0.008591</td>\n      <td>-0.039276</td>\n      <td>-0.010811</td>\n      <td>-0.012604</td>\n      <td>-0.013184</td>\n      <td>-0.022766</td>\n      <td>0.006912</td>\n      <td>0.043915</td>\n      <td>-0.001353</td>\n      <td>0.007919</td>\n      <td>0.003624</td>\n      <td>0.002455</td>\n    </tr>\n    <tr>\n      <th>2005441</th>\n      <td>1635091320</td>\n      <td>0.001995</td>\n      <td>0.001098</td>\n      <td>0.001890</td>\n      <td>0.001281</td>\n      <td>0.001850</td>\n      <td>-0.000157</td>\n      <td>-0.000675</td>\n      <td>0.000768</td>\n      <td>-0.003485</td>\n      <td>-0.006683</td>\n      <td>0.001768</td>\n      <td>0.002991</td>\n      <td>-0.003222</td>\n      <td>0.001167</td>\n      <td>-0.019943</td>\n      <td>-3.337891</td>\n      <td>-0.079834</td>\n      <td>-0.000131</td>\n      <td>-0.000056</td>\n      <td>-0.001637</td>\n      <td>-0.307373</td>\n      <td>-0.003990</td>\n      <td>-0.000211</td>\n      <td>-0.031311</td>\n      <td>-0.373535</td>\n      <td>-0.070435</td>\n      <td>-0.000060</td>\n      <td>-0.000011</td>\n      <td>2.011719</td>\n      <td>0.998535</td>\n      <td>3.535156</td>\n      <td>4.335938</td>\n      <td>0.708984</td>\n      <td>1.347656</td>\n      <td>2.224609</td>\n      <td>3.621094</td>\n      <td>1.430664</td>\n      <td>6.351562</td>\n      <td>0.474609</td>\n      <td>0.955566</td>\n      <td>1.542969</td>\n      <td>1.884766</td>\n      <td>45.46875</td>\n      <td>46.03125</td>\n      <td>41.93750</td>\n      <td>44.84375</td>\n      <td>51.43750</td>\n      <td>42.31250</td>\n      <td>45.50000</td>\n      <td>45.1875</td>\n      <td>45.06250</td>\n      <td>42.21875</td>\n      <td>41.37500</td>\n      <td>45.65625</td>\n      <td>44.00000</td>\n      <td>41.18750</td>\n      <td>-0.003565</td>\n      <td>-0.005878</td>\n      <td>-0.008026</td>\n      <td>-0.016251</td>\n      <td>-0.017426</td>\n      <td>-0.028229</td>\n      <td>-0.002623</td>\n      <td>-0.005299</td>\n      <td>-0.008560</td>\n      <td>-0.018417</td>\n      <td>-0.015129</td>\n      <td>-0.020966</td>\n      <td>-0.005733</td>\n      <td>-0.012283</td>\n      <td>-0.017395</td>\n      <td>-0.036285</td>\n      <td>-0.025757</td>\n      <td>-0.034790</td>\n      <td>-0.004715</td>\n      <td>-0.007351</td>\n      <td>-0.009895</td>\n      <td>-0.018234</td>\n      <td>-0.018158</td>\n      <td>-0.027725</td>\n      <td>-0.010544</td>\n      <td>-0.031250</td>\n      <td>-0.021378</td>\n      <td>0.022217</td>\n      <td>0.002680</td>\n      <td>0.005844</td>\n      <td>-0.008934</td>\n      <td>-0.023712</td>\n      <td>-0.029999</td>\n      <td>-0.055725</td>\n      <td>-0.026169</td>\n      <td>0.002926</td>\n      <td>-0.005577</td>\n      <td>-0.008827</td>\n      <td>-0.011551</td>\n      <td>-0.023712</td>\n      <td>-0.023483</td>\n      <td>-0.037323</td>\n      <td>-0.004833</td>\n      <td>-0.010803</td>\n      <td>-0.010689</td>\n      <td>-0.021713</td>\n      <td>-0.020142</td>\n      <td>-0.032745</td>\n      <td>-0.009834</td>\n      <td>-0.019226</td>\n      <td>-0.020615</td>\n      <td>-0.071838</td>\n      <td>-0.030762</td>\n      <td>-0.028732</td>\n      <td>-0.009399</td>\n      <td>-0.018234</td>\n      <td>-0.021347</td>\n      <td>-0.038330</td>\n      <td>-0.033936</td>\n      <td>-0.038818</td>\n      <td>-0.008194</td>\n      <td>-0.014755</td>\n      <td>-0.020828</td>\n      <td>-0.035278</td>\n      <td>-0.033813</td>\n      <td>-0.045441</td>\n      <td>-0.007858</td>\n      <td>-0.016251</td>\n      <td>-0.021805</td>\n      <td>-0.024719</td>\n      <td>-0.013885</td>\n      <td>0.019348</td>\n      <td>-0.006611</td>\n      <td>-0.014755</td>\n      <td>-0.015671</td>\n      <td>-0.027725</td>\n      <td>-0.021194</td>\n      <td>-0.014755</td>\n      <td>-0.005089</td>\n      <td>-0.011787</td>\n      <td>-0.014244</td>\n      <td>-0.031250</td>\n      <td>-0.017456</td>\n      <td>-0.021713</td>\n      <td>-0.006680</td>\n      <td>-0.014313</td>\n      <td>0.003115</td>\n      <td>0.008438</td>\n      <td>0.004059</td>\n      <td>0.009018</td>\n      <td>0.000945</td>\n      <td>0.002031</td>\n      <td>0.001966</td>\n      <td>0.006966</td>\n      <td>-0.003860</td>\n      <td>-0.016937</td>\n      <td>-0.002256</td>\n      <td>-0.009399</td>\n      <td>0.001101</td>\n      <td>0.005489</td>\n      <td>0.001845</td>\n      <td>0.003511</td>\n      <td>-0.003157</td>\n      <td>-0.004910</td>\n      <td>-0.002720</td>\n      <td>-0.003918</td>\n      <td>-0.001516</td>\n      <td>-0.000440</td>\n      <td>-0.001179</td>\n      <td>-0.001936</td>\n      <td>0.000070</td>\n      <td>-0.000440</td>\n      <td>0.001589</td>\n      <td>0.002527</td>\n      <td>-0.016571</td>\n      <td>-0.028381</td>\n      <td>0.008553</td>\n      <td>0.012123</td>\n      <td>0.008011</td>\n      <td>0.009949</td>\n      <td>-0.000819</td>\n      <td>-0.007912</td>\n      <td>0.006676</td>\n      <td>0.010139</td>\n      <td>-0.004799</td>\n      <td>0.050598</td>\n      <td>-0.013428</td>\n      <td>-0.027344</td>\n      <td>0.005020</td>\n      <td>0.004665</td>\n      <td>0.005882</td>\n      <td>0.006664</td>\n      <td>-0.004040</td>\n      <td>-0.043457</td>\n      <td>-0.004784</td>\n      <td>-0.009956</td>\n      <td>-0.004261</td>\n      <td>-0.006901</td>\n      <td>-0.005238</td>\n      <td>0.003656</td>\n      <td>0.000899</td>\n      <td>0.000651</td>\n      <td>0.002329</td>\n      <td>-0.002874</td>\n      <td>-0.021042</td>\n      <td>-0.021652</td>\n      <td>0.003618</td>\n      <td>-0.006577</td>\n      <td>0.005920</td>\n      <td>0.000686</td>\n      <td>-0.004715</td>\n      <td>-0.013138</td>\n      <td>0.002892</td>\n      <td>-0.006073</td>\n      <td>0.023727</td>\n      <td>0.027496</td>\n      <td>-0.005123</td>\n      <td>0.024582</td>\n      <td>-0.002438</td>\n      <td>-0.015671</td>\n      <td>0.000907</td>\n      <td>-0.011093</td>\n      <td>-0.009720</td>\n      <td>-0.007080</td>\n      <td>-0.012894</td>\n      <td>-0.017166</td>\n      <td>-0.012779</td>\n      <td>-0.023788</td>\n      <td>0.007160</td>\n      <td>0.040985</td>\n      <td>-0.000152</td>\n      <td>0.006897</td>\n      <td>0.003590</td>\n      <td>-0.000062</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2136957</th>\n      <td>1642982280</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.135498</td>\n      <td>8.132812</td>\n      <td>0.113953</td>\n      <td>0.000429</td>\n      <td>0.000032</td>\n      <td>0.000668</td>\n      <td>0.979004</td>\n      <td>0.008644</td>\n      <td>0.000309</td>\n      <td>0.035126</td>\n      <td>0.418213</td>\n      <td>0.054779</td>\n      <td>0.000068</td>\n      <td>0.000012</td>\n      <td>0.530762</td>\n      <td>0.644043</td>\n      <td>0.851562</td>\n      <td>1.186523</td>\n      <td>0.598145</td>\n      <td>0.450439</td>\n      <td>0.832031</td>\n      <td>0.422607</td>\n      <td>1.555664</td>\n      <td>0.582031</td>\n      <td>0.348633</td>\n      <td>0.171143</td>\n      <td>2.171875</td>\n      <td>0.782715</td>\n      <td>55.65625</td>\n      <td>56.84375</td>\n      <td>56.75000</td>\n      <td>55.06250</td>\n      <td>53.84375</td>\n      <td>54.62500</td>\n      <td>56.87500</td>\n      <td>55.5625</td>\n      <td>53.09375</td>\n      <td>55.81250</td>\n      <td>52.62500</td>\n      <td>53.34375</td>\n      <td>55.53125</td>\n      <td>54.21875</td>\n      <td>0.007030</td>\n      <td>0.035492</td>\n      <td>0.036987</td>\n      <td>0.040192</td>\n      <td>0.020447</td>\n      <td>0.001951</td>\n      <td>0.002705</td>\n      <td>0.020645</td>\n      <td>0.024490</td>\n      <td>0.035980</td>\n      <td>0.021210</td>\n      <td>0.015366</td>\n      <td>0.008522</td>\n      <td>0.038330</td>\n      <td>0.038910</td>\n      <td>0.050476</td>\n      <td>0.034485</td>\n      <td>0.021255</td>\n      <td>0.006191</td>\n      <td>0.037384</td>\n      <td>0.040100</td>\n      <td>0.047668</td>\n      <td>0.018402</td>\n      <td>0.001951</td>\n      <td>0.003750</td>\n      <td>0.022217</td>\n      <td>0.026413</td>\n      <td>0.034546</td>\n      <td>0.022949</td>\n      <td>0.013580</td>\n      <td>0.005184</td>\n      <td>0.029831</td>\n      <td>0.029572</td>\n      <td>0.035492</td>\n      <td>0.020981</td>\n      <td>0.003899</td>\n      <td>0.008240</td>\n      <td>0.037384</td>\n      <td>0.041626</td>\n      <td>0.050476</td>\n      <td>0.031494</td>\n      <td>0.015503</td>\n      <td>0.007595</td>\n      <td>0.034546</td>\n      <td>0.037842</td>\n      <td>0.053253</td>\n      <td>0.031769</td>\n      <td>0.019348</td>\n      <td>0.012558</td>\n      <td>0.042084</td>\n      <td>0.034271</td>\n      <td>0.028870</td>\n      <td>0.016159</td>\n      <td>-0.012283</td>\n      <td>0.007431</td>\n      <td>0.033600</td>\n      <td>0.032318</td>\n      <td>0.048615</td>\n      <td>0.031097</td>\n      <td>0.024124</td>\n      <td>0.000755</td>\n      <td>0.025070</td>\n      <td>0.019608</td>\n      <td>0.021255</td>\n      <td>-0.004524</td>\n      <td>-0.031738</td>\n      <td>0.008331</td>\n      <td>0.030777</td>\n      <td>0.033112</td>\n      <td>0.027924</td>\n      <td>0.022232</td>\n      <td>-0.001466</td>\n      <td>0.008842</td>\n      <td>0.035492</td>\n      <td>0.035797</td>\n      <td>0.043945</td>\n      <td>0.032471</td>\n      <td>0.025070</td>\n      <td>0.000345</td>\n      <td>0.021255</td>\n      <td>0.019852</td>\n      <td>0.026031</td>\n      <td>0.011200</td>\n      <td>-0.009323</td>\n      <td>0.006248</td>\n      <td>0.031708</td>\n      <td>0.000781</td>\n      <td>0.003771</td>\n      <td>-0.003544</td>\n      <td>-0.011078</td>\n      <td>0.002272</td>\n      <td>0.006607</td>\n      <td>-0.000056</td>\n      <td>0.005661</td>\n      <td>-0.002499</td>\n      <td>-0.009506</td>\n      <td>-0.001066</td>\n      <td>-0.001891</td>\n      <td>0.001993</td>\n      <td>0.005661</td>\n      <td>0.001348</td>\n      <td>0.002825</td>\n      <td>0.006310</td>\n      <td>0.010361</td>\n      <td>0.001184</td>\n      <td>0.001878</td>\n      <td>-0.005493</td>\n      <td>-0.006653</td>\n      <td>0.002079</td>\n      <td>-0.000945</td>\n      <td>0.002594</td>\n      <td>0.003771</td>\n      <td>-0.005905</td>\n      <td>-0.010468</td>\n      <td>0.032196</td>\n      <td>0.038910</td>\n      <td>0.004791</td>\n      <td>0.001282</td>\n      <td>-0.007713</td>\n      <td>-0.002918</td>\n      <td>0.006699</td>\n      <td>0.011566</td>\n      <td>0.007904</td>\n      <td>0.008759</td>\n      <td>-0.005798</td>\n      <td>-0.004364</td>\n      <td>-0.002642</td>\n      <td>-0.003418</td>\n      <td>0.009407</td>\n      <td>0.011566</td>\n      <td>0.005646</td>\n      <td>0.014343</td>\n      <td>0.002073</td>\n      <td>-0.010040</td>\n      <td>0.000113</td>\n      <td>0.009705</td>\n      <td>-0.012604</td>\n      <td>-0.017654</td>\n      <td>0.000888</td>\n      <td>-0.010986</td>\n      <td>0.003588</td>\n      <td>0.005035</td>\n      <td>-0.012352</td>\n      <td>-0.012878</td>\n      <td>0.022171</td>\n      <td>0.006233</td>\n      <td>-0.001726</td>\n      <td>-0.004280</td>\n      <td>-0.000965</td>\n      <td>0.009140</td>\n      <td>0.012321</td>\n      <td>0.015022</td>\n      <td>-0.003769</td>\n      <td>-0.004280</td>\n      <td>0.000784</td>\n      <td>0.007347</td>\n      <td>-0.001193</td>\n      <td>-0.002333</td>\n      <td>0.009331</td>\n      <td>0.009270</td>\n      <td>0.009590</td>\n      <td>0.013115</td>\n      <td>-0.006004</td>\n      <td>-0.018509</td>\n      <td>0.008934</td>\n      <td>0.017899</td>\n      <td>-0.026703</td>\n      <td>-0.037964</td>\n      <td>0.000061</td>\n      <td>-0.007698</td>\n      <td>0.010307</td>\n      <td>0.018845</td>\n      <td>-0.010971</td>\n      <td>-0.015556</td>\n    </tr>\n    <tr>\n      <th>2136958</th>\n      <td>1642982340</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.134888</td>\n      <td>8.062500</td>\n      <td>0.114197</td>\n      <td>0.000434</td>\n      <td>0.000032</td>\n      <td>0.000672</td>\n      <td>0.982910</td>\n      <td>0.008675</td>\n      <td>0.000312</td>\n      <td>0.035278</td>\n      <td>0.417480</td>\n      <td>0.055115</td>\n      <td>0.000068</td>\n      <td>0.000012</td>\n      <td>0.394531</td>\n      <td>0.555176</td>\n      <td>0.331055</td>\n      <td>0.971680</td>\n      <td>0.585938</td>\n      <td>0.440186</td>\n      <td>0.757812</td>\n      <td>0.761719</td>\n      <td>0.686035</td>\n      <td>0.243896</td>\n      <td>0.648438</td>\n      <td>0.405029</td>\n      <td>0.609863</td>\n      <td>0.556152</td>\n      <td>55.75000</td>\n      <td>56.96875</td>\n      <td>57.00000</td>\n      <td>55.43750</td>\n      <td>54.06250</td>\n      <td>54.84375</td>\n      <td>57.00000</td>\n      <td>55.5625</td>\n      <td>53.37500</td>\n      <td>55.93750</td>\n      <td>52.71875</td>\n      <td>53.71875</td>\n      <td>55.75000</td>\n      <td>54.40625</td>\n      <td>0.005821</td>\n      <td>0.033600</td>\n      <td>0.036224</td>\n      <td>0.041138</td>\n      <td>0.019791</td>\n      <td>-0.002445</td>\n      <td>0.002371</td>\n      <td>0.020798</td>\n      <td>0.024384</td>\n      <td>0.036499</td>\n      <td>0.021210</td>\n      <td>0.011810</td>\n      <td>0.007881</td>\n      <td>0.038330</td>\n      <td>0.038727</td>\n      <td>0.052307</td>\n      <td>0.034485</td>\n      <td>0.017426</td>\n      <td>0.007259</td>\n      <td>0.041138</td>\n      <td>0.041687</td>\n      <td>0.051392</td>\n      <td>0.020142</td>\n      <td>-0.001955</td>\n      <td>0.003363</td>\n      <td>0.023163</td>\n      <td>0.026291</td>\n      <td>0.035492</td>\n      <td>0.022949</td>\n      <td>0.010681</td>\n      <td>0.005550</td>\n      <td>0.029831</td>\n      <td>0.030304</td>\n      <td>0.037384</td>\n      <td>0.021835</td>\n      <td>0.000976</td>\n      <td>0.007607</td>\n      <td>0.038330</td>\n      <td>0.041443</td>\n      <td>0.050476</td>\n      <td>0.031494</td>\n      <td>0.008751</td>\n      <td>0.006382</td>\n      <td>0.036438</td>\n      <td>0.037048</td>\n      <td>0.053253</td>\n      <td>0.031128</td>\n      <td>0.013580</td>\n      <td>0.013618</td>\n      <td>0.044891</td>\n      <td>0.035980</td>\n      <td>0.030777</td>\n      <td>0.017975</td>\n      <td>-0.014755</td>\n      <td>0.007412</td>\n      <td>0.035492</td>\n      <td>0.032715</td>\n      <td>0.049530</td>\n      <td>0.031647</td>\n      <td>0.019348</td>\n      <td>0.000828</td>\n      <td>0.028870</td>\n      <td>0.020081</td>\n      <td>0.021255</td>\n      <td>-0.003941</td>\n      <td>-0.033752</td>\n      <td>0.010132</td>\n      <td>0.035492</td>\n      <td>0.035370</td>\n      <td>0.031708</td>\n      <td>0.024628</td>\n      <td>-0.003424</td>\n      <td>0.008858</td>\n      <td>0.035492</td>\n      <td>0.036255</td>\n      <td>0.045807</td>\n      <td>0.033051</td>\n      <td>0.021255</td>\n      <td>0.000504</td>\n      <td>0.022217</td>\n      <td>0.020294</td>\n      <td>0.026978</td>\n      <td>0.011749</td>\n      <td>-0.012283</td>\n      <td>0.006256</td>\n      <td>0.033142</td>\n      <td>-0.000434</td>\n      <td>0.000451</td>\n      <td>-0.003885</td>\n      <td>-0.012344</td>\n      <td>0.001626</td>\n      <td>0.005180</td>\n      <td>0.001004</td>\n      <td>0.007988</td>\n      <td>-0.002893</td>\n      <td>-0.009987</td>\n      <td>-0.000704</td>\n      <td>-0.003319</td>\n      <td>0.001352</td>\n      <td>0.005180</td>\n      <td>0.000124</td>\n      <td>0.003288</td>\n      <td>0.007362</td>\n      <td>0.011742</td>\n      <td>0.001154</td>\n      <td>0.002342</td>\n      <td>-0.005428</td>\n      <td>-0.004280</td>\n      <td>0.003874</td>\n      <td>0.002342</td>\n      <td>0.002602</td>\n      <td>0.002342</td>\n      <td>-0.005753</td>\n      <td>-0.010933</td>\n      <td>0.032623</td>\n      <td>0.040283</td>\n      <td>0.003582</td>\n      <td>0.000852</td>\n      <td>-0.008240</td>\n      <td>-0.003782</td>\n      <td>0.006104</td>\n      <td>0.012024</td>\n      <td>0.009048</td>\n      <td>0.011108</td>\n      <td>-0.006336</td>\n      <td>-0.004795</td>\n      <td>-0.002329</td>\n      <td>-0.002901</td>\n      <td>0.008820</td>\n      <td>0.010193</td>\n      <td>0.004429</td>\n      <td>0.012970</td>\n      <td>0.003347</td>\n      <td>-0.009506</td>\n      <td>0.000084</td>\n      <td>0.009247</td>\n      <td>-0.012550</td>\n      <td>-0.019028</td>\n      <td>0.002756</td>\n      <td>-0.008575</td>\n      <td>0.003622</td>\n      <td>0.005520</td>\n      <td>-0.012329</td>\n      <td>-0.013306</td>\n      <td>0.022720</td>\n      <td>0.002516</td>\n      <td>-0.002926</td>\n      <td>-0.004959</td>\n      <td>-0.001519</td>\n      <td>0.009300</td>\n      <td>0.011749</td>\n      <td>0.014908</td>\n      <td>-0.002583</td>\n      <td>-0.004471</td>\n      <td>0.000221</td>\n      <td>0.008163</td>\n      <td>-0.000887</td>\n      <td>-0.001539</td>\n      <td>0.008766</td>\n      <td>0.006237</td>\n      <td>0.008400</td>\n      <td>0.011063</td>\n      <td>-0.004742</td>\n      <td>-0.017273</td>\n      <td>0.008911</td>\n      <td>0.016830</td>\n      <td>-0.026657</td>\n      <td>-0.036255</td>\n      <td>0.001905</td>\n      <td>-0.005939</td>\n      <td>0.010338</td>\n      <td>0.018738</td>\n      <td>-0.010979</td>\n      <td>-0.014801</td>\n    </tr>\n    <tr>\n      <th>2136959</th>\n      <td>1642982400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.131714</td>\n      <td>7.855469</td>\n      <td>0.112976</td>\n      <td>0.000428</td>\n      <td>0.000031</td>\n      <td>0.000650</td>\n      <td>0.969727</td>\n      <td>0.008430</td>\n      <td>0.000305</td>\n      <td>0.034637</td>\n      <td>0.416504</td>\n      <td>0.054413</td>\n      <td>0.000066</td>\n      <td>0.000012</td>\n      <td>1.596680</td>\n      <td>1.416992</td>\n      <td>0.914551</td>\n      <td>0.688965</td>\n      <td>0.812500</td>\n      <td>6.011719</td>\n      <td>1.068359</td>\n      <td>2.173828</td>\n      <td>0.456543</td>\n      <td>0.916992</td>\n      <td>0.686523</td>\n      <td>0.591309</td>\n      <td>8.945312</td>\n      <td>0.541992</td>\n      <td>54.84375</td>\n      <td>56.31250</td>\n      <td>56.06250</td>\n      <td>54.53125</td>\n      <td>53.18750</td>\n      <td>53.71875</td>\n      <td>56.34375</td>\n      <td>54.1250</td>\n      <td>52.56250</td>\n      <td>55.09375</td>\n      <td>52.31250</td>\n      <td>53.06250</td>\n      <td>54.75000</td>\n      <td>54.06250</td>\n      <td>0.002691</td>\n      <td>0.031708</td>\n      <td>0.033478</td>\n      <td>0.034546</td>\n      <td>0.017197</td>\n      <td>-0.004406</td>\n      <td>0.000519</td>\n      <td>0.020889</td>\n      <td>0.022766</td>\n      <td>0.033142</td>\n      <td>0.019684</td>\n      <td>0.011513</td>\n      <td>0.005604</td>\n      <td>0.038330</td>\n      <td>0.036926</td>\n      <td>0.044891</td>\n      <td>0.032806</td>\n      <td>0.016464</td>\n      <td>0.003155</td>\n      <td>0.038330</td>\n      <td>0.038055</td>\n      <td>0.042999</td>\n      <td>0.016663</td>\n      <td>-0.003424</td>\n      <td>-0.000446</td>\n      <td>0.022217</td>\n      <td>0.022751</td>\n      <td>0.027924</td>\n      <td>0.019485</td>\n      <td>0.008751</td>\n      <td>0.000762</td>\n      <td>0.029831</td>\n      <td>0.025894</td>\n      <td>0.028870</td>\n      <td>0.017532</td>\n      <td>-0.001955</td>\n      <td>0.004623</td>\n      <td>0.037384</td>\n      <td>0.038940</td>\n      <td>0.045807</td>\n      <td>0.029114</td>\n      <td>0.008751</td>\n      <td>0.001489</td>\n      <td>0.032654</td>\n      <td>0.032562</td>\n      <td>0.039246</td>\n      <td>0.026749</td>\n      <td>0.011650</td>\n      <td>0.008713</td>\n      <td>0.042999</td>\n      <td>0.031708</td>\n      <td>0.024124</td>\n      <td>0.013802</td>\n      <td>-0.016739</td>\n      <td>0.004063</td>\n      <td>0.033600</td>\n      <td>0.029785</td>\n      <td>0.042084</td>\n      <td>0.028824</td>\n      <td>0.018387</td>\n      <td>0.000373</td>\n      <td>0.027924</td>\n      <td>0.020020</td>\n      <td>0.018387</td>\n      <td>-0.003902</td>\n      <td>-0.034790</td>\n      <td>0.007202</td>\n      <td>0.032654</td>\n      <td>0.032898</td>\n      <td>0.026031</td>\n      <td>0.022232</td>\n      <td>-0.003914</td>\n      <td>0.004044</td>\n      <td>0.033600</td>\n      <td>0.031860</td>\n      <td>0.038330</td>\n      <td>0.028778</td>\n      <td>0.019348</td>\n      <td>-0.000911</td>\n      <td>0.021255</td>\n      <td>0.019150</td>\n      <td>0.025070</td>\n      <td>0.010696</td>\n      <td>-0.011787</td>\n      <td>0.002991</td>\n      <td>0.031677</td>\n      <td>-0.000301</td>\n      <td>0.000038</td>\n      <td>-0.002472</td>\n      <td>-0.010773</td>\n      <td>0.002611</td>\n      <td>0.006660</td>\n      <td>0.000164</td>\n      <td>0.006660</td>\n      <td>-0.003437</td>\n      <td>-0.009453</td>\n      <td>-0.002230</td>\n      <td>-0.001839</td>\n      <td>0.001633</td>\n      <td>0.005714</td>\n      <td>-0.001503</td>\n      <td>0.000984</td>\n      <td>0.005722</td>\n      <td>0.011330</td>\n      <td>0.001072</td>\n      <td>0.001930</td>\n      <td>-0.002619</td>\n      <td>-0.003746</td>\n      <td>0.004211</td>\n      <td>0.000984</td>\n      <td>0.001054</td>\n      <td>0.001930</td>\n      <td>-0.003902</td>\n      <td>-0.010414</td>\n      <td>0.029770</td>\n      <td>0.033661</td>\n      <td>0.003716</td>\n      <td>0.000871</td>\n      <td>-0.007004</td>\n      <td>-0.000531</td>\n      <td>0.007156</td>\n      <td>0.011215</td>\n      <td>0.008286</td>\n      <td>0.009323</td>\n      <td>-0.007023</td>\n      <td>-0.005753</td>\n      <td>-0.003868</td>\n      <td>-0.004807</td>\n      <td>0.009155</td>\n      <td>0.012131</td>\n      <td>0.002794</td>\n      <td>0.005569</td>\n      <td>0.001926</td>\n      <td>-0.009552</td>\n      <td>0.000014</td>\n      <td>0.008408</td>\n      <td>-0.009750</td>\n      <td>-0.015289</td>\n      <td>0.003132</td>\n      <td>-0.007645</td>\n      <td>0.002090</td>\n      <td>0.004654</td>\n      <td>-0.010620</td>\n      <td>-0.008606</td>\n      <td>0.019974</td>\n      <td>0.001275</td>\n      <td>-0.002783</td>\n      <td>-0.005680</td>\n      <td>-0.000294</td>\n      <td>0.010239</td>\n      <td>0.012825</td>\n      <td>0.015190</td>\n      <td>-0.003311</td>\n      <td>-0.004700</td>\n      <td>-0.000492</td>\n      <td>0.007477</td>\n      <td>-0.002441</td>\n      <td>-0.003229</td>\n      <td>0.009140</td>\n      <td>0.007477</td>\n      <td>0.006771</td>\n      <td>0.010376</td>\n      <td>-0.006176</td>\n      <td>-0.018021</td>\n      <td>0.008850</td>\n      <td>0.017105</td>\n      <td>-0.023880</td>\n      <td>-0.036072</td>\n      <td>0.002262</td>\n      <td>-0.005188</td>\n      <td>0.008804</td>\n      <td>0.018066</td>\n      <td>-0.009277</td>\n      <td>-0.013062</td>\n    </tr>\n  </tbody>\n</table>\n<p>131521 rows × 231 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:43.105274Z",
     "iopub.status.busy": "2023-12-05T15:56:43.104956Z",
     "iopub.status.idle": "2023-12-05T15:56:43.112336Z",
     "shell.execute_reply": "2023-12-05T15:56:43.110615Z",
     "shell.execute_reply.started": "2023-12-05T15:56:43.105246Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:09:27.883386800Z",
     "start_time": "2023-12-07T08:09:27.864995100Z"
    }
   },
   "outputs": [],
   "source": [
    "no_use_columns = [f'Target_{i}' for i in range(14)]\n",
    "no_use_columns.append('timestamp')\n",
    "no_use_columns.extend(FEATURES_EXCLUDE)\n",
    "features = test_df.columns\n",
    "features = features.drop(no_use_columns)\n",
    "features = list(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model_Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T15:56:43.117263Z",
     "iopub.status.busy": "2023-12-05T15:56:43.116903Z",
     "iopub.status.idle": "2023-12-05T15:56:56.849082Z",
     "shell.execute_reply": "2023-12-05T15:56:56.847679Z",
     "shell.execute_reply.started": "2023-12-05T15:56:43.117233Z"
    },
    "ExecuteTime": {
     "end_time": "2023-12-07T08:18:03.362144Z",
     "start_time": "2023-12-07T08:17:27.253100200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation score for asset_0: -0.033459853129969394\n",
      "correlation score for asset_1: 0.022814148586106187\n",
      "correlation score for asset_2: 0.06956885626701224\n",
      "correlation score for asset_3: 0.11410353691937435\n",
      "correlation score for asset_4: 0.08062148578321958\n",
      "correlation score for asset_5: 0.023172611895467304\n",
      "correlation score for asset_6: 0.027502049861807747\n",
      "correlation score for asset_7: 0.011872069194940356\n",
      "correlation score for asset_8: 0.02433590333167075\n",
      "correlation score for asset_9: 0.04617486610399766\n",
      "correlation score for asset_10: 0.04063943839674869\n",
      "correlation score for asset_11: 0.0018361484330397912\n",
      "correlation score for asset_12: 0.04636054422075633\n",
      "correlation score for asset_13: 0.05598820720058765\n",
      "overall weighted correlation score for 14 asset: 0.038347613925340425\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "df_asset_details = pd.read_csv(ASSET_DETAILS_CSV)\n",
    "sum_of_weight = df_asset_details['Weight'].sum()\n",
    "overall_correlation_score = 0.\n",
    "correlation_list = []\n",
    "# start inference\n",
    "for asset_id in ASSET_ID_SELECTED_FOR_INFERENCE:\n",
    "    model_path = f'filnal_new_models/model_id{asset_id}.txt'\n",
    "    model = lgb.Booster(model_file=model_path)\n",
    "    weight = df_asset_details[df_asset_details['Asset_ID'] == asset_id]['Weight'].values[0]\n",
    "    X = test_df.loc[(test_df[f'Target_{asset_id}'] == test_df[f'Target_{asset_id}'])].loc[:,features]\n",
    "    y = test_df.loc[(test_df[f'Target_{asset_id}'] == test_df[f'Target_{asset_id}'])].loc[:,f'Target_{asset_id}']\n",
    "    y_pred = model.predict(X)\n",
    "    correlation = correlation_scorer(y,y_pred)\n",
    "    correlation_list.append(correlation)\n",
    "    overall_correlation_score += (correlation*weight)/sum_of_weight\n",
    "    print(f'correlation score for asset_{asset_id}: {correlation}')\n",
    "print(f'overall weighted correlation score for 14 asset: {overall_correlation_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.880921Z",
     "start_time": "2023-12-07T08:10:02.855867900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.886516200Z",
     "start_time": "2023-12-07T08:10:02.861679600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.887024700Z",
     "start_time": "2023-12-07T08:10:02.867860400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.887024700Z",
     "start_time": "2023-12-07T08:10:02.874269800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.911299800Z",
     "start_time": "2023-12-07T08:10:02.880921Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.944804900Z",
     "start_time": "2023-12-07T08:10:02.887024700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T08:10:02.952814800Z",
     "start_time": "2023-12-07T08:10:02.896617800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d490669-15d3-4c35-bad0-ed2652d11f01",
    "_uuid": "2e107c87-fe93-4837-bc49-80ae3c4f2fc2"
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4043536,
     "sourceId": 7030037,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4043626,
     "sourceId": 7030154,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4045593,
     "sourceId": 7033033,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4097756,
     "sourceId": 7107545,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4102054,
     "sourceId": 7113530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4102893,
     "sourceId": 7114655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
