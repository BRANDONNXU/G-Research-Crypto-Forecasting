{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group members:\n",
    "Bohao XU        ETU20211498\n",
    "Jijun TAN       ETU20211472\n",
    "Yilin ZHANG     ETU20211520\n",
    "Junxin HUANG    ETU20211420\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.727346200Z",
     "start_time": "2023-12-05T20:42:17.193601400Z"
    },
    "_cell_guid": "8f8fbc9e-d6c0-48f2-83cd-05c7e26499b3",
    "_uuid": "87495b9f-8db8-4e2e-8cd4-3a71e006318a",
    "execution": {
     "iopub.execute_input": "2023-11-25T17:04:55.761459Z",
     "iopub.status.busy": "2023-11-25T17:04:55.759694Z",
     "iopub.status.idle": "2023-11-25T17:04:55.778704Z",
     "shell.execute_reply": "2023-11-25T17:04:55.776885Z",
     "shell.execute_reply.started": "2023-11-25T17:04:55.761391Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import csv\n",
    "import optuna\n",
    "\n",
    "seed0 = 8586\n",
    "\n",
    "creator='group4' # for create a certain folder since we 4 group members work in the same server\n",
    "TRAIN_CSV = '2018_2022_Cleaned_dataset_with_CloseVolumeTarget.csv'\n",
    "ASSET_DETAILS_CSV = 'asset_details.csv'\n",
    "ASSET_ID_SELECTED_FOR_TRAIN = [for i in range(14)] # when you train a certain model for asset_id=x, write x in this tuple like (x,)\n",
    "FEATURES_EXCLUDE = [] # for those features need to be excluded for training\n",
    "N_TRIALS = 10 # 10 means that the bayes optimizer would iterate for 10 times\n",
    "\n",
    "pd.set_option('display.max_rows', 6)\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.731348900Z",
     "start_time": "2023-12-05T20:42:17.197607100Z"
    },
    "_cell_guid": "98a2ab4b-8d0b-4b85-91f2-23ecaa15a578",
    "_uuid": "54101aa4-4b05-4ee1-97c1-4c0e430ca5f1",
    "execution": {
     "iopub.execute_input": "2023-11-25T17:53:30.026650Z",
     "iopub.status.busy": "2023-11-25T17:53:30.026246Z",
     "iopub.status.idle": "2023-11-25T17:53:30.039377Z",
     "shell.execute_reply": "2023-11-25T17:53:30.037484Z",
     "shell.execute_reply.started": "2023-11-25T17:53:30.026619Z"
    }
   },
   "outputs": [],
   "source": [
    "lags = [60,300,900]\n",
    "\n",
    "# Range of hyperparameters to be tuned\n",
    "param_ranges = {\n",
    "'learning_rate': (0.15, 0.2), \n",
    "    'max_depth': (14, 18), \n",
    "    'num_leaves': (55, 85), \n",
    "    'lambda_l1': (3.2, 3.8), \n",
    "    'lambda_l2': (2.2, 2.8), \n",
    "    'max_bin': (900, 1100), \n",
    "    'min_data_in_leaf': (48, 80)\n",
    "}\n",
    "\n",
    "# Parameters that do not require tuning\n",
    "params = {\n",
    "    'early_stopping_rounds': 100,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'correlation',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_fraction_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'extra_trees': True,\n",
    "    'extra_seed': seed0,\n",
    "    'zero_as_missing': True,\n",
    "    \"first_metric_only\": True\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.731348900Z",
     "start_time": "2023-12-05T20:42:17.211929400Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.014781Z",
     "iopub.status.busy": "2023-11-25T16:06:38.014222Z",
     "iopub.status.idle": "2023-11-25T16:06:38.032195Z",
     "shell.execute_reply": "2023-11-25T16:06:38.030900Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.014738Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.747346300Z",
     "start_time": "2023-12-05T20:42:17.230867200Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.036704Z",
     "iopub.status.busy": "2023-11-25T16:06:38.035733Z",
     "iopub.status.idle": "2023-11-25T16:06:38.054640Z",
     "shell.execute_reply": "2023-11-25T16:06:38.053379Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.036657Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.747346300Z",
     "start_time": "2023-12-05T20:42:17.235378800Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.057230Z",
     "iopub.status.busy": "2023-11-25T16:06:38.056687Z",
     "iopub.status.idle": "2023-11-25T16:06:38.072191Z",
     "shell.execute_reply": "2023-11-25T16:06:38.071049Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.057195Z"
    }
   },
   "outputs": [],
   "source": [
    "def MACD(df, short_term=12 * 30, long_term=26 * 30):\n",
    "    '''Pass in a df and return the df with the 'MACD_id' columns'''\n",
    "    tmp_col = ['EMA_short', 'EMA_long', 'DIF', 'DEA']\n",
    "    for id in range(14):\n",
    "        cols_to_select = [f'Close_{id}', 'timestamp', f'Target_{id}']\n",
    "        df_selected = df[cols_to_select].sort_values('timestamp')\n",
    "\n",
    "        # Calculate short-term (fast) moving averages（EMA12）\n",
    "        df_selected['EMA_short'] = df_selected[f'Close_{id}'].ewm(span=short_term, adjust=False).mean()\n",
    "\n",
    "        # Calculating long-term (slow) moving averages（EMA26）\n",
    "        df_selected['EMA_long'] = df_selected[f'Close_{id}'].ewm(span=long_term, adjust=False).mean()\n",
    "\n",
    "        # Calculation of deviation（DIF）\n",
    "        df_selected['DIF'] = df_selected['EMA_short'] - df_selected['EMA_long']\n",
    "\n",
    "        # Calculating the DEA using a 9-day exponential moving average\n",
    "        df_selected['DEA'] = df_selected['DIF'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "        # Calculating the MACD line\n",
    "        df_selected[f'MACD_id{id}'] = df_selected['DIF'] - df_selected['DEA']\n",
    "\n",
    "        # Merge back into the original DataFrame\n",
    "        df = df.merge(df_selected[['timestamp', f'MACD_id{id}']], on='timestamp', how='left')\n",
    "\n",
    "    # Check and delete temporary columns\n",
    "    df.drop(columns=tmp_col, errors='ignore', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.747346300Z",
     "start_time": "2023-12-05T20:42:17.248332200Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.074614Z",
     "iopub.status.busy": "2023-11-25T16:06:38.074149Z",
     "iopub.status.idle": "2023-11-25T16:06:38.097353Z",
     "shell.execute_reply": "2023-11-25T16:06:38.095841Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.074575Z"
    }
   },
   "outputs": [],
   "source": [
    "def VolumeRatio(df, mean_term=300):\n",
    "    '''\n",
    "    Pass in a df and return the df with the column 'Volume_Ratio_id'\n",
    "    '''\n",
    "    tmp_col = ['Volume_Mean']\n",
    "    \n",
    "    for id in range(14):\n",
    "        # Calculate the 'Volume Mean' column, which represents the volume mean for the previous n days\n",
    "        df[f'Volume_Mean'] = df[f'Volume_{id}'].rolling(window=mean_term).mean()\n",
    "        \n",
    "        # Calculate the 'Volume Ratio' column\n",
    "        df[f'Volume_Ratio_id{id}'] = df[f'Volume_{id}'] / df[f'Volume_Mean']\n",
    "    \n",
    "    # Check and delete temporary columns\n",
    "    df.drop(columns=tmp_col, errors='ignore', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.751347Z",
     "start_time": "2023-12-05T20:42:17.262610Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.100675Z",
     "iopub.status.busy": "2023-11-25T16:06:38.099363Z",
     "iopub.status.idle": "2023-11-25T16:06:38.113247Z",
     "shell.execute_reply": "2023-11-25T16:06:38.112063Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.100610Z"
    }
   },
   "outputs": [],
   "source": [
    "def RSI(test_df, window_size=300):\n",
    "    '''\n",
    "    Pass in a df, return the df with the 'RSI_id' columns\n",
    "    '''\n",
    "    tmp_col = ['Price_Change', 'Gain', 'Loss', 'Avg_Gain', 'Avg_Loss', 'RS']\n",
    "    \n",
    "    for id in range(14):\n",
    "        col_name = f'Close_{id}'\n",
    "    \n",
    "        # Calculate daily price changes\n",
    "        test_df['Price_Change'] = test_df[col_name].diff()\n",
    "    \n",
    "        # Categorising price changes into upward and downward\n",
    "        test_df['Gain'] = test_df['Price_Change'].apply(lambda x: x if x > 0 else 0)\n",
    "        test_df['Loss'] = test_df['Price_Change'].apply(lambda x: -x if x < 0 else 0)\n",
    "    \n",
    "        # Calculation of the average of increase and decrease\n",
    "        test_df['Avg_Gain'] = test_df['Gain'].rolling(window=window_size).mean()\n",
    "        test_df['Avg_Loss'] = test_df['Loss'].rolling(window=window_size).mean()\n",
    "    \n",
    "        # Calculation of the relative strength index（RSI）\n",
    "        test_df['RS'] = test_df['Avg_Gain'] / test_df['Avg_Loss']\n",
    "        test_df[f'RSI_id{id}'] = 100 - (100 / (1 + test_df['RS']))\n",
    "    \n",
    "    test_df.drop(tmp_col, axis=1, inplace=True)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.751347Z",
     "start_time": "2023-12-05T20:42:17.274478200Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.116246Z",
     "iopub.status.busy": "2023-11-25T16:06:38.115177Z",
     "iopub.status.idle": "2023-11-25T16:06:38.134279Z",
     "shell.execute_reply": "2023-11-25T16:06:38.132829Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.116206Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df, train=True):   \n",
    "    for id in range(14):    # 14\n",
    "        for lag in lags: # 3\n",
    "            # feature 1\n",
    "            # Convolutional (smoothing) processing + fill in the vacant bits with 1 + roll rolls 1 to the top\n",
    "            # This feature represents, for example, the logarithm of the price of asset 1 on day 60 versus the average price from day 1 - day 60\n",
    "            df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
    "            # feature 2\n",
    "            # Logarithmic returns at lagged lag period for asset with id=id\n",
    "            # This feature represents, for example, the logarithm of the ratio of the price of asset 1 on day 60 to the price on day 1\n",
    "            df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
    "    for lag in lags: #3\n",
    "        # Feature A\n",
    "        # This feature represents, for example, the mean of the 60th order lag of feature 1 for all assets. \n",
    "        df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
    "        # Feature B\n",
    "        # This feature represents, for example, the mean of the 60th order lag of feature 2 for all assets. \n",
    "        df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
    "        for id in range(14):\n",
    "            # feature 5\n",
    "            # This feature represents, for example, the difference between feature 1 and feature A of asset 1\n",
    "            df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
    "            # feature NO.6\n",
    "            # This feature represents, for example, the difference between feature 2 and feature B for asset 1\n",
    "            df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
    "\n",
    "    for id in range(14):\n",
    "        df = df.drop([f'Close_{id}'], axis=1)\n",
    "        df = df.drop([f'Volume_{id}'],axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.751347Z",
     "start_time": "2023-12-05T20:42:17.278508100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.172584Z",
     "iopub.status.busy": "2023-11-25T16:06:38.172079Z",
     "iopub.status.idle": "2023-11-25T16:06:38.180747Z",
     "shell.execute_reply": "2023-11-25T16:06:38.179099Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.172544Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_below_first_non_empty_900_nan(column):\n",
    "    first_non_empty_index = column.first_valid_index()\n",
    "    \n",
    "    if first_non_empty_index is not None:\n",
    "        column.loc[first_non_empty_index:first_non_empty_index+900] = np.nan\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.751347Z",
     "start_time": "2023-12-05T20:42:17.294545800Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T17:06:44.316577Z",
     "iopub.status.busy": "2023-11-25T17:06:44.315845Z",
     "iopub.status.idle": "2023-11-25T17:06:44.324578Z",
     "shell.execute_reply": "2023-11-25T17:06:44.323694Z",
     "shell.execute_reply.started": "2023-11-25T17:06:44.316529Z"
    }
   },
   "outputs": [],
   "source": [
    "def OverallPerformSave(current_time,formatted_time,params_range_set,adjusted_params,params,asset_id,train_score,valid_socre,feature_importance):\n",
    "    '''This is a function that outputs training results in real time during the training process. It helps us save the results of each round of training to a csv file so that we can view the results at the end of the training (or during the process).'''\n",
    "    Overall_Performance_file_path =  '/kaggle/working/' + f'training_result/{creator}/整体表现{ASSET_ID_SELECTED_FOR_TRAIN}.csv'\n",
    "    with open(Overall_Performance_file_path,'a',newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # write data\n",
    "        csv_writer.writerow([current_time,formatted_time, str(params_range_set),str(adjusted_params),str(params), asset_id, train_score, valid_socre, feature_importance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.306027900Z"
    },
    "_cell_guid": "7e703b5e-159c-48fa-b36a-e3790d3ba935",
    "_uuid": "f838143b-3361-4bff-9df7-4bc1cdb86ed3",
    "execution": {
     "iopub.execute_input": "2023-11-25T17:06:41.557533Z",
     "iopub.status.busy": "2023-11-25T17:06:41.557070Z",
     "iopub.status.idle": "2023-11-25T17:06:41.570327Z",
     "shell.execute_reply": "2023-11-25T17:06:41.568966Z",
     "shell.execute_reply.started": "2023-11-25T17:06:41.557500Z"
    }
   },
   "outputs": [],
   "source": [
    "def CreateFolderForEachTraining(creator):\n",
    "    '''\n",
    "    This is the function used to create a folder where the csv files of the training results are stored, it creates a folder named after the current time before the start of each training session, under which all the files of this training session are stored.\n",
    "    While creating the folder it creates a csv file which is written to the table header. This csv file is used to write the hyperparameters, training scores, feature importance etc. for the current round in real time during the training process.\n",
    "    '''\n",
    "    # Specify the name of the folder to be created\n",
    "    folder_name = formatted_time\n",
    "\n",
    "    # Create a folder in the current directory\n",
    "    base_path = os.getcwd() \n",
    "    folder_path = os.path.join(base_path, f'training_result/{creator}/', folder_name)\n",
    "    os.makedirs(folder_path)\n",
    "    models_trained_path = folder_path + '/models_trained'\n",
    "    os.makedirs(models_trained_path)\n",
    "    \n",
    "    Overall_Performance_file_path = '/kaggle/working/' + f'training_result/{creator}/整体表现{ASSET_ID_SELECTED_FOR_TRAIN}.csv'\n",
    "    # Check if 'Overall_Performance.csv' exists\n",
    "    if not os.path.isfile(Overall_Performance_file_path):\n",
    "        # If the file does not exist, create and write the table header\n",
    "        with open(Overall_Performance_file_path, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Output_time','Start_time', 'Params_range_set','Adjusted_Params','Params', 'Asset_id', 'Train_score', 'Valid_score','Feature_importance'])\n",
    "        \n",
    "    \n",
    "    # Write hyperparameters\n",
    "    params_file_path = os.path.join(base_path, f'training_result/{creator}/', folder_name,'params.txt')\n",
    "\n",
    "    # Open the file and write the parameters\n",
    "    with open(params_file_path, 'w') as params_file:\n",
    "        for key, value in params.items():\n",
    "            value_str = str(value)\n",
    "            params_file.write(f\"{key}: {value_str}\\n\")\n",
    "    print(f\"Parameters have been written to '{params_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.315098500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.281471Z",
     "iopub.status.busy": "2023-11-25T16:06:38.280801Z",
     "iopub.status.idle": "2023-11-25T16:06:38.301599Z",
     "shell.execute_reply": "2023-11-25T16:06:38.299477Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.281425Z"
    }
   },
   "outputs": [],
   "source": [
    "def totimestamp(x):\n",
    "    '''This is a date-timestamp conversion tool'''\n",
    "    return np.int32(time.mktime(datetime.datetime.strptime(x, \"%d/%m/%Y\").timetuple()))# totimestamp(\"21/06/2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.322310400Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.347777Z",
     "iopub.status.busy": "2023-11-25T16:06:38.346997Z",
     "iopub.status.idle": "2023-11-25T16:06:38.371233Z",
     "shell.execute_reply": "2023-11-25T16:06:38.369438Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.347730Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation(y_true, y_pred):\n",
    "    '''\n",
    "    This is a function that evaluates whether or not to let the lgbm model perform earlystopping, and it is called at the completion of each weak learner build, returning the correlation coefficient score of the strong learner that the whole model is composed of when iterating over the current weak learner. We want this score to be as large as possible, so the third value of return is True according to lgbm's rules.\n",
    "    '''\n",
    "    # Calculate your evaluation metrics here, assuming result is your evaluation result\n",
    "    correlation, _ = np.corrcoef(y_true, y_pred)\n",
    "\n",
    "    # Returns a tuple containing the name of the evaluation metric, the value of the metric, and information on whether a larger metric is better or not\n",
    "    return 'correlation', correlation[1], True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.326216Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_scorer(y_true, y_pred):\n",
    "    '''This is a function used to evaluate the correlation coefficient scores of two columns of data'''\n",
    "    correlation, _ = np.corrcoef(y_true, y_pred)\n",
    "    return correlation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.337099500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.398763Z",
     "iopub.status.busy": "2023-11-25T16:06:38.398158Z",
     "iopub.status.idle": "2023-11-25T16:06:38.414199Z",
     "shell.execute_reply": "2023-11-25T16:06:38.413020Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.398730Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_valid_splits(data,test_size):\n",
    "    '''\n",
    "    This function is called inside the my_custom_cv function as we train each asset_id\n",
    "    It returns the timestamp of the training set and validation set for the asset_id currently being trained\n",
    "    This timestamp will be passed to the my_custom_cv function for the final division of the training and validation set\n",
    "    '''\n",
    "    all_train_timestamps = data['timestamp'].unique()\n",
    "\n",
    "    whole_length = len(all_train_timestamps)\n",
    "    test_length = int(test_size * whole_length)\n",
    "\n",
    "    test_split = all_train_timestamps[-test_length:]\n",
    "    train_split = all_train_timestamps[:(whole_length-test_length)]\n",
    "\n",
    "    train_test_zip = zip([train_split], [test_split])\n",
    "    return train_test_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.345101500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.433791Z",
     "iopub.status.busy": "2023-11-25T16:06:38.433038Z",
     "iopub.status.idle": "2023-11-25T16:06:38.446430Z",
     "shell.execute_reply": "2023-11-25T16:06:38.445162Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.433748Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_custom_cv(df_proc, asset_id):\n",
    "    '''\n",
    "    This is the function used to divide the training set from the validation set\n",
    "    It returns x_train, y_train, x_val, y_val\n",
    "    '''\n",
    "    # Filter out rows with non-missing values in the f'Target_{asset_id}' column of the target variable.\n",
    "    # folder_name = formatted_time\n",
    "    df_proc = df_proc.loc[(df_proc[f'Target_{asset_id}'] == df_proc[f'Target_{asset_id}'])]\n",
    "\n",
    "    train_test_zip = get_train_valid_splits(df_proc,test_size=0.2)\n",
    "    train_split, test_split = zip(*train_test_zip)\n",
    "    train_split = [item for sublist in train_split for item in sublist]\n",
    "    test_split = [item for sublist in test_split for item in sublist]\n",
    "    gc.collect()\n",
    "\n",
    "    train_split_index = df_proc['timestamp'].isin(train_split)\n",
    "    test_split_index = df_proc['timestamp'].isin(test_split)\n",
    "\n",
    "    # Obtain sample matrices and target variables for training and validation sets\n",
    "    X_train = df_proc.loc[train_split_index, features]\n",
    "    y_train = df_proc.loc[train_split_index, f'Target_{asset_id}']\n",
    "    X_val = df_proc.loc[test_split_index, features]\n",
    "    y_val = df_proc.loc[test_split_index, f'Target_{asset_id}']\n",
    "\n",
    "    print(f\"number of train data: {X_train.shape[0]}\")\n",
    "    print(f\"number of val data:   {X_val.shape[0]}\")\n",
    "\n",
    "    yield X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:17.759348200Z",
     "start_time": "2023-12-05T20:42:17.355164100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:28:53.481014Z",
     "iopub.status.busy": "2023-11-25T16:28:53.480462Z",
     "iopub.status.idle": "2023-11-25T16:28:53.503517Z",
     "shell.execute_reply": "2023-11-25T16:28:53.502036Z",
     "shell.execute_reply.started": "2023-11-25T16:28:53.480975Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    '''\n",
    "    This is the OBJECTIVE function of the Bayesian optimiser, which is called during each round of model training\n",
    "    It guides the Bayesian optimiser in the direction we want it to go (i.e. to improve the correlation_score)\n",
    "    It returns the correlation_score on the validation set\n",
    "    '''\n",
    "    # Copy from global parameters\n",
    "    local_params = params.copy()\n",
    "\n",
    "    # Setting the parameters to be optimised\n",
    "    local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
    "    local_params['max_depth'] = trial.suggest_int('max_depth', *param_ranges['max_depth'])\n",
    "    local_params['num_leaves'] = trial.suggest_int('num_leaves', *param_ranges['num_leaves'])\n",
    "    local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
    "    local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n",
    "    local_params['max_bin'] = trial.suggest_int('max_bin', *param_ranges['max_bin'])\n",
    "    local_params['min_data_in_leaf'] = trial.suggest_int('min_data_in_leaf', *param_ranges['min_data_in_leaf'])\n",
    "\n",
    "    # Creating a LightGBM model\n",
    "    model = lgb.LGBMRegressor(**local_params,n_estimators=2500)\n",
    "\n",
    "    # Create 4 lists for receiving the true and predicted values for each fold (used to hold the results of a single round of training if k-fold cross-validation is not used)\n",
    "    all_y_pred = []\n",
    "    all_y_pred_on_train = []\n",
    "    all_y_val = []\n",
    "    all_y_train = []\n",
    "    print('current_training_id:' + f'{asset_id}')\n",
    "    print('current_Bayesian_trial:' + f'{trial_num} '+f'out of {N_TRIALS}')\n",
    "    \n",
    "    # Train each k-fold (or only 1 time if k-fold cross-validation is not used)\n",
    "    for X_train, y_train, X_val, y_val in my_custom_cv(test_df,asset_id):\n",
    "        model.fit(X_train, \n",
    "                  y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                  eval_names=['tr', 'vl'], \n",
    "                  eval_metric=correlation, )\n",
    "        \n",
    "        y_pred_on_train = model.predict(X_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        all_y_pred_on_train.extend(y_pred_on_train)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_val.extend(y_val)\n",
    "        all_y_train.extend(y_train)\n",
    "    \n",
    "    # Calculate the training set score and validation set score separately\n",
    "    all_trainset_score = correlation_scorer(all_y_pred_on_train,all_y_train)\n",
    "    all_validset_score = correlation_scorer(all_y_pred,all_y_val)\n",
    "    \n",
    "    # Get the hyperparameters for the current training, which are used to write to a csv file to record the training information\n",
    "    adjusted_params = {}\n",
    "    for key, value in trial.params.items():\n",
    "        adjusted_params[key] = value\n",
    "    \n",
    "    # Get feature importance and feature names for writing to csv file to record training information\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_names = model.booster_.feature_name()\n",
    "    # Merge into a two-dimensional list\n",
    "    merged_list = list(zip(feature_names, feature_importance))\n",
    "    # Sort by size of feature_importance\n",
    "    sorted_list = sorted(merged_list, key=lambda x: x[1], reverse=True)\n",
    "    # Convert to Dictionary\n",
    "    result_string = dict(sorted_list)\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    OverallPerformSave(current_time,formatted_time,param_ranges,adjusted_params,params,asset_id,all_trainset_score,all_validset_score,result_string)\n",
    "    print(f'training_set_score: {all_trainset_score}')\n",
    "    print(f'validation_set_score: {all_validset_score}')\n",
    "    return all_validset_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:37.128049600Z",
     "start_time": "2023-12-05T20:42:17.371353800Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:06:38.498650Z",
     "iopub.status.busy": "2023-11-25T16:06:38.498210Z",
     "iopub.status.idle": "2023-11-25T16:07:28.481941Z",
     "shell.execute_reply": "2023-11-25T16:07:28.480077Z",
     "shell.execute_reply.started": "2023-11-25T16:06:38.498610Z"
    }
   },
   "outputs": [],
   "source": [
    "#Reading a dataset\n",
    "test_df = pd.read_csv(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:37.376083800Z",
     "start_time": "2023-12-05T20:42:37.128049600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning out useless columns\n",
    "test_df = test_df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:42:37.436842400Z",
     "start_time": "2023-12-05T20:42:37.384086100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:07:28.490861Z",
     "iopub.status.busy": "2023-11-25T16:07:28.490392Z",
     "iopub.status.idle": "2023-11-25T16:07:28.951520Z",
     "shell.execute_reply": "2023-11-25T16:07:28.949897Z",
     "shell.execute_reply.started": "2023-11-25T16:07:28.490823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Close_0</th>\n",
       "      <th>Target_0</th>\n",
       "      <th>Volume_0</th>\n",
       "      <th>Close_1</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>Volume_1</th>\n",
       "      <th>Close_2</th>\n",
       "      <th>Target_2</th>\n",
       "      <th>Volume_2</th>\n",
       "      <th>Close_3</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>Volume_3</th>\n",
       "      <th>Close_4</th>\n",
       "      <th>Target_4</th>\n",
       "      <th>Volume_4</th>\n",
       "      <th>Close_5</th>\n",
       "      <th>Target_5</th>\n",
       "      <th>Volume_5</th>\n",
       "      <th>Close_6</th>\n",
       "      <th>Target_6</th>\n",
       "      <th>Volume_6</th>\n",
       "      <th>Close_7</th>\n",
       "      <th>Target_7</th>\n",
       "      <th>Volume_7</th>\n",
       "      <th>Close_8</th>\n",
       "      <th>Target_8</th>\n",
       "      <th>Volume_8</th>\n",
       "      <th>Close_9</th>\n",
       "      <th>Target_9</th>\n",
       "      <th>Volume_9</th>\n",
       "      <th>Close_10</th>\n",
       "      <th>Target_10</th>\n",
       "      <th>Volume_10</th>\n",
       "      <th>Close_11</th>\n",
       "      <th>Target_11</th>\n",
       "      <th>Volume_11</th>\n",
       "      <th>Close_12</th>\n",
       "      <th>Target_12</th>\n",
       "      <th>Volume_12</th>\n",
       "      <th>Close_13</th>\n",
       "      <th>Target_13</th>\n",
       "      <th>Volume_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>78.380000</td>\n",
       "      <td>-0.014399</td>\n",
       "      <td>78.380000</td>\n",
       "      <td>31.550062</td>\n",
       "      <td>-0.014643</td>\n",
       "      <td>31.550062</td>\n",
       "      <td>19.233005</td>\n",
       "      <td>-0.004218</td>\n",
       "      <td>19.233005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6626.713370</td>\n",
       "      <td>-0.013922</td>\n",
       "      <td>6626.713370</td>\n",
       "      <td>335.987856</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>335.987856</td>\n",
       "      <td>121.087310</td>\n",
       "      <td>-0.008264</td>\n",
       "      <td>121.087310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>411.896642</td>\n",
       "      <td>-0.009791</td>\n",
       "      <td>411.896642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.635710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.635710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764920</td>\n",
       "      <td>71.390000</td>\n",
       "      <td>-0.015875</td>\n",
       "      <td>71.390000</td>\n",
       "      <td>31.046432</td>\n",
       "      <td>-0.015037</td>\n",
       "      <td>31.046432</td>\n",
       "      <td>24.050259</td>\n",
       "      <td>-0.004079</td>\n",
       "      <td>24.050259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3277.475494</td>\n",
       "      <td>-0.014534</td>\n",
       "      <td>3277.475494</td>\n",
       "      <td>232.793141</td>\n",
       "      <td>-0.004441</td>\n",
       "      <td>232.793141</td>\n",
       "      <td>1.468019</td>\n",
       "      <td>-0.029902</td>\n",
       "      <td>1.468019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3640.502706</td>\n",
       "      <td>-0.012991</td>\n",
       "      <td>3640.502706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.349420</td>\n",
       "      <td>-0.009690</td>\n",
       "      <td>0.349420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1514764980</td>\n",
       "      <td>1546.820000</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>1546.820000</td>\n",
       "      <td>55.061820</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>55.061820</td>\n",
       "      <td>42.676438</td>\n",
       "      <td>-0.002892</td>\n",
       "      <td>42.676438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5623.557585</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>5623.557585</td>\n",
       "      <td>174.138031</td>\n",
       "      <td>-0.004206</td>\n",
       "      <td>174.138031</td>\n",
       "      <td>76.163922</td>\n",
       "      <td>-0.030832</td>\n",
       "      <td>76.163922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328.350286</td>\n",
       "      <td>-0.003572</td>\n",
       "      <td>328.350286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.189553</td>\n",
       "      <td>0.006567</td>\n",
       "      <td>1.189553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136957</th>\n",
       "      <td>1642982280</td>\n",
       "      <td>496.467537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>496.467537</td>\n",
       "      <td>49.867700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.867700</td>\n",
       "      <td>198.196485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198.196485</td>\n",
       "      <td>430719.654610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>430719.654610</td>\n",
       "      <td>7.433686e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.433686e+05</td>\n",
       "      <td>14907.125055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14907.125055</td>\n",
       "      <td>821.614133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>821.614133</td>\n",
       "      <td>1334.601131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1334.601131</td>\n",
       "      <td>20511.345658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20511.345658</td>\n",
       "      <td>790.632440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>790.632440</td>\n",
       "      <td>2.142748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.142748</td>\n",
       "      <td>21.597886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.597886</td>\n",
       "      <td>7.076212e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.076212e+05</td>\n",
       "      <td>1.301079e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.301079e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136958</th>\n",
       "      <td>1642982340</td>\n",
       "      <td>368.684921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>368.684921</td>\n",
       "      <td>43.030556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.030556</td>\n",
       "      <td>76.954035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.954035</td>\n",
       "      <td>352929.794282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352929.794282</td>\n",
       "      <td>7.287195e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.287195e+05</td>\n",
       "      <td>14559.759824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14559.759824</td>\n",
       "      <td>749.548301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>749.548301</td>\n",
       "      <td>2410.205214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2410.205214</td>\n",
       "      <td>9064.791667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9064.791667</td>\n",
       "      <td>331.208442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331.208442</td>\n",
       "      <td>3.985169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.985169</td>\n",
       "      <td>51.115980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.115980</td>\n",
       "      <td>1.988516e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.988516e+05</td>\n",
       "      <td>9.245588e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.245588e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136959</th>\n",
       "      <td>1642982400</td>\n",
       "      <td>1490.571077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1490.571077</td>\n",
       "      <td>110.053151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.053151</td>\n",
       "      <td>209.645675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.645675</td>\n",
       "      <td>249923.229256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249923.229256</td>\n",
       "      <td>1.009688e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.009688e+06</td>\n",
       "      <td>202194.015531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202194.015531</td>\n",
       "      <td>1057.765667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057.765667</td>\n",
       "      <td>6709.816310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6709.816310</td>\n",
       "      <td>6019.269834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6019.269834</td>\n",
       "      <td>1243.860266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1243.860266</td>\n",
       "      <td>4.190290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.190290</td>\n",
       "      <td>74.397471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.397471</td>\n",
       "      <td>2.983796e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.983796e+06</td>\n",
       "      <td>8.988253e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.988253e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2136960 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp      Close_0  Target_0     Volume_0     Close_1  Target_1  \\\n",
       "0        1514764860    78.380000 -0.014399    78.380000   31.550062 -0.014643   \n",
       "1        1514764920    71.390000 -0.015875    71.390000   31.046432 -0.015037   \n",
       "2        1514764980  1546.820000 -0.015410  1546.820000   55.061820 -0.010309   \n",
       "...             ...          ...       ...          ...         ...       ...   \n",
       "2136957  1642982280   496.467537       NaN   496.467537   49.867700       NaN   \n",
       "2136958  1642982340   368.684921       NaN   368.684921   43.030556       NaN   \n",
       "2136959  1642982400  1490.571077       NaN  1490.571077  110.053151       NaN   \n",
       "\n",
       "           Volume_1     Close_2  Target_2    Volume_2        Close_3  \\\n",
       "0         31.550062   19.233005 -0.004218   19.233005            NaN   \n",
       "1         31.046432   24.050259 -0.004079   24.050259            NaN   \n",
       "2         55.061820   42.676438 -0.002892   42.676438            NaN   \n",
       "...             ...         ...       ...         ...            ...   \n",
       "2136957   49.867700  198.196485       NaN  198.196485  430719.654610   \n",
       "2136958   43.030556   76.954035       NaN   76.954035  352929.794282   \n",
       "2136959  110.053151  209.645675       NaN  209.645675  249923.229256   \n",
       "\n",
       "         Target_3       Volume_3       Close_4  Target_4      Volume_4  \\\n",
       "0             NaN            NaN           NaN       NaN           NaN   \n",
       "1             NaN            NaN           NaN       NaN           NaN   \n",
       "2             NaN            NaN           NaN       NaN           NaN   \n",
       "...           ...            ...           ...       ...           ...   \n",
       "2136957       NaN  430719.654610  7.433686e+05       NaN  7.433686e+05   \n",
       "2136958       NaN  352929.794282  7.287195e+05       NaN  7.287195e+05   \n",
       "2136959       NaN  249923.229256  1.009688e+06       NaN  1.009688e+06   \n",
       "\n",
       "               Close_5  Target_5       Volume_5      Close_6  Target_6  \\\n",
       "0          6626.713370 -0.013922    6626.713370   335.987856 -0.004809   \n",
       "1          3277.475494 -0.014534    3277.475494   232.793141 -0.004441   \n",
       "2          5623.557585 -0.012546    5623.557585   174.138031 -0.004206   \n",
       "...                ...       ...            ...          ...       ...   \n",
       "2136957   14907.125055       NaN   14907.125055   821.614133       NaN   \n",
       "2136958   14559.759824       NaN   14559.759824   749.548301       NaN   \n",
       "2136959  202194.015531       NaN  202194.015531  1057.765667       NaN   \n",
       "\n",
       "            Volume_6      Close_7  Target_7     Volume_7       Close_8  \\\n",
       "0         335.987856   121.087310 -0.008264   121.087310           NaN   \n",
       "1         232.793141     1.468019 -0.029902     1.468019           NaN   \n",
       "2         174.138031    76.163922 -0.030832    76.163922           NaN   \n",
       "...              ...          ...       ...          ...           ...   \n",
       "2136957   821.614133  1334.601131       NaN  1334.601131  20511.345658   \n",
       "2136958   749.548301  2410.205214       NaN  2410.205214   9064.791667   \n",
       "2136959  1057.765667  6709.816310       NaN  6709.816310   6019.269834   \n",
       "\n",
       "         Target_8      Volume_8      Close_9  Target_9     Volume_9  Close_10  \\\n",
       "0             NaN           NaN   411.896642 -0.009791   411.896642       NaN   \n",
       "1             NaN           NaN  3640.502706 -0.012991  3640.502706       NaN   \n",
       "2             NaN           NaN   328.350286 -0.003572   328.350286       NaN   \n",
       "...           ...           ...          ...       ...          ...       ...   \n",
       "2136957       NaN  20511.345658   790.632440       NaN   790.632440  2.142748   \n",
       "2136958       NaN   9064.791667   331.208442       NaN   331.208442  3.985169   \n",
       "2136959       NaN   6019.269834  1243.860266       NaN  1243.860266  4.190290   \n",
       "\n",
       "         Target_10  Volume_10   Close_11  Target_11  Volume_11      Close_12  \\\n",
       "0              NaN        NaN   6.635710        NaN   6.635710           NaN   \n",
       "1              NaN        NaN   0.349420  -0.009690   0.349420           NaN   \n",
       "2              NaN        NaN   1.189553   0.006567   1.189553           NaN   \n",
       "...            ...        ...        ...        ...        ...           ...   \n",
       "2136957        NaN   2.142748  21.597886        NaN  21.597886  7.076212e+05   \n",
       "2136958        NaN   3.985169  51.115980        NaN  51.115980  1.988516e+05   \n",
       "2136959        NaN   4.190290  74.397471        NaN  74.397471  2.983796e+06   \n",
       "\n",
       "         Target_12     Volume_12      Close_13  Target_13     Volume_13  \n",
       "0              NaN           NaN           NaN        NaN           NaN  \n",
       "1              NaN           NaN           NaN        NaN           NaN  \n",
       "2              NaN           NaN           NaN        NaN           NaN  \n",
       "...            ...           ...           ...        ...           ...  \n",
       "2136957        NaN  7.076212e+05  1.301079e+06        NaN  1.301079e+06  \n",
       "2136958        NaN  1.988516e+05  9.245588e+05        NaN  9.245588e+05  \n",
       "2136959        NaN  2.983796e+06  8.988253e+05        NaN  8.988253e+05  \n",
       "\n",
       "[2136960 rows x 43 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:44:45.072679800Z",
     "start_time": "2023-12-05T20:42:37.435715700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 701.06 MB\n",
      "Memory usage after optimization is: 260.86 MB\n",
      "Decreased by 62.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: overflow encountered in divide\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}_id{id}'] = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}_id{id}']     = np.log( np.array(df[f'Close_{id}']) /  np.roll(np.array(df[f'Close_{id}']), lag)  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\Python\\3.11.4\\Lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\Python\\3.11.4\\Lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_close/mean_{lag}'] =  np.mean(df.iloc[:,df.columns.str.startswith(f'log_close/mean_{lag}_id')], axis=1)\n",
      "C:\\Users\\严厉的父亲\\Python\\3.11.4\\Lib\\site-packages\\numpy\\core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'mean_log_returns_{lag}'] = np.mean(df.iloc[:,df.columns.str.startswith(f'log_return_{lag}_id')] ,    axis=1)\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: RuntimeWarning: invalid value encountered in subtract\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_close/mean_{lag}-mean_close/mean_{lag}_id{id}'] = np.array( df[f'log_close/mean_{lag}_id{id}']) - np.array( df[f'mean_close/mean_{lag}']  )\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\3330322436.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'log_return_{lag}-mean_log_returns_{lag}_id{id}']    = np.array( df[f'log_return_{lag}_id{id}'])     - np.array( df[f'mean_log_returns_{lag}'] )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2828.69 MB\n",
      "Memory usage after optimization is: 2262.14 MB\n",
      "Decreased by 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Call custom feature engineering functions to get features\n",
    "test_df = reduce_mem_usage(test_df)\n",
    "test_df=MACD(test_df)\n",
    "test_df=VolumeRatio(test_df)\n",
    "test_df = RSI(test_df)\n",
    "test_df=get_features(test_df)\n",
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:03.428620500Z",
     "start_time": "2023-12-05T20:44:45.080680300Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:07:28.955012Z",
     "iopub.status.busy": "2023-11-25T16:07:28.954199Z",
     "iopub.status.idle": "2023-11-25T16:07:29.028168Z",
     "shell.execute_reply": "2023-11-25T16:07:29.026142Z",
     "shell.execute_reply.started": "2023-11-25T16:07:28.954970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the first 900 non-null values (eliminating rows affected by the maximum lag order)\n",
    "test_df.iloc[:, 15:] = test_df.iloc[:, 15:].apply(replace_below_first_non_empty_900_nan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:03.503124400Z",
     "start_time": "2023-12-05T20:45:03.498624600Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:07:29.031476Z",
     "iopub.status.busy": "2023-11-25T16:07:29.030814Z",
     "iopub.status.idle": "2023-11-25T16:07:29.097020Z",
     "shell.execute_reply": "2023-11-25T16:07:29.095396Z",
     "shell.execute_reply.started": "2023-11-25T16:07:29.031422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the first 900 rows (because the targets in the first 900 rows can't be used for training because their corresponding feature values are all NaN)\n",
    "test_df = test_df.iloc[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:15.568463300Z",
     "start_time": "2023-12-05T20:45:03.508110500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:07:29.099380Z",
     "iopub.status.busy": "2023-11-25T16:07:29.098862Z",
     "iopub.status.idle": "2023-11-25T16:10:34.585708Z",
     "shell.execute_reply": "2023-11-25T16:10:34.584252Z",
     "shell.execute_reply.started": "2023-11-25T16:07:29.099331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2135.72 MB\n",
      "Memory usage after optimization is: 2135.72 MB\n",
      "Decreased by 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Leaving 132,961 rows of data from the last final 3 months (2021-10-24 - 2022-1-24) as the prediction set for the final inference stage.\n",
    "# At this point the training data is ready, it is 2003099*231 df, named test_df\n",
    "test_df = test_df.loc[test_df['timestamp']<1635004800,]\n",
    "test_df = reduce_mem_usage(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:15.754516Z",
     "start_time": "2023-12-05T20:45:15.564460Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:10:34.588381Z",
     "iopub.status.busy": "2023-11-25T16:10:34.587846Z",
     "iopub.status.idle": "2023-11-25T16:10:44.911868Z",
     "shell.execute_reply": "2023-11-25T16:10:44.910550Z",
     "shell.execute_reply.started": "2023-11-25T16:10:34.588338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Target_0</th>\n",
       "      <th>Target_1</th>\n",
       "      <th>Target_2</th>\n",
       "      <th>Target_3</th>\n",
       "      <th>Target_4</th>\n",
       "      <th>Target_5</th>\n",
       "      <th>Target_6</th>\n",
       "      <th>Target_7</th>\n",
       "      <th>Target_8</th>\n",
       "      <th>Target_9</th>\n",
       "      <th>Target_10</th>\n",
       "      <th>Target_11</th>\n",
       "      <th>Target_12</th>\n",
       "      <th>Target_13</th>\n",
       "      <th>MACD_id0</th>\n",
       "      <th>MACD_id1</th>\n",
       "      <th>MACD_id2</th>\n",
       "      <th>MACD_id3</th>\n",
       "      <th>MACD_id4</th>\n",
       "      <th>MACD_id5</th>\n",
       "      <th>MACD_id6</th>\n",
       "      <th>MACD_id7</th>\n",
       "      <th>MACD_id8</th>\n",
       "      <th>MACD_id9</th>\n",
       "      <th>MACD_id10</th>\n",
       "      <th>MACD_id11</th>\n",
       "      <th>MACD_id12</th>\n",
       "      <th>MACD_id13</th>\n",
       "      <th>Volume_Ratio_id0</th>\n",
       "      <th>Volume_Ratio_id1</th>\n",
       "      <th>Volume_Ratio_id2</th>\n",
       "      <th>Volume_Ratio_id3</th>\n",
       "      <th>Volume_Ratio_id4</th>\n",
       "      <th>Volume_Ratio_id5</th>\n",
       "      <th>Volume_Ratio_id6</th>\n",
       "      <th>Volume_Ratio_id7</th>\n",
       "      <th>Volume_Ratio_id8</th>\n",
       "      <th>Volume_Ratio_id9</th>\n",
       "      <th>Volume_Ratio_id10</th>\n",
       "      <th>Volume_Ratio_id11</th>\n",
       "      <th>Volume_Ratio_id12</th>\n",
       "      <th>Volume_Ratio_id13</th>\n",
       "      <th>RSI_id0</th>\n",
       "      <th>RSI_id1</th>\n",
       "      <th>RSI_id2</th>\n",
       "      <th>RSI_id3</th>\n",
       "      <th>RSI_id4</th>\n",
       "      <th>RSI_id5</th>\n",
       "      <th>RSI_id6</th>\n",
       "      <th>RSI_id7</th>\n",
       "      <th>RSI_id8</th>\n",
       "      <th>RSI_id9</th>\n",
       "      <th>RSI_id10</th>\n",
       "      <th>RSI_id11</th>\n",
       "      <th>RSI_id12</th>\n",
       "      <th>RSI_id13</th>\n",
       "      <th>log_close/mean_60_id0</th>\n",
       "      <th>log_return_60_id0</th>\n",
       "      <th>log_close/mean_300_id0</th>\n",
       "      <th>log_return_300_id0</th>\n",
       "      <th>log_close/mean_900_id0</th>\n",
       "      <th>log_return_900_id0</th>\n",
       "      <th>log_close/mean_60_id1</th>\n",
       "      <th>log_return_60_id1</th>\n",
       "      <th>log_close/mean_300_id1</th>\n",
       "      <th>log_return_300_id1</th>\n",
       "      <th>log_close/mean_900_id1</th>\n",
       "      <th>log_return_900_id1</th>\n",
       "      <th>log_close/mean_60_id2</th>\n",
       "      <th>log_return_60_id2</th>\n",
       "      <th>log_close/mean_300_id2</th>\n",
       "      <th>log_return_300_id2</th>\n",
       "      <th>log_close/mean_900_id2</th>\n",
       "      <th>log_return_900_id2</th>\n",
       "      <th>log_close/mean_60_id3</th>\n",
       "      <th>log_return_60_id3</th>\n",
       "      <th>log_close/mean_300_id3</th>\n",
       "      <th>log_return_300_id3</th>\n",
       "      <th>log_close/mean_900_id3</th>\n",
       "      <th>log_return_900_id3</th>\n",
       "      <th>log_close/mean_60_id4</th>\n",
       "      <th>log_return_60_id4</th>\n",
       "      <th>log_close/mean_300_id4</th>\n",
       "      <th>log_return_300_id4</th>\n",
       "      <th>log_close/mean_900_id4</th>\n",
       "      <th>log_return_900_id4</th>\n",
       "      <th>log_close/mean_60_id5</th>\n",
       "      <th>log_return_60_id5</th>\n",
       "      <th>log_close/mean_300_id5</th>\n",
       "      <th>log_return_300_id5</th>\n",
       "      <th>log_close/mean_900_id5</th>\n",
       "      <th>log_return_900_id5</th>\n",
       "      <th>log_close/mean_60_id6</th>\n",
       "      <th>log_return_60_id6</th>\n",
       "      <th>log_close/mean_300_id6</th>\n",
       "      <th>log_return_300_id6</th>\n",
       "      <th>log_close/mean_900_id6</th>\n",
       "      <th>log_return_900_id6</th>\n",
       "      <th>log_close/mean_60_id7</th>\n",
       "      <th>log_return_60_id7</th>\n",
       "      <th>log_close/mean_300_id7</th>\n",
       "      <th>log_return_300_id7</th>\n",
       "      <th>log_close/mean_900_id7</th>\n",
       "      <th>log_return_900_id7</th>\n",
       "      <th>log_close/mean_60_id8</th>\n",
       "      <th>log_return_60_id8</th>\n",
       "      <th>log_close/mean_300_id8</th>\n",
       "      <th>log_return_300_id8</th>\n",
       "      <th>log_close/mean_900_id8</th>\n",
       "      <th>log_return_900_id8</th>\n",
       "      <th>log_close/mean_60_id9</th>\n",
       "      <th>log_return_60_id9</th>\n",
       "      <th>log_close/mean_300_id9</th>\n",
       "      <th>log_return_300_id9</th>\n",
       "      <th>log_close/mean_900_id9</th>\n",
       "      <th>log_return_900_id9</th>\n",
       "      <th>log_close/mean_60_id10</th>\n",
       "      <th>log_return_60_id10</th>\n",
       "      <th>log_close/mean_300_id10</th>\n",
       "      <th>log_return_300_id10</th>\n",
       "      <th>log_close/mean_900_id10</th>\n",
       "      <th>log_return_900_id10</th>\n",
       "      <th>log_close/mean_60_id11</th>\n",
       "      <th>log_return_60_id11</th>\n",
       "      <th>log_close/mean_300_id11</th>\n",
       "      <th>log_return_300_id11</th>\n",
       "      <th>log_close/mean_900_id11</th>\n",
       "      <th>log_return_900_id11</th>\n",
       "      <th>log_close/mean_60_id12</th>\n",
       "      <th>log_return_60_id12</th>\n",
       "      <th>log_close/mean_300_id12</th>\n",
       "      <th>log_return_300_id12</th>\n",
       "      <th>log_close/mean_900_id12</th>\n",
       "      <th>log_return_900_id12</th>\n",
       "      <th>log_close/mean_60_id13</th>\n",
       "      <th>log_return_60_id13</th>\n",
       "      <th>log_close/mean_300_id13</th>\n",
       "      <th>log_return_300_id13</th>\n",
       "      <th>log_close/mean_900_id13</th>\n",
       "      <th>log_return_900_id13</th>\n",
       "      <th>mean_close/mean_60</th>\n",
       "      <th>mean_log_returns_60</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id0</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id0</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id1</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id1</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id2</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id2</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id3</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id3</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id4</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id4</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id5</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id5</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id6</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id6</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id7</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id7</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id8</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id8</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id9</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id9</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id10</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id10</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id11</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id11</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id12</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id12</th>\n",
       "      <th>log_close/mean_60-mean_close/mean_60_id13</th>\n",
       "      <th>log_return_60-mean_log_returns_60_id13</th>\n",
       "      <th>mean_close/mean_300</th>\n",
       "      <th>mean_log_returns_300</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id0</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id0</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id1</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id1</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id2</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id2</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id3</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id3</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id4</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id4</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id5</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id5</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id6</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id6</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id7</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id7</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id8</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id8</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id9</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id9</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id10</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id10</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id11</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id11</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id12</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id12</th>\n",
       "      <th>log_close/mean_300-mean_close/mean_300_id13</th>\n",
       "      <th>log_return_300-mean_log_returns_300_id13</th>\n",
       "      <th>mean_close/mean_900</th>\n",
       "      <th>mean_log_returns_900</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id0</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id0</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id1</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id1</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id2</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id2</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id3</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id3</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id4</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id4</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id5</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id5</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id6</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id6</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id7</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id7</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id8</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id8</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id9</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id9</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id10</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id10</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id11</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id11</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id12</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id12</th>\n",
       "      <th>log_close/mean_900-mean_close/mean_900_id13</th>\n",
       "      <th>log_return_900-mean_log_returns_900_id13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>1514818860</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.009682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.027252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>1514818920</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032043</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011620</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488525</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>0.127075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.289062</td>\n",
       "      <td>0.421143</td>\n",
       "      <td>0.331189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.437500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857422</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>1.074219</td>\n",
       "      <td>3.695312</td>\n",
       "      <td>1.310547</td>\n",
       "      <td>3.330078</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>1.240234</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>1.057617</td>\n",
       "      <td>-0.846680</td>\n",
       "      <td>0.495605</td>\n",
       "      <td>-0.427979</td>\n",
       "      <td>1.048828</td>\n",
       "      <td>-0.279785</td>\n",
       "      <td>-0.594727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>3.751953</td>\n",
       "      <td>0.409912</td>\n",
       "      <td>0.690918</td>\n",
       "      <td>0.621094</td>\n",
       "      <td>-0.249268</td>\n",
       "      <td>-0.369141</td>\n",
       "      <td>0.486084</td>\n",
       "      <td>-0.142944</td>\n",
       "      <td>-0.592773</td>\n",
       "      <td>-0.133667</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>-0.740234</td>\n",
       "      <td>1.807617</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>1.960938</td>\n",
       "      <td>-0.985352</td>\n",
       "      <td>5.675781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.486572</td>\n",
       "      <td>-0.113342</td>\n",
       "      <td>-0.091858</td>\n",
       "      <td>1.287109</td>\n",
       "      <td>-0.411377</td>\n",
       "      <td>-2.744141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.315619</td>\n",
       "      <td>2.146484</td>\n",
       "      <td>-0.837071</td>\n",
       "      <td>-0.014755</td>\n",
       "      <td>-0.754139</td>\n",
       "      <td>3.257812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.360293</td>\n",
       "      <td>1.248572</td>\n",
       "      <td>1.217481</td>\n",
       "      <td>-0.833078</td>\n",
       "      <td>0.529750</td>\n",
       "      <td>-0.250525</td>\n",
       "      <td>-0.486544</td>\n",
       "      <td>-0.752966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.209732</td>\n",
       "      <td>2.503654</td>\n",
       "      <td>-0.008902</td>\n",
       "      <td>-0.762539</td>\n",
       "      <td>-0.379919</td>\n",
       "      <td>0.559472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.126272</td>\n",
       "      <td>-1.361931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.955327</td>\n",
       "      <td>0.897913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005624</td>\n",
       "      <td>1.164455</td>\n",
       "      <td>1.080069</td>\n",
       "      <td>2.530615</td>\n",
       "      <td>0.601179</td>\n",
       "      <td>0.075779</td>\n",
       "      <td>-0.422472</td>\n",
       "      <td>-0.115627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>-0.473736</td>\n",
       "      <td>-0.137376</td>\n",
       "      <td>-1.757041</td>\n",
       "      <td>-0.619332</td>\n",
       "      <td>0.796323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086261</td>\n",
       "      <td>0.122897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.831447</td>\n",
       "      <td>-1.179210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>1.175482</td>\n",
       "      <td>1.293650</td>\n",
       "      <td>2.155421</td>\n",
       "      <td>0.753307</td>\n",
       "      <td>-0.117865</td>\n",
       "      <td>-0.297075</td>\n",
       "      <td>-1.770209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>-1.424733</td>\n",
       "      <td>-0.151005</td>\n",
       "      <td>-1.506588</td>\n",
       "      <td>-1.002505</td>\n",
       "      <td>4.500334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.428646</td>\n",
       "      <td>-3.918690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.771426</td>\n",
       "      <td>2.082330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>1514818980</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033173</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.286133</td>\n",
       "      <td>0.014160</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.031250</td>\n",
       "      <td>0.100159</td>\n",
       "      <td>-1.023582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.041992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.162842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.324219</td>\n",
       "      <td>-3.894531</td>\n",
       "      <td>-2.181641</td>\n",
       "      <td>-0.871094</td>\n",
       "      <td>-1.942383</td>\n",
       "      <td>-3.001953</td>\n",
       "      <td>-0.929199</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>-0.508301</td>\n",
       "      <td>0.215332</td>\n",
       "      <td>-0.332520</td>\n",
       "      <td>-0.619629</td>\n",
       "      <td>-1.195312</td>\n",
       "      <td>0.536621</td>\n",
       "      <td>-0.774902</td>\n",
       "      <td>0.797852</td>\n",
       "      <td>-0.623535</td>\n",
       "      <td>-1.514648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.711914</td>\n",
       "      <td>2.287109</td>\n",
       "      <td>1.293945</td>\n",
       "      <td>1.583984</td>\n",
       "      <td>1.513672</td>\n",
       "      <td>0.104248</td>\n",
       "      <td>-1.117188</td>\n",
       "      <td>-0.688965</td>\n",
       "      <td>-0.896973</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>-0.886230</td>\n",
       "      <td>-0.793945</td>\n",
       "      <td>-1.547852</td>\n",
       "      <td>-1.286133</td>\n",
       "      <td>-1.441406</td>\n",
       "      <td>-0.969238</td>\n",
       "      <td>-1.802734</td>\n",
       "      <td>0.909668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.562988</td>\n",
       "      <td>-0.014412</td>\n",
       "      <td>-0.170898</td>\n",
       "      <td>1.787109</td>\n",
       "      <td>-0.487793</td>\n",
       "      <td>-0.414062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.775507</td>\n",
       "      <td>3.205078</td>\n",
       "      <td>1.277415</td>\n",
       "      <td>5.554688</td>\n",
       "      <td>1.368025</td>\n",
       "      <td>4.160156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.773667</td>\n",
       "      <td>-0.053340</td>\n",
       "      <td>-1.550377</td>\n",
       "      <td>-3.841115</td>\n",
       "      <td>-0.155529</td>\n",
       "      <td>-0.516972</td>\n",
       "      <td>-0.421319</td>\n",
       "      <td>0.589962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.485487</td>\n",
       "      <td>2.339701</td>\n",
       "      <td>-0.343298</td>\n",
       "      <td>-0.635759</td>\n",
       "      <td>-0.774664</td>\n",
       "      <td>-1.233167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.549174</td>\n",
       "      <td>3.258419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.425388</td>\n",
       "      <td>1.101220</td>\n",
       "      <td>-1.756459</td>\n",
       "      <td>-1.972388</td>\n",
       "      <td>-0.083140</td>\n",
       "      <td>-0.885888</td>\n",
       "      <td>-0.349605</td>\n",
       "      <td>-0.303369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.719062</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>-0.471470</td>\n",
       "      <td>-0.390406</td>\n",
       "      <td>-1.015638</td>\n",
       "      <td>-2.070456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254448</td>\n",
       "      <td>0.686177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.702803</td>\n",
       "      <td>4.453467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.399232</td>\n",
       "      <td>-0.146278</td>\n",
       "      <td>-1.543607</td>\n",
       "      <td>-2.855433</td>\n",
       "      <td>0.066813</td>\n",
       "      <td>-0.473351</td>\n",
       "      <td>-0.224501</td>\n",
       "      <td>-1.368371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.913083</td>\n",
       "      <td>0.250501</td>\n",
       "      <td>-0.487098</td>\n",
       "      <td>-0.647692</td>\n",
       "      <td>-1.403457</td>\n",
       "      <td>1.055759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.088491</td>\n",
       "      <td>-0.267847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.767257</td>\n",
       "      <td>4.306434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003996</th>\n",
       "      <td>1635004620</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>-0.002438</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>1.094727</td>\n",
       "      <td>0.097961</td>\n",
       "      <td>-0.300293</td>\n",
       "      <td>591.418579</td>\n",
       "      <td>3287.896240</td>\n",
       "      <td>-1.572266</td>\n",
       "      <td>5.027344</td>\n",
       "      <td>1.399485</td>\n",
       "      <td>37.312500</td>\n",
       "      <td>-3.355469</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.059967</td>\n",
       "      <td>-346.152832</td>\n",
       "      <td>5289.912109</td>\n",
       "      <td>1.033203</td>\n",
       "      <td>0.941406</td>\n",
       "      <td>0.355469</td>\n",
       "      <td>0.974609</td>\n",
       "      <td>1.374023</td>\n",
       "      <td>0.694336</td>\n",
       "      <td>1.184570</td>\n",
       "      <td>0.766113</td>\n",
       "      <td>1.719727</td>\n",
       "      <td>0.454590</td>\n",
       "      <td>1.046875</td>\n",
       "      <td>0.607910</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>1.423828</td>\n",
       "      <td>50.03125</td>\n",
       "      <td>49.40625</td>\n",
       "      <td>49.90625</td>\n",
       "      <td>49.875</td>\n",
       "      <td>50.03125</td>\n",
       "      <td>50.06250</td>\n",
       "      <td>50.125</td>\n",
       "      <td>50.09375</td>\n",
       "      <td>49.8750</td>\n",
       "      <td>50.03125</td>\n",
       "      <td>49.96875</td>\n",
       "      <td>49.84375</td>\n",
       "      <td>49.87500</td>\n",
       "      <td>49.71875</td>\n",
       "      <td>0.131592</td>\n",
       "      <td>0.309326</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.130005</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>-0.519531</td>\n",
       "      <td>-0.060272</td>\n",
       "      <td>-1.182617</td>\n",
       "      <td>0.408936</td>\n",
       "      <td>0.496826</td>\n",
       "      <td>-0.542480</td>\n",
       "      <td>-0.487305</td>\n",
       "      <td>-1.034180</td>\n",
       "      <td>-0.738770</td>\n",
       "      <td>-0.660156</td>\n",
       "      <td>-0.027725</td>\n",
       "      <td>-0.019012</td>\n",
       "      <td>0.247559</td>\n",
       "      <td>-0.025635</td>\n",
       "      <td>-0.417969</td>\n",
       "      <td>0.239746</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.422119</td>\n",
       "      <td>1.418945</td>\n",
       "      <td>0.317627</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.384277</td>\n",
       "      <td>0.581543</td>\n",
       "      <td>-0.476562</td>\n",
       "      <td>-0.318359</td>\n",
       "      <td>-0.364502</td>\n",
       "      <td>0.305176</td>\n",
       "      <td>0.070068</td>\n",
       "      <td>-0.506348</td>\n",
       "      <td>-0.435303</td>\n",
       "      <td>1.681641</td>\n",
       "      <td>0.169067</td>\n",
       "      <td>0.397705</td>\n",
       "      <td>0.500488</td>\n",
       "      <td>-1.287109</td>\n",
       "      <td>-0.559570</td>\n",
       "      <td>0.260498</td>\n",
       "      <td>-0.266357</td>\n",
       "      <td>0.839355</td>\n",
       "      <td>0.024139</td>\n",
       "      <td>0.461182</td>\n",
       "      <td>0.734863</td>\n",
       "      <td>1.751953</td>\n",
       "      <td>0.541992</td>\n",
       "      <td>-0.331055</td>\n",
       "      <td>0.661133</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.506836</td>\n",
       "      <td>-0.243164</td>\n",
       "      <td>-0.788574</td>\n",
       "      <td>0.574707</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>-1.027344</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>1.059570</td>\n",
       "      <td>0.045966</td>\n",
       "      <td>-0.152100</td>\n",
       "      <td>0.397991</td>\n",
       "      <td>2.060547</td>\n",
       "      <td>-0.312255</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>-0.497678</td>\n",
       "      <td>-0.950684</td>\n",
       "      <td>-0.257082</td>\n",
       "      <td>0.651855</td>\n",
       "      <td>-0.316650</td>\n",
       "      <td>0.684570</td>\n",
       "      <td>-0.124878</td>\n",
       "      <td>-0.517578</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.463867</td>\n",
       "      <td>0.615723</td>\n",
       "      <td>1.342773</td>\n",
       "      <td>0.353271</td>\n",
       "      <td>-0.359131</td>\n",
       "      <td>0.632324</td>\n",
       "      <td>1.111328</td>\n",
       "      <td>-0.085742</td>\n",
       "      <td>0.515693</td>\n",
       "      <td>0.217382</td>\n",
       "      <td>-0.206380</td>\n",
       "      <td>0.142377</td>\n",
       "      <td>-1.035224</td>\n",
       "      <td>-0.456515</td>\n",
       "      <td>-1.002998</td>\n",
       "      <td>0.066731</td>\n",
       "      <td>-0.268155</td>\n",
       "      <td>0.507831</td>\n",
       "      <td>0.903592</td>\n",
       "      <td>-0.390857</td>\n",
       "      <td>-0.834015</td>\n",
       "      <td>-0.349589</td>\n",
       "      <td>1.165816</td>\n",
       "      <td>-0.474044</td>\n",
       "      <td>-0.255259</td>\n",
       "      <td>0.820498</td>\n",
       "      <td>1.235934</td>\n",
       "      <td>-0.421058</td>\n",
       "      <td>-0.758873</td>\n",
       "      <td>0.093211</td>\n",
       "      <td>0.543877</td>\n",
       "      <td>-0.226513</td>\n",
       "      <td>-0.483985</td>\n",
       "      <td>-0.230942</td>\n",
       "      <td>0.168904</td>\n",
       "      <td>0.701487</td>\n",
       "      <td>0.826765</td>\n",
       "      <td>-0.121514</td>\n",
       "      <td>-0.160677</td>\n",
       "      <td>0.154359</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.061234</td>\n",
       "      <td>-1.021940</td>\n",
       "      <td>-0.912795</td>\n",
       "      <td>-0.578093</td>\n",
       "      <td>0.095884</td>\n",
       "      <td>-0.257392</td>\n",
       "      <td>0.439124</td>\n",
       "      <td>0.314017</td>\n",
       "      <td>-0.243098</td>\n",
       "      <td>0.465861</td>\n",
       "      <td>0.290528</td>\n",
       "      <td>0.558469</td>\n",
       "      <td>-0.144752</td>\n",
       "      <td>1.000013</td>\n",
       "      <td>0.663690</td>\n",
       "      <td>-0.170391</td>\n",
       "      <td>-0.666893</td>\n",
       "      <td>0.735357</td>\n",
       "      <td>0.167480</td>\n",
       "      <td>0.008577</td>\n",
       "      <td>-0.376164</td>\n",
       "      <td>-0.790007</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>-0.356717</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>-0.198456</td>\n",
       "      <td>0.174812</td>\n",
       "      <td>0.102076</td>\n",
       "      <td>0.053212</td>\n",
       "      <td>-0.254406</td>\n",
       "      <td>0.234227</td>\n",
       "      <td>0.394750</td>\n",
       "      <td>-0.835210</td>\n",
       "      <td>-0.129802</td>\n",
       "      <td>0.064915</td>\n",
       "      <td>0.179292</td>\n",
       "      <td>0.209413</td>\n",
       "      <td>0.479444</td>\n",
       "      <td>-0.104716</td>\n",
       "      <td>-0.608651</td>\n",
       "      <td>0.325464</td>\n",
       "      <td>-1.388903</td>\n",
       "      <td>-0.150668</td>\n",
       "      <td>0.359018</td>\n",
       "      <td>0.486160</td>\n",
       "      <td>-0.852314</td>\n",
       "      <td>-0.505124</td>\n",
       "      <td>-1.129628</td>\n",
       "      <td>0.223145</td>\n",
       "      <td>1.958470</td>\n",
       "      <td>-0.431894</td>\n",
       "      <td>0.549779</td>\n",
       "      <td>-0.026414</td>\n",
       "      <td>-0.565852</td>\n",
       "      <td>0.457456</td>\n",
       "      <td>1.008801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003997</th>\n",
       "      <td>1635004680</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>-0.001911</td>\n",
       "      <td>-0.002472</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.418945</td>\n",
       "      <td>0.093445</td>\n",
       "      <td>-0.378906</td>\n",
       "      <td>438.684570</td>\n",
       "      <td>2384.702148</td>\n",
       "      <td>-30.765625</td>\n",
       "      <td>3.492188</td>\n",
       "      <td>-0.430889</td>\n",
       "      <td>8.265625</td>\n",
       "      <td>-4.867188</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-558.652893</td>\n",
       "      <td>2578.300049</td>\n",
       "      <td>0.421387</td>\n",
       "      <td>0.909668</td>\n",
       "      <td>0.251709</td>\n",
       "      <td>0.766602</td>\n",
       "      <td>0.672852</td>\n",
       "      <td>0.365479</td>\n",
       "      <td>0.777344</td>\n",
       "      <td>0.517578</td>\n",
       "      <td>0.175415</td>\n",
       "      <td>0.144409</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>0.133179</td>\n",
       "      <td>0.400146</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>49.71875</td>\n",
       "      <td>49.75000</td>\n",
       "      <td>49.90625</td>\n",
       "      <td>49.875</td>\n",
       "      <td>49.34375</td>\n",
       "      <td>49.96875</td>\n",
       "      <td>49.750</td>\n",
       "      <td>49.28125</td>\n",
       "      <td>49.9375</td>\n",
       "      <td>49.90625</td>\n",
       "      <td>49.50000</td>\n",
       "      <td>49.68750</td>\n",
       "      <td>49.68750</td>\n",
       "      <td>49.12500</td>\n",
       "      <td>-0.768555</td>\n",
       "      <td>-0.045715</td>\n",
       "      <td>-0.864258</td>\n",
       "      <td>-1.257812</td>\n",
       "      <td>-0.672363</td>\n",
       "      <td>-0.149414</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>-0.094910</td>\n",
       "      <td>-0.690430</td>\n",
       "      <td>0.370850</td>\n",
       "      <td>0.331787</td>\n",
       "      <td>-0.870117</td>\n",
       "      <td>-1.325195</td>\n",
       "      <td>-1.379883</td>\n",
       "      <td>-1.001953</td>\n",
       "      <td>-1.005859</td>\n",
       "      <td>-1.126953</td>\n",
       "      <td>-0.259521</td>\n",
       "      <td>-0.059814</td>\n",
       "      <td>-0.265381</td>\n",
       "      <td>-0.478027</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>0.385010</td>\n",
       "      <td>-0.297363</td>\n",
       "      <td>-0.223633</td>\n",
       "      <td>-0.395996</td>\n",
       "      <td>-1.621094</td>\n",
       "      <td>-0.336914</td>\n",
       "      <td>-0.999512</td>\n",
       "      <td>-1.044922</td>\n",
       "      <td>-2.652344</td>\n",
       "      <td>-1.006836</td>\n",
       "      <td>-0.239380</td>\n",
       "      <td>-0.572266</td>\n",
       "      <td>-0.106079</td>\n",
       "      <td>-0.863281</td>\n",
       "      <td>1.086914</td>\n",
       "      <td>-0.251709</td>\n",
       "      <td>-0.672363</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>0.130371</td>\n",
       "      <td>-0.961914</td>\n",
       "      <td>-0.056030</td>\n",
       "      <td>-0.658203</td>\n",
       "      <td>-1.958008</td>\n",
       "      <td>-0.378418</td>\n",
       "      <td>0.254150</td>\n",
       "      <td>-1.540039</td>\n",
       "      <td>-1.265625</td>\n",
       "      <td>-1.740234</td>\n",
       "      <td>-0.909180</td>\n",
       "      <td>-1.622070</td>\n",
       "      <td>-1.111328</td>\n",
       "      <td>-1.636719</td>\n",
       "      <td>-1.927734</td>\n",
       "      <td>-1.935547</td>\n",
       "      <td>-1.432617</td>\n",
       "      <td>-1.477539</td>\n",
       "      <td>-1.295898</td>\n",
       "      <td>-0.570566</td>\n",
       "      <td>-0.970703</td>\n",
       "      <td>-0.537760</td>\n",
       "      <td>-1.787109</td>\n",
       "      <td>-0.196243</td>\n",
       "      <td>3.185547</td>\n",
       "      <td>-1.835282</td>\n",
       "      <td>-0.396973</td>\n",
       "      <td>-2.015630</td>\n",
       "      <td>-2.732422</td>\n",
       "      <td>-1.781444</td>\n",
       "      <td>0.253174</td>\n",
       "      <td>-1.112305</td>\n",
       "      <td>-0.047760</td>\n",
       "      <td>-0.916016</td>\n",
       "      <td>-1.468750</td>\n",
       "      <td>-0.645996</td>\n",
       "      <td>-0.906738</td>\n",
       "      <td>-0.219604</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>-0.469727</td>\n",
       "      <td>-1.429688</td>\n",
       "      <td>-0.197632</td>\n",
       "      <td>0.708984</td>\n",
       "      <td>-0.855006</td>\n",
       "      <td>-0.451587</td>\n",
       "      <td>0.086480</td>\n",
       "      <td>0.405887</td>\n",
       "      <td>0.863426</td>\n",
       "      <td>1.437916</td>\n",
       "      <td>-0.014896</td>\n",
       "      <td>-0.873608</td>\n",
       "      <td>0.595378</td>\n",
       "      <td>0.391770</td>\n",
       "      <td>0.557641</td>\n",
       "      <td>0.227989</td>\n",
       "      <td>-0.189712</td>\n",
       "      <td>-2.200254</td>\n",
       "      <td>-0.008235</td>\n",
       "      <td>1.538228</td>\n",
       "      <td>-0.106848</td>\n",
       "      <td>0.395545</td>\n",
       "      <td>-0.684624</td>\n",
       "      <td>-0.814332</td>\n",
       "      <td>-0.781282</td>\n",
       "      <td>-1.476118</td>\n",
       "      <td>0.284440</td>\n",
       "      <td>-0.519116</td>\n",
       "      <td>-0.980276</td>\n",
       "      <td>0.054615</td>\n",
       "      <td>-0.256902</td>\n",
       "      <td>0.403830</td>\n",
       "      <td>0.635410</td>\n",
       "      <td>1.027649</td>\n",
       "      <td>-0.895103</td>\n",
       "      <td>-1.262675</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.800211</td>\n",
       "      <td>0.572246</td>\n",
       "      <td>-0.484536</td>\n",
       "      <td>0.260722</td>\n",
       "      <td>0.629629</td>\n",
       "      <td>0.784707</td>\n",
       "      <td>0.499224</td>\n",
       "      <td>-0.358079</td>\n",
       "      <td>-0.111734</td>\n",
       "      <td>1.023339</td>\n",
       "      <td>0.643359</td>\n",
       "      <td>0.590264</td>\n",
       "      <td>0.236844</td>\n",
       "      <td>-0.694845</td>\n",
       "      <td>-0.845300</td>\n",
       "      <td>0.353329</td>\n",
       "      <td>-1.040138</td>\n",
       "      <td>-0.169819</td>\n",
       "      <td>0.357422</td>\n",
       "      <td>-0.524434</td>\n",
       "      <td>-1.120526</td>\n",
       "      <td>-1.469747</td>\n",
       "      <td>-0.020822</td>\n",
       "      <td>-0.205887</td>\n",
       "      <td>0.425495</td>\n",
       "      <td>-0.166922</td>\n",
       "      <td>-0.603025</td>\n",
       "      <td>-0.031961</td>\n",
       "      <td>-0.069386</td>\n",
       "      <td>-0.117421</td>\n",
       "      <td>0.973989</td>\n",
       "      <td>0.363749</td>\n",
       "      <td>-0.403301</td>\n",
       "      <td>-1.094992</td>\n",
       "      <td>0.600989</td>\n",
       "      <td>0.417007</td>\n",
       "      <td>0.266060</td>\n",
       "      <td>-0.967450</td>\n",
       "      <td>0.030636</td>\n",
       "      <td>-0.074098</td>\n",
       "      <td>0.679915</td>\n",
       "      <td>0.162279</td>\n",
       "      <td>0.224564</td>\n",
       "      <td>0.286025</td>\n",
       "      <td>-1.019002</td>\n",
       "      <td>-1.079647</td>\n",
       "      <td>-0.874982</td>\n",
       "      <td>-1.264278</td>\n",
       "      <td>0.406738</td>\n",
       "      <td>3.217508</td>\n",
       "      <td>-1.178419</td>\n",
       "      <td>0.285135</td>\n",
       "      <td>-0.043210</td>\n",
       "      <td>-0.874567</td>\n",
       "      <td>0.405366</td>\n",
       "      <td>0.740750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003998</th>\n",
       "      <td>1635004740</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>-0.000375</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.424805</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>-0.354004</td>\n",
       "      <td>279.279358</td>\n",
       "      <td>1466.282959</td>\n",
       "      <td>30.515625</td>\n",
       "      <td>2.115234</td>\n",
       "      <td>-2.290436</td>\n",
       "      <td>-7.800781</td>\n",
       "      <td>-3.585938</td>\n",
       "      <td>0.013664</td>\n",
       "      <td>-0.111572</td>\n",
       "      <td>-243.123032</td>\n",
       "      <td>3358.561035</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.436523</td>\n",
       "      <td>0.525391</td>\n",
       "      <td>0.642578</td>\n",
       "      <td>0.592773</td>\n",
       "      <td>2.052734</td>\n",
       "      <td>0.676270</td>\n",
       "      <td>0.391357</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>1.057617</td>\n",
       "      <td>1.634766</td>\n",
       "      <td>0.487305</td>\n",
       "      <td>1.432617</td>\n",
       "      <td>1.189453</td>\n",
       "      <td>49.90625</td>\n",
       "      <td>49.87500</td>\n",
       "      <td>49.71875</td>\n",
       "      <td>50.000</td>\n",
       "      <td>49.81250</td>\n",
       "      <td>50.37500</td>\n",
       "      <td>50.000</td>\n",
       "      <td>49.84375</td>\n",
       "      <td>49.4375</td>\n",
       "      <td>49.68750</td>\n",
       "      <td>49.68750</td>\n",
       "      <td>49.75000</td>\n",
       "      <td>49.65625</td>\n",
       "      <td>49.75000</td>\n",
       "      <td>-0.014267</td>\n",
       "      <td>0.120361</td>\n",
       "      <td>-0.106873</td>\n",
       "      <td>-0.332764</td>\n",
       "      <td>0.083435</td>\n",
       "      <td>0.227905</td>\n",
       "      <td>-0.684082</td>\n",
       "      <td>-1.828125</td>\n",
       "      <td>-0.829102</td>\n",
       "      <td>-0.710449</td>\n",
       "      <td>-0.364258</td>\n",
       "      <td>-0.433838</td>\n",
       "      <td>-0.086548</td>\n",
       "      <td>-1.487305</td>\n",
       "      <td>-0.643066</td>\n",
       "      <td>-1.210938</td>\n",
       "      <td>-0.272949</td>\n",
       "      <td>-0.818848</td>\n",
       "      <td>-0.396973</td>\n",
       "      <td>-1.520508</td>\n",
       "      <td>-0.442139</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>-0.176025</td>\n",
       "      <td>-1.358398</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.616699</td>\n",
       "      <td>-0.522461</td>\n",
       "      <td>-0.841309</td>\n",
       "      <td>-0.465576</td>\n",
       "      <td>-0.798828</td>\n",
       "      <td>0.688477</td>\n",
       "      <td>-0.069946</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>1.327148</td>\n",
       "      <td>1.157227</td>\n",
       "      <td>0.413818</td>\n",
       "      <td>-0.998535</td>\n",
       "      <td>-0.454590</td>\n",
       "      <td>-0.390869</td>\n",
       "      <td>0.058014</td>\n",
       "      <td>-0.058411</td>\n",
       "      <td>-1.515625</td>\n",
       "      <td>-1.228516</td>\n",
       "      <td>-1.456055</td>\n",
       "      <td>-0.937988</td>\n",
       "      <td>-1.065430</td>\n",
       "      <td>-0.660156</td>\n",
       "      <td>-0.486572</td>\n",
       "      <td>-0.655273</td>\n",
       "      <td>-1.236328</td>\n",
       "      <td>-0.868164</td>\n",
       "      <td>-2.005859</td>\n",
       "      <td>-0.758789</td>\n",
       "      <td>0.063660</td>\n",
       "      <td>0.355713</td>\n",
       "      <td>-0.202393</td>\n",
       "      <td>0.056366</td>\n",
       "      <td>-0.875977</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.602539</td>\n",
       "      <td>0.435617</td>\n",
       "      <td>1.065430</td>\n",
       "      <td>0.491331</td>\n",
       "      <td>-0.733398</td>\n",
       "      <td>0.825009</td>\n",
       "      <td>1.483398</td>\n",
       "      <td>-0.540659</td>\n",
       "      <td>-0.283691</td>\n",
       "      <td>-0.718696</td>\n",
       "      <td>-1.478516</td>\n",
       "      <td>-0.489825</td>\n",
       "      <td>-0.228882</td>\n",
       "      <td>0.157837</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.359619</td>\n",
       "      <td>-0.735840</td>\n",
       "      <td>0.623047</td>\n",
       "      <td>0.711426</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>-0.021805</td>\n",
       "      <td>0.173096</td>\n",
       "      <td>-0.398926</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>1.675781</td>\n",
       "      <td>-0.211643</td>\n",
       "      <td>-0.568598</td>\n",
       "      <td>0.197373</td>\n",
       "      <td>0.688944</td>\n",
       "      <td>-0.472552</td>\n",
       "      <td>-1.259527</td>\n",
       "      <td>0.125105</td>\n",
       "      <td>-0.918707</td>\n",
       "      <td>-0.185340</td>\n",
       "      <td>-0.951856</td>\n",
       "      <td>-0.205802</td>\n",
       "      <td>-0.048223</td>\n",
       "      <td>0.899905</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>-0.787082</td>\n",
       "      <td>0.114129</td>\n",
       "      <td>-1.016521</td>\n",
       "      <td>-0.887917</td>\n",
       "      <td>-0.443570</td>\n",
       "      <td>-0.668010</td>\n",
       "      <td>0.567338</td>\n",
       "      <td>0.366212</td>\n",
       "      <td>0.647260</td>\n",
       "      <td>1.634027</td>\n",
       "      <td>-0.329016</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.369494</td>\n",
       "      <td>0.600579</td>\n",
       "      <td>0.633410</td>\n",
       "      <td>0.546798</td>\n",
       "      <td>-0.261447</td>\n",
       "      <td>-0.641927</td>\n",
       "      <td>0.154598</td>\n",
       "      <td>0.309042</td>\n",
       "      <td>-0.567562</td>\n",
       "      <td>-0.068522</td>\n",
       "      <td>-0.381736</td>\n",
       "      <td>-0.569010</td>\n",
       "      <td>-0.180692</td>\n",
       "      <td>0.660199</td>\n",
       "      <td>-0.261229</td>\n",
       "      <td>-0.199426</td>\n",
       "      <td>0.980150</td>\n",
       "      <td>1.969330</td>\n",
       "      <td>-0.129434</td>\n",
       "      <td>0.699930</td>\n",
       "      <td>-0.676576</td>\n",
       "      <td>-0.423963</td>\n",
       "      <td>-0.606505</td>\n",
       "      <td>-1.364408</td>\n",
       "      <td>0.317798</td>\n",
       "      <td>-0.234285</td>\n",
       "      <td>0.752930</td>\n",
       "      <td>-0.091471</td>\n",
       "      <td>-0.457249</td>\n",
       "      <td>-0.836588</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>-0.093853</td>\n",
       "      <td>0.434584</td>\n",
       "      <td>0.243026</td>\n",
       "      <td>0.028022</td>\n",
       "      <td>-0.033038</td>\n",
       "      <td>0.055441</td>\n",
       "      <td>0.260966</td>\n",
       "      <td>-0.392261</td>\n",
       "      <td>-0.400800</td>\n",
       "      <td>-0.300944</td>\n",
       "      <td>-0.785810</td>\n",
       "      <td>-0.203995</td>\n",
       "      <td>-1.325451</td>\n",
       "      <td>-0.493536</td>\n",
       "      <td>-0.765929</td>\n",
       "      <td>1.128969</td>\n",
       "      <td>0.446938</td>\n",
       "      <td>-0.086424</td>\n",
       "      <td>-1.482910</td>\n",
       "      <td>-0.688368</td>\n",
       "      <td>-0.453511</td>\n",
       "      <td>-0.786596</td>\n",
       "      <td>0.096694</td>\n",
       "      <td>0.479796</td>\n",
       "      <td>0.635456</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>1.516436</td>\n",
       "      <td>-0.517847</td>\n",
       "      <td>-0.195844</td>\n",
       "      <td>0.595061</td>\n",
       "      <td>0.744506</td>\n",
       "      <td>0.413717</td>\n",
       "      <td>1.709260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2003099 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  Target_0  Target_1  Target_2  Target_3  Target_4  \\\n",
       "900      1514818860  0.003670 -0.000046 -0.009682       NaN       NaN   \n",
       "901      1514818920  0.000658 -0.001395  0.005344       NaN       NaN   \n",
       "902      1514818980  0.002512  0.000497  0.001775       NaN       NaN   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "2003996  1635004620  0.004490 -0.002438  0.000604 -0.001764 -0.003277   \n",
       "2003997  1635004680  0.004589 -0.002665  0.000402 -0.001911 -0.002472   \n",
       "2003998  1635004740  0.003696 -0.001935  0.000716 -0.000983 -0.001260   \n",
       "\n",
       "         Target_5  Target_6  Target_7  Target_8  Target_9  Target_10  \\\n",
       "900      0.026840  0.005878  0.027252       NaN -0.004345        NaN   \n",
       "901      0.032043  0.005554  0.019608       NaN  0.000405        NaN   \n",
       "902      0.033173  0.002216  0.026505       NaN -0.002016        NaN   \n",
       "...           ...       ...       ...       ...       ...        ...   \n",
       "2003996  0.002123  0.003429  0.002497  0.003248 -0.000154   0.002647   \n",
       "2003997 -0.000771  0.002779  0.003443  0.002815 -0.000671   0.002897   \n",
       "2003998 -0.000049  0.000637  0.002508  0.001601 -0.000375   0.002010   \n",
       "\n",
       "         Target_11  Target_12  Target_13  MACD_id0  MACD_id1  MACD_id2  \\\n",
       "900      -0.000653        NaN        NaN       NaN       NaN       NaN   \n",
       "901       0.011620        NaN        NaN  0.488525  0.099976  0.127075   \n",
       "902       0.001867        NaN        NaN -1.286133  0.014160  0.068909   \n",
       "...            ...        ...        ...       ...       ...       ...   \n",
       "2003996   0.003078  -0.000059   0.002581  1.094727  0.097961 -0.300293   \n",
       "2003997   0.003733  -0.000678   0.002525  0.418945  0.093445 -0.378906   \n",
       "2003998   0.002522  -0.000515   0.002510  0.424805  0.029968 -0.354004   \n",
       "\n",
       "           MACD_id3     MACD_id4   MACD_id5  MACD_id6  MACD_id7   MACD_id8  \\\n",
       "900             NaN          NaN        NaN       NaN       NaN        NaN   \n",
       "901             NaN          NaN  11.289062  0.421143  0.331189        NaN   \n",
       "902             NaN          NaN  20.031250  0.100159 -1.023582        NaN   \n",
       "...             ...          ...        ...       ...       ...        ...   \n",
       "2003996  591.418579  3287.896240  -1.572266  5.027344  1.399485  37.312500   \n",
       "2003997  438.684570  2384.702148 -30.765625  3.492188 -0.430889   8.265625   \n",
       "2003998  279.279358  1466.282959  30.515625  2.115234 -2.290436  -7.800781   \n",
       "\n",
       "         MACD_id9  MACD_id10  MACD_id11   MACD_id12    MACD_id13  \\\n",
       "900           NaN        NaN        NaN         NaN          NaN   \n",
       "901      1.437500        NaN   0.048615         NaN          NaN   \n",
       "902      1.041992        NaN   0.162842         NaN          NaN   \n",
       "...           ...        ...        ...         ...          ...   \n",
       "2003996 -3.355469   0.017715   0.059967 -346.152832  5289.912109   \n",
       "2003997 -4.867188   0.011330  -0.071472 -558.652893  2578.300049   \n",
       "2003998 -3.585938   0.013664  -0.111572 -243.123032  3358.561035   \n",
       "\n",
       "         Volume_Ratio_id0  Volume_Ratio_id1  Volume_Ratio_id2  \\\n",
       "900                   NaN               NaN               NaN   \n",
       "901                   NaN               NaN               NaN   \n",
       "902                   NaN               NaN               NaN   \n",
       "...                   ...               ...               ...   \n",
       "2003996          1.033203          0.941406          0.355469   \n",
       "2003997          0.421387          0.909668          0.251709   \n",
       "2003998          0.898438          0.436523          0.525391   \n",
       "\n",
       "         Volume_Ratio_id3  Volume_Ratio_id4  Volume_Ratio_id5  \\\n",
       "900                   NaN               NaN               NaN   \n",
       "901                   NaN               NaN               NaN   \n",
       "902                   NaN               NaN               NaN   \n",
       "...                   ...               ...               ...   \n",
       "2003996          0.974609          1.374023          0.694336   \n",
       "2003997          0.766602          0.672852          0.365479   \n",
       "2003998          0.642578          0.592773          2.052734   \n",
       "\n",
       "         Volume_Ratio_id6  Volume_Ratio_id7  Volume_Ratio_id8  \\\n",
       "900                   NaN               NaN               NaN   \n",
       "901                   NaN               NaN               NaN   \n",
       "902                   NaN               NaN               NaN   \n",
       "...                   ...               ...               ...   \n",
       "2003996          1.184570          0.766113          1.719727   \n",
       "2003997          0.777344          0.517578          0.175415   \n",
       "2003998          0.676270          0.391357          0.419922   \n",
       "\n",
       "         Volume_Ratio_id9  Volume_Ratio_id10  Volume_Ratio_id11  \\\n",
       "900                   NaN                NaN                NaN   \n",
       "901                   NaN                NaN                NaN   \n",
       "902                   NaN                NaN                NaN   \n",
       "...                   ...                ...                ...   \n",
       "2003996          0.454590           1.046875           0.607910   \n",
       "2003997          0.144409           0.583984           0.133179   \n",
       "2003998          1.057617           1.634766           0.487305   \n",
       "\n",
       "         Volume_Ratio_id12  Volume_Ratio_id13   RSI_id0   RSI_id1   RSI_id2  \\\n",
       "900                    NaN                NaN       NaN       NaN       NaN   \n",
       "901                    NaN                NaN       NaN       NaN       NaN   \n",
       "902                    NaN                NaN       NaN       NaN       NaN   \n",
       "...                    ...                ...       ...       ...       ...   \n",
       "2003996           0.882812           1.423828  50.03125  49.40625  49.90625   \n",
       "2003997           0.400146           0.625488  49.71875  49.75000  49.90625   \n",
       "2003998           1.432617           1.189453  49.90625  49.87500  49.71875   \n",
       "\n",
       "         RSI_id3   RSI_id4   RSI_id5  RSI_id6   RSI_id7  RSI_id8   RSI_id9  \\\n",
       "900          NaN       NaN       NaN      NaN       NaN      NaN       NaN   \n",
       "901          NaN       NaN       NaN      NaN       NaN      NaN       NaN   \n",
       "902          NaN       NaN       NaN      NaN       NaN      NaN       NaN   \n",
       "...          ...       ...       ...      ...       ...      ...       ...   \n",
       "2003996   49.875  50.03125  50.06250   50.125  50.09375  49.8750  50.03125   \n",
       "2003997   49.875  49.34375  49.96875   49.750  49.28125  49.9375  49.90625   \n",
       "2003998   50.000  49.81250  50.37500   50.000  49.84375  49.4375  49.68750   \n",
       "\n",
       "         RSI_id10  RSI_id11  RSI_id12  RSI_id13  log_close/mean_60_id0  \\\n",
       "900           NaN       NaN       NaN       NaN                    NaN   \n",
       "901           NaN       NaN       NaN       NaN               0.857422   \n",
       "902           NaN       NaN       NaN       NaN              -2.324219   \n",
       "...           ...       ...       ...       ...                    ...   \n",
       "2003996  49.96875  49.84375  49.87500  49.71875               0.131592   \n",
       "2003997  49.50000  49.68750  49.68750  49.12500              -0.768555   \n",
       "2003998  49.68750  49.75000  49.65625  49.75000              -0.014267   \n",
       "\n",
       "         log_return_60_id0  log_close/mean_300_id0  log_return_300_id0  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               0.415527                1.074219            3.695312   \n",
       "902              -3.894531               -2.181641           -0.871094   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           0.309326                0.032837            0.130005   \n",
       "2003997          -0.045715               -0.864258           -1.257812   \n",
       "2003998           0.120361               -0.106873           -0.332764   \n",
       "\n",
       "         log_close/mean_900_id0  log_return_900_id0  log_close/mean_60_id1  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                    1.310547            3.330078               0.169434   \n",
       "902                   -1.942383           -3.001953              -0.929199   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.228027           -0.152344               0.056641   \n",
       "2003997               -0.672363           -0.149414               0.008423   \n",
       "2003998                0.083435            0.227905              -0.684082   \n",
       "\n",
       "         log_return_60_id1  log_close/mean_300_id1  log_return_300_id1  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               0.998047                0.595703            1.240234   \n",
       "902              -0.570312               -0.508301            0.215332   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996          -0.519531               -0.060272           -1.182617   \n",
       "2003997           0.986328               -0.094910           -0.690430   \n",
       "2003998          -1.828125               -0.829102           -0.710449   \n",
       "\n",
       "         log_close/mean_900_id1  log_return_900_id1  log_close/mean_60_id2  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                    0.770508            1.057617              -0.846680   \n",
       "902                   -0.332520           -0.619629              -1.195312   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.408936            0.496826              -0.542480   \n",
       "2003997                0.370850            0.331787              -0.870117   \n",
       "2003998               -0.364258           -0.433838              -0.086548   \n",
       "\n",
       "         log_return_60_id2  log_close/mean_300_id2  log_return_300_id2  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               0.495605               -0.427979            1.048828   \n",
       "902               0.536621               -0.774902            0.797852   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996          -0.487305               -1.034180           -0.738770   \n",
       "2003997          -1.325195               -1.379883           -1.001953   \n",
       "2003998          -1.487305               -0.643066           -1.210938   \n",
       "\n",
       "         log_close/mean_900_id2  log_return_900_id2  log_close/mean_60_id3  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                   -0.279785           -0.594727                    NaN   \n",
       "902                   -0.623535           -1.514648                    NaN   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996               -0.660156           -0.027725              -0.019012   \n",
       "2003997               -1.005859           -1.126953              -0.259521   \n",
       "2003998               -0.272949           -0.818848              -0.396973   \n",
       "\n",
       "         log_return_60_id3  log_close/mean_300_id3  log_return_300_id3  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901                    NaN                     NaN                 NaN   \n",
       "902                    NaN                     NaN                 NaN   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           0.247559               -0.025635           -0.417969   \n",
       "2003997          -0.059814               -0.265381           -0.478027   \n",
       "2003998          -1.520508               -0.442139            0.018265   \n",
       "\n",
       "         log_close/mean_900_id3  log_return_900_id3  log_close/mean_60_id4  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                         NaN                 NaN                    NaN   \n",
       "902                         NaN                 NaN                    NaN   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.239746            0.281250               0.422119   \n",
       "2003997               -0.002037            0.385010              -0.297363   \n",
       "2003998               -0.176025           -1.358398              -0.417480   \n",
       "\n",
       "         log_return_60_id4  log_close/mean_300_id4  log_return_300_id4  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901                    NaN                     NaN                 NaN   \n",
       "902                    NaN                     NaN                 NaN   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           1.418945                0.317627            0.153320   \n",
       "2003997          -0.223633               -0.395996           -1.621094   \n",
       "2003998          -0.616699               -0.522461           -0.841309   \n",
       "\n",
       "         log_close/mean_900_id4  log_return_900_id4  log_close/mean_60_id5  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                         NaN                 NaN              -0.150513   \n",
       "902                         NaN                 NaN               0.711914   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.384277            0.581543              -0.476562   \n",
       "2003997               -0.336914           -0.999512              -1.044922   \n",
       "2003998               -0.465576           -0.798828               0.688477   \n",
       "\n",
       "         log_return_60_id5  log_close/mean_300_id5  log_return_300_id5  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               3.751953                0.409912            0.690918   \n",
       "902               2.287109                1.293945            1.583984   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996          -0.318359               -0.364502            0.305176   \n",
       "2003997          -2.652344               -1.006836           -0.239380   \n",
       "2003998          -0.069946                0.718750            1.327148   \n",
       "\n",
       "         log_close/mean_900_id5  log_return_900_id5  log_close/mean_60_id6  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                    0.621094           -0.249268              -0.369141   \n",
       "902                    1.513672            0.104248              -1.117188   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.070068           -0.506348              -0.435303   \n",
       "2003997               -0.572266           -0.106079              -0.863281   \n",
       "2003998                1.157227            0.413818              -0.998535   \n",
       "\n",
       "         log_return_60_id6  log_close/mean_300_id6  log_return_300_id6  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               0.486084               -0.142944           -0.592773   \n",
       "902              -0.688965               -0.896973            0.710938   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           1.681641                0.169067            0.397705   \n",
       "2003997           1.086914               -0.251709           -0.672363   \n",
       "2003998          -0.454590               -0.390869            0.058014   \n",
       "\n",
       "         log_close/mean_900_id6  log_return_900_id6  log_close/mean_60_id7  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                   -0.133667           -0.331055              -0.740234   \n",
       "902                   -0.886230           -0.793945              -1.547852   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.500488           -1.287109              -0.559570   \n",
       "2003997                0.076904            0.130371              -0.961914   \n",
       "2003998               -0.058411           -1.515625              -1.228516   \n",
       "\n",
       "         log_return_60_id7  log_close/mean_300_id7  log_return_300_id7  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901               1.807617               -0.625000            1.960938   \n",
       "902              -1.286133               -1.441406           -0.969238   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           0.260498               -0.266357            0.839355   \n",
       "2003997          -0.056030               -0.658203           -1.958008   \n",
       "2003998          -1.456055               -0.937988           -1.065430   \n",
       "\n",
       "         log_close/mean_900_id7  log_return_900_id7  log_close/mean_60_id8  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                   -0.985352            5.675781                    NaN   \n",
       "902                   -1.802734            0.909668                    NaN   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.024139            0.461182               0.734863   \n",
       "2003997               -0.378418            0.254150              -1.540039   \n",
       "2003998               -0.660156           -0.486572              -0.655273   \n",
       "\n",
       "         log_return_60_id8  log_close/mean_300_id8  log_return_300_id8  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901                    NaN                     NaN                 NaN   \n",
       "902                    NaN                     NaN                 NaN   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996           1.751953                0.541992           -0.331055   \n",
       "2003997          -1.265625               -1.740234           -0.909180   \n",
       "2003998          -1.236328               -0.868164           -2.005859   \n",
       "\n",
       "         log_close/mean_900_id8  log_return_900_id8  log_close/mean_60_id9  \\\n",
       "900                         NaN                 NaN                    NaN   \n",
       "901                         NaN                 NaN              -0.486572   \n",
       "902                         NaN                 NaN              -0.562988   \n",
       "...                         ...                 ...                    ...   \n",
       "2003996                0.661133           -0.750000              -0.506836   \n",
       "2003997               -1.622070           -1.111328              -1.636719   \n",
       "2003998               -0.758789            0.063660               0.355713   \n",
       "\n",
       "         log_return_60_id9  log_close/mean_300_id9  log_return_300_id9  \\\n",
       "900                    NaN                     NaN                 NaN   \n",
       "901              -0.113342               -0.091858            1.287109   \n",
       "902              -0.014412               -0.170898            1.787109   \n",
       "...                    ...                     ...                 ...   \n",
       "2003996          -0.243164               -0.788574            0.574707   \n",
       "2003997          -1.927734               -1.935547           -1.432617   \n",
       "2003998          -0.202393                0.056366           -0.875977   \n",
       "\n",
       "         log_close/mean_900_id9  log_return_900_id9  log_close/mean_60_id10  \\\n",
       "900                         NaN                 NaN                     NaN   \n",
       "901                   -0.411377           -2.744141                     NaN   \n",
       "902                   -0.487793           -0.414062                     NaN   \n",
       "...                         ...                 ...                     ...   \n",
       "2003996               -0.330322           -1.027344                0.007469   \n",
       "2003997               -1.477539           -1.295898               -0.570566   \n",
       "2003998                0.507812            0.602539                0.435617   \n",
       "\n",
       "         log_return_60_id10  log_close/mean_300_id10  log_return_300_id10  \\\n",
       "900                     NaN                      NaN                  NaN   \n",
       "901                     NaN                      NaN                  NaN   \n",
       "902                     NaN                      NaN                  NaN   \n",
       "...                     ...                      ...                  ...   \n",
       "2003996            1.059570                 0.045966            -0.152100   \n",
       "2003997           -0.970703                -0.537760            -1.787109   \n",
       "2003998            1.065430                 0.491331            -0.733398   \n",
       "\n",
       "         log_close/mean_900_id10  log_return_900_id10  log_close/mean_60_id11  \\\n",
       "900                          NaN                  NaN                     NaN   \n",
       "901                          NaN                  NaN               -1.315619   \n",
       "902                          NaN                  NaN                0.775507   \n",
       "...                          ...                  ...                     ...   \n",
       "2003996                 0.397991             2.060547               -0.312255   \n",
       "2003997                -0.196243             3.185547               -1.835282   \n",
       "2003998                 0.825009             1.483398               -0.540659   \n",
       "\n",
       "         log_return_60_id11  log_close/mean_300_id11  log_return_300_id11  \\\n",
       "900                     NaN                      NaN                  NaN   \n",
       "901                2.146484                -0.837071            -0.014755   \n",
       "902                3.205078                 1.277415             5.554688   \n",
       "...                     ...                      ...                  ...   \n",
       "2003996            0.031708                -0.497678            -0.950684   \n",
       "2003997           -0.396973                -2.015630            -2.732422   \n",
       "2003998           -0.283691                -0.718696            -1.478516   \n",
       "\n",
       "         log_close/mean_900_id11  log_return_900_id11  log_close/mean_60_id12  \\\n",
       "900                          NaN                  NaN                     NaN   \n",
       "901                    -0.754139             3.257812                     NaN   \n",
       "902                     1.368025             4.160156                     NaN   \n",
       "...                          ...                  ...                     ...   \n",
       "2003996                -0.257082             0.651855               -0.316650   \n",
       "2003997                -1.781444             0.253174               -1.112305   \n",
       "2003998                -0.489825            -0.228882                0.157837   \n",
       "\n",
       "         log_return_60_id12  log_close/mean_300_id12  log_return_300_id12  \\\n",
       "900                     NaN                      NaN                  NaN   \n",
       "901                     NaN                      NaN                  NaN   \n",
       "902                     NaN                      NaN                  NaN   \n",
       "...                     ...                      ...                  ...   \n",
       "2003996            0.684570                -0.124878            -0.517578   \n",
       "2003997           -0.047760                -0.916016            -1.468750   \n",
       "2003998            0.031982                 0.359619            -0.735840   \n",
       "\n",
       "         log_close/mean_900_id12  log_return_900_id12  log_close/mean_60_id13  \\\n",
       "900                          NaN                  NaN                     NaN   \n",
       "901                          NaN                  NaN                     NaN   \n",
       "902                          NaN                  NaN                     NaN   \n",
       "...                          ...                  ...                     ...   \n",
       "2003996                 0.148438            -0.463867                0.615723   \n",
       "2003997                -0.645996            -0.906738               -0.219604   \n",
       "2003998                 0.623047             0.711426                0.421875   \n",
       "\n",
       "         log_return_60_id13  log_close/mean_300_id13  log_return_300_id13  \\\n",
       "900                     NaN                      NaN                  NaN   \n",
       "901                     NaN                      NaN                  NaN   \n",
       "902                     NaN                      NaN                  NaN   \n",
       "...                     ...                      ...                  ...   \n",
       "2003996            1.342773                 0.353271            -0.359131   \n",
       "2003997            0.576172                -0.469727            -1.429688   \n",
       "2003998           -0.021805                 0.173096            -0.398926   \n",
       "\n",
       "         log_close/mean_900_id13  log_return_900_id13  mean_close/mean_60  \\\n",
       "900                          NaN                  NaN                 NaN   \n",
       "901                          NaN                  NaN           -0.360293   \n",
       "902                          NaN                  NaN           -0.773667   \n",
       "...                          ...                  ...                 ...   \n",
       "2003996                 0.632324             1.111328           -0.085742   \n",
       "2003997                -0.197632             0.708984           -0.855006   \n",
       "2003998                 0.441650             1.675781           -0.211643   \n",
       "\n",
       "         mean_log_returns_60  log_close/mean_60-mean_close/mean_60_id0  \\\n",
       "900                      NaN                                       NaN   \n",
       "901                 1.248572                                  1.217481   \n",
       "902                -0.053340                                 -1.550377   \n",
       "...                      ...                                       ...   \n",
       "2003996             0.515693                                  0.217382   \n",
       "2003997            -0.451587                                  0.086480   \n",
       "2003998            -0.568598                                  0.197373   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id0  \\\n",
       "900                                        NaN   \n",
       "901                                  -0.833078   \n",
       "902                                  -3.841115   \n",
       "...                                        ...   \n",
       "2003996                              -0.206380   \n",
       "2003997                               0.405887   \n",
       "2003998                               0.688944   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id1  \\\n",
       "900                                           NaN   \n",
       "901                                      0.529750   \n",
       "902                                     -0.155529   \n",
       "...                                           ...   \n",
       "2003996                                  0.142377   \n",
       "2003997                                  0.863426   \n",
       "2003998                                 -0.472552   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id1  \\\n",
       "900                                        NaN   \n",
       "901                                  -0.250525   \n",
       "902                                  -0.516972   \n",
       "...                                        ...   \n",
       "2003996                              -1.035224   \n",
       "2003997                               1.437916   \n",
       "2003998                              -1.259527   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id2  \\\n",
       "900                                           NaN   \n",
       "901                                     -0.486544   \n",
       "902                                     -0.421319   \n",
       "...                                           ...   \n",
       "2003996                                 -0.456515   \n",
       "2003997                                 -0.014896   \n",
       "2003998                                  0.125105   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id2  \\\n",
       "900                                        NaN   \n",
       "901                                  -0.752966   \n",
       "902                                   0.589962   \n",
       "...                                        ...   \n",
       "2003996                              -1.002998   \n",
       "2003997                              -0.873608   \n",
       "2003998                              -0.918707   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id3  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                  0.066731   \n",
       "2003997                                  0.595378   \n",
       "2003998                                 -0.185340   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id3  \\\n",
       "900                                        NaN   \n",
       "901                                        NaN   \n",
       "902                                        NaN   \n",
       "...                                        ...   \n",
       "2003996                              -0.268155   \n",
       "2003997                               0.391770   \n",
       "2003998                              -0.951856   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id4  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                  0.507831   \n",
       "2003997                                  0.557641   \n",
       "2003998                                 -0.205802   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id4  \\\n",
       "900                                        NaN   \n",
       "901                                        NaN   \n",
       "902                                        NaN   \n",
       "...                                        ...   \n",
       "2003996                               0.903592   \n",
       "2003997                               0.227989   \n",
       "2003998                              -0.048223   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id5  \\\n",
       "900                                           NaN   \n",
       "901                                      0.209732   \n",
       "902                                      1.485487   \n",
       "...                                           ...   \n",
       "2003996                                 -0.390857   \n",
       "2003997                                 -0.189712   \n",
       "2003998                                  0.899905   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id5  \\\n",
       "900                                        NaN   \n",
       "901                                   2.503654   \n",
       "902                                   2.339701   \n",
       "...                                        ...   \n",
       "2003996                              -0.834015   \n",
       "2003997                              -2.200254   \n",
       "2003998                               0.498644   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id6  \\\n",
       "900                                           NaN   \n",
       "901                                     -0.008902   \n",
       "902                                     -0.343298   \n",
       "...                                           ...   \n",
       "2003996                                 -0.349589   \n",
       "2003997                                 -0.008235   \n",
       "2003998                                 -0.787082   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id6  \\\n",
       "900                                        NaN   \n",
       "901                                  -0.762539   \n",
       "902                                  -0.635759   \n",
       "...                                        ...   \n",
       "2003996                               1.165816   \n",
       "2003997                               1.538228   \n",
       "2003998                               0.114129   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id7  \\\n",
       "900                                           NaN   \n",
       "901                                     -0.379919   \n",
       "902                                     -0.774664   \n",
       "...                                           ...   \n",
       "2003996                                 -0.474044   \n",
       "2003997                                 -0.106848   \n",
       "2003998                                 -1.016521   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id7  \\\n",
       "900                                        NaN   \n",
       "901                                   0.559472   \n",
       "902                                  -1.233167   \n",
       "...                                        ...   \n",
       "2003996                              -0.255259   \n",
       "2003997                               0.395545   \n",
       "2003998                              -0.887917   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id8  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                  0.820498   \n",
       "2003997                                 -0.684624   \n",
       "2003998                                 -0.443570   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id8  \\\n",
       "900                                        NaN   \n",
       "901                                        NaN   \n",
       "902                                        NaN   \n",
       "...                                        ...   \n",
       "2003996                               1.235934   \n",
       "2003997                              -0.814332   \n",
       "2003998                              -0.668010   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id9  \\\n",
       "900                                           NaN   \n",
       "901                                     -0.126272   \n",
       "902                                      0.210526   \n",
       "...                                           ...   \n",
       "2003996                                 -0.421058   \n",
       "2003997                                 -0.781282   \n",
       "2003998                                  0.567338   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id9  \\\n",
       "900                                        NaN   \n",
       "901                                  -1.361931   \n",
       "902                                   0.038932   \n",
       "...                                        ...   \n",
       "2003996                              -0.758873   \n",
       "2003997                              -1.476118   \n",
       "2003998                               0.366212   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id10  \\\n",
       "900                                            NaN   \n",
       "901                                            NaN   \n",
       "902                                            NaN   \n",
       "...                                            ...   \n",
       "2003996                                   0.093211   \n",
       "2003997                                   0.284440   \n",
       "2003998                                   0.647260   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id10  \\\n",
       "900                                         NaN   \n",
       "901                                         NaN   \n",
       "902                                         NaN   \n",
       "...                                         ...   \n",
       "2003996                                0.543877   \n",
       "2003997                               -0.519116   \n",
       "2003998                                1.634027   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id11  \\\n",
       "900                                            NaN   \n",
       "901                                      -0.955327   \n",
       "902                                       1.549174   \n",
       "...                                            ...   \n",
       "2003996                                  -0.226513   \n",
       "2003997                                  -0.980276   \n",
       "2003998                                  -0.329016   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id11  \\\n",
       "900                                         NaN   \n",
       "901                                    0.897913   \n",
       "902                                    3.258419   \n",
       "...                                         ...   \n",
       "2003996                               -0.483985   \n",
       "2003997                                0.054615   \n",
       "2003998                                0.284906   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id12  \\\n",
       "900                                            NaN   \n",
       "901                                            NaN   \n",
       "902                                            NaN   \n",
       "...                                            ...   \n",
       "2003996                                  -0.230942   \n",
       "2003997                                  -0.256902   \n",
       "2003998                                   0.369494   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id12  \\\n",
       "900                                         NaN   \n",
       "901                                         NaN   \n",
       "902                                         NaN   \n",
       "...                                         ...   \n",
       "2003996                                0.168904   \n",
       "2003997                                0.403830   \n",
       "2003998                                0.600579   \n",
       "\n",
       "         log_close/mean_60-mean_close/mean_60_id13  \\\n",
       "900                                            NaN   \n",
       "901                                            NaN   \n",
       "902                                            NaN   \n",
       "...                                            ...   \n",
       "2003996                                   0.701487   \n",
       "2003997                                   0.635410   \n",
       "2003998                                   0.633410   \n",
       "\n",
       "         log_return_60-mean_log_returns_60_id13  mean_close/mean_300  \\\n",
       "900                                         NaN                  NaN   \n",
       "901                                         NaN            -0.005624   \n",
       "902                                         NaN            -0.425388   \n",
       "...                                         ...                  ...   \n",
       "2003996                                0.826765            -0.121514   \n",
       "2003997                                1.027649            -0.895103   \n",
       "2003998                                0.546798            -0.261447   \n",
       "\n",
       "         mean_log_returns_300  log_close/mean_300-mean_close/mean_300_id0  \\\n",
       "900                       NaN                                         NaN   \n",
       "901                  1.164455                                    1.080069   \n",
       "902                  1.101220                                   -1.756459   \n",
       "...                       ...                                         ...   \n",
       "2003996             -0.160677                                    0.154359   \n",
       "2003997             -1.262675                                    0.030951   \n",
       "2003998             -0.641927                                    0.154598   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id0  \\\n",
       "900                                          NaN   \n",
       "901                                     2.530615   \n",
       "902                                    -1.972388   \n",
       "...                                          ...   \n",
       "2003996                                 0.290700   \n",
       "2003997                                 0.005126   \n",
       "2003998                                 0.309042   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id1  \\\n",
       "900                                             NaN   \n",
       "901                                        0.601179   \n",
       "902                                       -0.083140   \n",
       "...                                             ...   \n",
       "2003996                                    0.061234   \n",
       "2003997                                    0.800211   \n",
       "2003998                                   -0.567562   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id1  \\\n",
       "900                                          NaN   \n",
       "901                                     0.075779   \n",
       "902                                    -0.885888   \n",
       "...                                          ...   \n",
       "2003996                                -1.021940   \n",
       "2003997                                 0.572246   \n",
       "2003998                                -0.068522   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id2  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.422472   \n",
       "902                                       -0.349605   \n",
       "...                                             ...   \n",
       "2003996                                   -0.912795   \n",
       "2003997                                   -0.484536   \n",
       "2003998                                   -0.381736   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id2  \\\n",
       "900                                          NaN   \n",
       "901                                    -0.115627   \n",
       "902                                    -0.303369   \n",
       "...                                          ...   \n",
       "2003996                                -0.578093   \n",
       "2003997                                 0.260722   \n",
       "2003998                                -0.569010   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id3  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.095884   \n",
       "2003997                                    0.629629   \n",
       "2003998                                   -0.180692   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id3  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                -0.257392   \n",
       "2003997                                 0.784707   \n",
       "2003998                                 0.660199   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id4  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.439124   \n",
       "2003997                                    0.499224   \n",
       "2003998                                   -0.261229   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id4  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                 0.314017   \n",
       "2003997                                -0.358079   \n",
       "2003998                                -0.199426   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id5  \\\n",
       "900                                             NaN   \n",
       "901                                        0.415640   \n",
       "902                                        1.719062   \n",
       "...                                             ...   \n",
       "2003996                                   -0.243098   \n",
       "2003997                                   -0.111734   \n",
       "2003998                                    0.980150   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id5  \\\n",
       "900                                          NaN   \n",
       "901                                    -0.473736   \n",
       "902                                     0.482864   \n",
       "...                                          ...   \n",
       "2003996                                 0.465861   \n",
       "2003997                                 1.023339   \n",
       "2003998                                 1.969330   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id6  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.137376   \n",
       "902                                       -0.471470   \n",
       "...                                             ...   \n",
       "2003996                                    0.290528   \n",
       "2003997                                    0.643359   \n",
       "2003998                                   -0.129434   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id6  \\\n",
       "900                                          NaN   \n",
       "901                                    -1.757041   \n",
       "902                                    -0.390406   \n",
       "...                                          ...   \n",
       "2003996                                 0.558469   \n",
       "2003997                                 0.590264   \n",
       "2003998                                 0.699930   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id7  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.619332   \n",
       "902                                       -1.015638   \n",
       "...                                             ...   \n",
       "2003996                                   -0.144752   \n",
       "2003997                                    0.236844   \n",
       "2003998                                   -0.676576   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id7  \\\n",
       "900                                          NaN   \n",
       "901                                     0.796323   \n",
       "902                                    -2.070456   \n",
       "...                                          ...   \n",
       "2003996                                 1.000013   \n",
       "2003997                                -0.694845   \n",
       "2003998                                -0.423963   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id8  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.663690   \n",
       "2003997                                   -0.845300   \n",
       "2003998                                   -0.606505   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id8  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                -0.170391   \n",
       "2003997                                 0.353329   \n",
       "2003998                                -1.364408   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id9  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.086261   \n",
       "902                                        0.254448   \n",
       "...                                             ...   \n",
       "2003996                                   -0.666893   \n",
       "2003997                                   -1.040138   \n",
       "2003998                                    0.317798   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id9  \\\n",
       "900                                          NaN   \n",
       "901                                     0.122897   \n",
       "902                                     0.686177   \n",
       "...                                          ...   \n",
       "2003996                                 0.735357   \n",
       "2003997                                -0.169819   \n",
       "2003998                                -0.234285   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id10  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                     0.167480   \n",
       "2003997                                     0.357422   \n",
       "2003998                                     0.752930   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id10  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                  0.008577   \n",
       "2003997                                 -0.524434   \n",
       "2003998                                 -0.091471   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id11  \\\n",
       "900                                              NaN   \n",
       "901                                        -0.831447   \n",
       "902                                         1.702803   \n",
       "...                                              ...   \n",
       "2003996                                    -0.376164   \n",
       "2003997                                    -1.120526   \n",
       "2003998                                    -0.457249   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id11  \\\n",
       "900                                           NaN   \n",
       "901                                     -1.179210   \n",
       "902                                      4.453467   \n",
       "...                                           ...   \n",
       "2003996                                 -0.790007   \n",
       "2003997                                 -1.469747   \n",
       "2003998                                 -0.836588   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id12  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                    -0.003368   \n",
       "2003997                                    -0.020822   \n",
       "2003998                                     0.621075   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id12  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                 -0.356717   \n",
       "2003997                                 -0.205887   \n",
       "2003998                                 -0.093853   \n",
       "\n",
       "         log_close/mean_300-mean_close/mean_300_id13  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                     0.474771   \n",
       "2003997                                     0.425495   \n",
       "2003998                                     0.434584   \n",
       "\n",
       "         log_return_300-mean_log_returns_300_id13  mean_close/mean_900  \\\n",
       "900                                           NaN                  NaN   \n",
       "901                                           NaN             0.017286   \n",
       "902                                           NaN            -0.399232   \n",
       "...                                           ...                  ...   \n",
       "2003996                                 -0.198456             0.174812   \n",
       "2003997                                 -0.166922            -0.603025   \n",
       "2003998                                  0.243026             0.028022   \n",
       "\n",
       "         mean_log_returns_900  log_close/mean_900-mean_close/mean_900_id0  \\\n",
       "900                       NaN                                         NaN   \n",
       "901                  1.175482                                    1.293650   \n",
       "902                 -0.146278                                   -1.543607   \n",
       "...                       ...                                         ...   \n",
       "2003996              0.102076                                    0.053212   \n",
       "2003997             -0.031961                                   -0.069386   \n",
       "2003998             -0.033038                                    0.055441   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id0  \\\n",
       "900                                          NaN   \n",
       "901                                     2.155421   \n",
       "902                                    -2.855433   \n",
       "...                                          ...   \n",
       "2003996                                -0.254406   \n",
       "2003997                                -0.117421   \n",
       "2003998                                 0.260966   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id1  \\\n",
       "900                                             NaN   \n",
       "901                                        0.753307   \n",
       "902                                        0.066813   \n",
       "...                                             ...   \n",
       "2003996                                    0.234227   \n",
       "2003997                                    0.973989   \n",
       "2003998                                   -0.392261   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id1  \\\n",
       "900                                          NaN   \n",
       "901                                    -0.117865   \n",
       "902                                    -0.473351   \n",
       "...                                          ...   \n",
       "2003996                                 0.394750   \n",
       "2003997                                 0.363749   \n",
       "2003998                                -0.400800   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id2  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.297075   \n",
       "902                                       -0.224501   \n",
       "...                                             ...   \n",
       "2003996                                   -0.835210   \n",
       "2003997                                   -0.403301   \n",
       "2003998                                   -0.300944   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id2  \\\n",
       "900                                          NaN   \n",
       "901                                    -1.770209   \n",
       "902                                    -1.368371   \n",
       "...                                          ...   \n",
       "2003996                                -0.129802   \n",
       "2003997                                -1.094992   \n",
       "2003998                                -0.785810   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id3  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.064915   \n",
       "2003997                                    0.600989   \n",
       "2003998                                   -0.203995   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id3  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                 0.179292   \n",
       "2003997                                 0.417007   \n",
       "2003998                                -1.325451   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id4  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.209413   \n",
       "2003997                                    0.266060   \n",
       "2003998                                   -0.493536   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id4  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                 0.479444   \n",
       "2003997                                -0.967450   \n",
       "2003998                                -0.765929   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id5  \\\n",
       "900                                             NaN   \n",
       "901                                        0.603700   \n",
       "902                                        1.913083   \n",
       "...                                             ...   \n",
       "2003996                                   -0.104716   \n",
       "2003997                                    0.030636   \n",
       "2003998                                    1.128969   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id5  \\\n",
       "900                                          NaN   \n",
       "901                                    -1.424733   \n",
       "902                                     0.250501   \n",
       "...                                          ...   \n",
       "2003996                                -0.608651   \n",
       "2003997                                -0.074098   \n",
       "2003998                                 0.446938   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id6  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.151005   \n",
       "902                                       -0.487098   \n",
       "...                                             ...   \n",
       "2003996                                    0.325464   \n",
       "2003997                                    0.679915   \n",
       "2003998                                   -0.086424   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id6  \\\n",
       "900                                          NaN   \n",
       "901                                    -1.506588   \n",
       "902                                    -0.647692   \n",
       "...                                          ...   \n",
       "2003996                                -1.388903   \n",
       "2003997                                 0.162279   \n",
       "2003998                                -1.482910   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id7  \\\n",
       "900                                             NaN   \n",
       "901                                       -1.002505   \n",
       "902                                       -1.403457   \n",
       "...                                             ...   \n",
       "2003996                                   -0.150668   \n",
       "2003997                                    0.224564   \n",
       "2003998                                   -0.688368   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id7  \\\n",
       "900                                          NaN   \n",
       "901                                     4.500334   \n",
       "902                                     1.055759   \n",
       "...                                          ...   \n",
       "2003996                                 0.359018   \n",
       "2003997                                 0.286025   \n",
       "2003998                                -0.453511   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id8  \\\n",
       "900                                             NaN   \n",
       "901                                             NaN   \n",
       "902                                             NaN   \n",
       "...                                             ...   \n",
       "2003996                                    0.486160   \n",
       "2003997                                   -1.019002   \n",
       "2003998                                   -0.786596   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id8  \\\n",
       "900                                          NaN   \n",
       "901                                          NaN   \n",
       "902                                          NaN   \n",
       "...                                          ...   \n",
       "2003996                                -0.852314   \n",
       "2003997                                -1.079647   \n",
       "2003998                                 0.096694   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id9  \\\n",
       "900                                             NaN   \n",
       "901                                       -0.428646   \n",
       "902                                       -0.088491   \n",
       "...                                             ...   \n",
       "2003996                                   -0.505124   \n",
       "2003997                                   -0.874982   \n",
       "2003998                                    0.479796   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id9  \\\n",
       "900                                          NaN   \n",
       "901                                    -3.918690   \n",
       "902                                    -0.267847   \n",
       "...                                          ...   \n",
       "2003996                                -1.129628   \n",
       "2003997                                -1.264278   \n",
       "2003998                                 0.635456   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id10  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                     0.223145   \n",
       "2003997                                     0.406738   \n",
       "2003998                                     0.796875   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id10  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                  1.958470   \n",
       "2003997                                  3.217508   \n",
       "2003998                                  1.516436   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id11  \\\n",
       "900                                              NaN   \n",
       "901                                        -0.771426   \n",
       "902                                         1.767257   \n",
       "...                                              ...   \n",
       "2003996                                    -0.431894   \n",
       "2003997                                    -1.178419   \n",
       "2003998                                    -0.517847   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id11  \\\n",
       "900                                           NaN   \n",
       "901                                      2.082330   \n",
       "902                                      4.306434   \n",
       "...                                           ...   \n",
       "2003996                                  0.549779   \n",
       "2003997                                  0.285135   \n",
       "2003998                                 -0.195844   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id12  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                    -0.026414   \n",
       "2003997                                    -0.043210   \n",
       "2003998                                     0.595061   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id12  \\\n",
       "900                                           NaN   \n",
       "901                                           NaN   \n",
       "902                                           NaN   \n",
       "...                                           ...   \n",
       "2003996                                 -0.565852   \n",
       "2003997                                 -0.874567   \n",
       "2003998                                  0.744506   \n",
       "\n",
       "         log_close/mean_900-mean_close/mean_900_id13  \\\n",
       "900                                              NaN   \n",
       "901                                              NaN   \n",
       "902                                              NaN   \n",
       "...                                              ...   \n",
       "2003996                                     0.457456   \n",
       "2003997                                     0.405366   \n",
       "2003998                                     0.413717   \n",
       "\n",
       "         log_return_900-mean_log_returns_900_id13  \n",
       "900                                           NaN  \n",
       "901                                           NaN  \n",
       "902                                           NaN  \n",
       "...                                           ...  \n",
       "2003996                                  1.008801  \n",
       "2003997                                  0.740750  \n",
       "2003998                                  1.709260  \n",
       "\n",
       "[2003099 rows x 231 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:15.778176400Z",
     "start_time": "2023-12-05T20:45:15.751188800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column of features that need to be used for training\n",
    "no_use_columns = [f'Target_{i}' for i in range(14)]\n",
    "no_use_columns.append('timestamp')\n",
    "no_use_columns.extend(FEATURES_EXCLUDE)\n",
    "features = test_df.columns\n",
    "features = features.drop(no_use_columns)\n",
    "features = list(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T20:45:15.790685400Z",
     "start_time": "2023-12-05T20:45:15.767630500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MACD_id0',\n",
       " 'MACD_id1',\n",
       " 'MACD_id2',\n",
       " 'MACD_id3',\n",
       " 'MACD_id4',\n",
       " 'MACD_id5',\n",
       " 'MACD_id6',\n",
       " 'MACD_id7',\n",
       " 'MACD_id8',\n",
       " 'MACD_id9',\n",
       " 'MACD_id10',\n",
       " 'MACD_id11',\n",
       " 'MACD_id12',\n",
       " 'MACD_id13',\n",
       " 'Volume_Ratio_id0',\n",
       " 'Volume_Ratio_id1',\n",
       " 'Volume_Ratio_id2',\n",
       " 'Volume_Ratio_id3',\n",
       " 'Volume_Ratio_id4',\n",
       " 'Volume_Ratio_id5',\n",
       " 'Volume_Ratio_id6',\n",
       " 'Volume_Ratio_id7',\n",
       " 'Volume_Ratio_id8',\n",
       " 'Volume_Ratio_id9',\n",
       " 'Volume_Ratio_id10',\n",
       " 'Volume_Ratio_id11',\n",
       " 'Volume_Ratio_id12',\n",
       " 'Volume_Ratio_id13',\n",
       " 'RSI_id0',\n",
       " 'RSI_id1',\n",
       " 'RSI_id2',\n",
       " 'RSI_id3',\n",
       " 'RSI_id4',\n",
       " 'RSI_id5',\n",
       " 'RSI_id6',\n",
       " 'RSI_id7',\n",
       " 'RSI_id8',\n",
       " 'RSI_id9',\n",
       " 'RSI_id10',\n",
       " 'RSI_id11',\n",
       " 'RSI_id12',\n",
       " 'RSI_id13',\n",
       " 'log_close/mean_60_id0',\n",
       " 'log_return_60_id0',\n",
       " 'log_close/mean_300_id0',\n",
       " 'log_return_300_id0',\n",
       " 'log_close/mean_900_id0',\n",
       " 'log_return_900_id0',\n",
       " 'log_close/mean_60_id1',\n",
       " 'log_return_60_id1',\n",
       " 'log_close/mean_300_id1',\n",
       " 'log_return_300_id1',\n",
       " 'log_close/mean_900_id1',\n",
       " 'log_return_900_id1',\n",
       " 'log_close/mean_60_id2',\n",
       " 'log_return_60_id2',\n",
       " 'log_close/mean_300_id2',\n",
       " 'log_return_300_id2',\n",
       " 'log_close/mean_900_id2',\n",
       " 'log_return_900_id2',\n",
       " 'log_close/mean_60_id3',\n",
       " 'log_return_60_id3',\n",
       " 'log_close/mean_300_id3',\n",
       " 'log_return_300_id3',\n",
       " 'log_close/mean_900_id3',\n",
       " 'log_return_900_id3',\n",
       " 'log_close/mean_60_id4',\n",
       " 'log_return_60_id4',\n",
       " 'log_close/mean_300_id4',\n",
       " 'log_return_300_id4',\n",
       " 'log_close/mean_900_id4',\n",
       " 'log_return_900_id4',\n",
       " 'log_close/mean_60_id5',\n",
       " 'log_return_60_id5',\n",
       " 'log_close/mean_300_id5',\n",
       " 'log_return_300_id5',\n",
       " 'log_close/mean_900_id5',\n",
       " 'log_return_900_id5',\n",
       " 'log_close/mean_60_id6',\n",
       " 'log_return_60_id6',\n",
       " 'log_close/mean_300_id6',\n",
       " 'log_return_300_id6',\n",
       " 'log_close/mean_900_id6',\n",
       " 'log_return_900_id6',\n",
       " 'log_close/mean_60_id7',\n",
       " 'log_return_60_id7',\n",
       " 'log_close/mean_300_id7',\n",
       " 'log_return_300_id7',\n",
       " 'log_close/mean_900_id7',\n",
       " 'log_return_900_id7',\n",
       " 'log_close/mean_60_id8',\n",
       " 'log_return_60_id8',\n",
       " 'log_close/mean_300_id8',\n",
       " 'log_return_300_id8',\n",
       " 'log_close/mean_900_id8',\n",
       " 'log_return_900_id8',\n",
       " 'log_close/mean_60_id9',\n",
       " 'log_return_60_id9',\n",
       " 'log_close/mean_300_id9',\n",
       " 'log_return_300_id9',\n",
       " 'log_close/mean_900_id9',\n",
       " 'log_return_900_id9',\n",
       " 'log_close/mean_60_id10',\n",
       " 'log_return_60_id10',\n",
       " 'log_close/mean_300_id10',\n",
       " 'log_return_300_id10',\n",
       " 'log_close/mean_900_id10',\n",
       " 'log_return_900_id10',\n",
       " 'log_close/mean_60_id11',\n",
       " 'log_return_60_id11',\n",
       " 'log_close/mean_300_id11',\n",
       " 'log_return_300_id11',\n",
       " 'log_close/mean_900_id11',\n",
       " 'log_return_900_id11',\n",
       " 'log_close/mean_60_id12',\n",
       " 'log_return_60_id12',\n",
       " 'log_close/mean_300_id12',\n",
       " 'log_return_300_id12',\n",
       " 'log_close/mean_900_id12',\n",
       " 'log_return_900_id12',\n",
       " 'log_close/mean_60_id13',\n",
       " 'log_return_60_id13',\n",
       " 'log_close/mean_300_id13',\n",
       " 'log_return_300_id13',\n",
       " 'log_close/mean_900_id13',\n",
       " 'log_return_900_id13',\n",
       " 'mean_close/mean_60',\n",
       " 'mean_log_returns_60',\n",
       " 'log_close/mean_60-mean_close/mean_60_id0',\n",
       " 'log_return_60-mean_log_returns_60_id0',\n",
       " 'log_close/mean_60-mean_close/mean_60_id1',\n",
       " 'log_return_60-mean_log_returns_60_id1',\n",
       " 'log_close/mean_60-mean_close/mean_60_id2',\n",
       " 'log_return_60-mean_log_returns_60_id2',\n",
       " 'log_close/mean_60-mean_close/mean_60_id3',\n",
       " 'log_return_60-mean_log_returns_60_id3',\n",
       " 'log_close/mean_60-mean_close/mean_60_id4',\n",
       " 'log_return_60-mean_log_returns_60_id4',\n",
       " 'log_close/mean_60-mean_close/mean_60_id5',\n",
       " 'log_return_60-mean_log_returns_60_id5',\n",
       " 'log_close/mean_60-mean_close/mean_60_id6',\n",
       " 'log_return_60-mean_log_returns_60_id6',\n",
       " 'log_close/mean_60-mean_close/mean_60_id7',\n",
       " 'log_return_60-mean_log_returns_60_id7',\n",
       " 'log_close/mean_60-mean_close/mean_60_id8',\n",
       " 'log_return_60-mean_log_returns_60_id8',\n",
       " 'log_close/mean_60-mean_close/mean_60_id9',\n",
       " 'log_return_60-mean_log_returns_60_id9',\n",
       " 'log_close/mean_60-mean_close/mean_60_id10',\n",
       " 'log_return_60-mean_log_returns_60_id10',\n",
       " 'log_close/mean_60-mean_close/mean_60_id11',\n",
       " 'log_return_60-mean_log_returns_60_id11',\n",
       " 'log_close/mean_60-mean_close/mean_60_id12',\n",
       " 'log_return_60-mean_log_returns_60_id12',\n",
       " 'log_close/mean_60-mean_close/mean_60_id13',\n",
       " 'log_return_60-mean_log_returns_60_id13',\n",
       " 'mean_close/mean_300',\n",
       " 'mean_log_returns_300',\n",
       " 'log_close/mean_300-mean_close/mean_300_id0',\n",
       " 'log_return_300-mean_log_returns_300_id0',\n",
       " 'log_close/mean_300-mean_close/mean_300_id1',\n",
       " 'log_return_300-mean_log_returns_300_id1',\n",
       " 'log_close/mean_300-mean_close/mean_300_id2',\n",
       " 'log_return_300-mean_log_returns_300_id2',\n",
       " 'log_close/mean_300-mean_close/mean_300_id3',\n",
       " 'log_return_300-mean_log_returns_300_id3',\n",
       " 'log_close/mean_300-mean_close/mean_300_id4',\n",
       " 'log_return_300-mean_log_returns_300_id4',\n",
       " 'log_close/mean_300-mean_close/mean_300_id5',\n",
       " 'log_return_300-mean_log_returns_300_id5',\n",
       " 'log_close/mean_300-mean_close/mean_300_id6',\n",
       " 'log_return_300-mean_log_returns_300_id6',\n",
       " 'log_close/mean_300-mean_close/mean_300_id7',\n",
       " 'log_return_300-mean_log_returns_300_id7',\n",
       " 'log_close/mean_300-mean_close/mean_300_id8',\n",
       " 'log_return_300-mean_log_returns_300_id8',\n",
       " 'log_close/mean_300-mean_close/mean_300_id9',\n",
       " 'log_return_300-mean_log_returns_300_id9',\n",
       " 'log_close/mean_300-mean_close/mean_300_id10',\n",
       " 'log_return_300-mean_log_returns_300_id10',\n",
       " 'log_close/mean_300-mean_close/mean_300_id11',\n",
       " 'log_return_300-mean_log_returns_300_id11',\n",
       " 'log_close/mean_300-mean_close/mean_300_id12',\n",
       " 'log_return_300-mean_log_returns_300_id12',\n",
       " 'log_close/mean_300-mean_close/mean_300_id13',\n",
       " 'log_return_300-mean_log_returns_300_id13',\n",
       " 'mean_close/mean_900',\n",
       " 'mean_log_returns_900',\n",
       " 'log_close/mean_900-mean_close/mean_900_id0',\n",
       " 'log_return_900-mean_log_returns_900_id0',\n",
       " 'log_close/mean_900-mean_close/mean_900_id1',\n",
       " 'log_return_900-mean_log_returns_900_id1',\n",
       " 'log_close/mean_900-mean_close/mean_900_id2',\n",
       " 'log_return_900-mean_log_returns_900_id2',\n",
       " 'log_close/mean_900-mean_close/mean_900_id3',\n",
       " 'log_return_900-mean_log_returns_900_id3',\n",
       " 'log_close/mean_900-mean_close/mean_900_id4',\n",
       " 'log_return_900-mean_log_returns_900_id4',\n",
       " 'log_close/mean_900-mean_close/mean_900_id5',\n",
       " 'log_return_900-mean_log_returns_900_id5',\n",
       " 'log_close/mean_900-mean_close/mean_900_id6',\n",
       " 'log_return_900-mean_log_returns_900_id6',\n",
       " 'log_close/mean_900-mean_close/mean_900_id7',\n",
       " 'log_return_900-mean_log_returns_900_id7',\n",
       " 'log_close/mean_900-mean_close/mean_900_id8',\n",
       " 'log_return_900-mean_log_returns_900_id8',\n",
       " 'log_close/mean_900-mean_close/mean_900_id9',\n",
       " 'log_return_900-mean_log_returns_900_id9',\n",
       " 'log_close/mean_900-mean_close/mean_900_id10',\n",
       " 'log_return_900-mean_log_returns_900_id10',\n",
       " 'log_close/mean_900-mean_close/mean_900_id11',\n",
       " 'log_return_900-mean_log_returns_900_id11',\n",
       " 'log_close/mean_900-mean_close/mean_900_id12',\n",
       " 'log_return_900-mean_log_returns_900_id12',\n",
       " 'log_close/mean_900-mean_close/mean_900_id13',\n",
       " 'log_return_900-mean_log_returns_900_id13']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:17:36.747965100Z",
     "start_time": "2023-12-05T21:17:36.719421700Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.317049700Z",
     "start_time": "2023-12-05T20:45:15.798685300Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:10:44.913952Z",
     "iopub.status.busy": "2023-11-25T16:10:44.913497Z",
     "iopub.status.idle": "2023-11-25T16:10:44.920671Z",
     "shell.execute_reply": "2023-11-25T16:10:44.919560Z",
     "shell.execute_reply.started": "2023-11-25T16:10:44.913894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for TRON (ID=13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:45:18,413] A new study created in memory with name: no-name-5e871e03-1da2-48d5-aa2c-6bfebb08987b\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.038309233354679714\n",
      "validation_set_score: 0.019314335945582073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:47:46,752] Trial 0 finished with value: 0.019314335945582073 and parameters: {'learning_rate': 0.17103342039433958, 'max_depth': 16, 'num_leaves': 73, 'lambda_l1': 3.5414462176375663, 'lambda_l2': 2.2156494174066674, 'max_bin': 922, 'min_data_in_leaf': 66}. Best is trial 0 with value: 0.019314335945582073.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04665526283827228\n",
      "validation_set_score: 0.021453635491829562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:49:47,010] Trial 1 finished with value: 0.021453635491829562 and parameters: {'learning_rate': 0.17147517451217056, 'max_depth': 16, 'num_leaves': 85, 'lambda_l1': 3.2867247313606596, 'lambda_l2': 2.479134155334773, 'max_bin': 1080, 'min_data_in_leaf': 80}. Best is trial 1 with value: 0.021453635491829562.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04375855131325042\n",
      "validation_set_score: 0.02182920394652923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:51:31,061] Trial 2 finished with value: 0.02182920394652923 and parameters: {'learning_rate': 0.18775525060281287, 'max_depth': 16, 'num_leaves': 72, 'lambda_l1': 3.595672965324523, 'lambda_l2': 2.6280997468379885, 'max_bin': 975, 'min_data_in_leaf': 59}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04320966811757967\n",
      "validation_set_score: 0.020376455539917147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:53:10,059] Trial 3 finished with value: 0.020376455539917147 and parameters: {'learning_rate': 0.17835832755500075, 'max_depth': 16, 'num_leaves': 76, 'lambda_l1': 3.4148987422722645, 'lambda_l2': 2.767095059777164, 'max_bin': 1078, 'min_data_in_leaf': 59}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.03972018509311678\n",
      "validation_set_score: 0.020533713129957255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:54:56,865] Trial 4 finished with value: 0.020533713129957255 and parameters: {'learning_rate': 0.19207367836712985, 'max_depth': 15, 'num_leaves': 60, 'lambda_l1': 3.6916167210647672, 'lambda_l2': 2.421891547613181, 'max_bin': 1021, 'min_data_in_leaf': 73}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04343960808440715\n",
      "validation_set_score: 0.02101909956096167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:56:38,770] Trial 5 finished with value: 0.02101909956096167 and parameters: {'learning_rate': 0.19389570284294644, 'max_depth': 18, 'num_leaves': 84, 'lambda_l1': 3.583688824096252, 'lambda_l2': 2.5622336246471287, 'max_bin': 942, 'min_data_in_leaf': 77}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.033585876589426536\n",
      "validation_set_score: 0.020431492410442936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 21:58:26,815] Trial 6 finished with value: 0.020431492410442936 and parameters: {'learning_rate': 0.1705694482841451, 'max_depth': 14, 'num_leaves': 84, 'lambda_l1': 3.7596953307221748, 'lambda_l2': 2.6673484606744955, 'max_bin': 957, 'min_data_in_leaf': 75}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.07170671305642118\n",
      "validation_set_score: 0.020091966521703863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 22:01:13,020] Trial 7 finished with value: 0.020091966521703863 and parameters: {'learning_rate': 0.152366395949454, 'max_depth': 17, 'num_leaves': 76, 'lambda_l1': 3.22968915537249, 'lambda_l2': 2.5047901896334626, 'max_bin': 1094, 'min_data_in_leaf': 79}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04594944291478268\n",
      "validation_set_score: 0.02021948698288218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 22:03:05,318] Trial 8 finished with value: 0.02021948698288218 and parameters: {'learning_rate': 0.16447226805141438, 'max_depth': 15, 'num_leaves': 77, 'lambda_l1': 3.273927589671661, 'lambda_l2': 2.2425971527355486, 'max_bin': 991, 'min_data_in_leaf': 62}. Best is trial 2 with value: 0.02182920394652923.\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  local_params['learning_rate'] = trial.suggest_loguniform('learning_rate', *param_ranges['learning_rate'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l1'] = trial.suggest_uniform('lambda_l1', *param_ranges['lambda_l1'])\n",
      "C:\\Users\\严厉的父亲\\AppData\\Local\\Temp\\ipykernel_21704\\849264096.py:15: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  local_params['lambda_l2'] = trial.suggest_uniform('lambda_l2', *param_ranges['lambda_l2'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_training_id:13\n",
      "current_Bayesian_trial:1 out of 10\n",
      "number of train data: 1520409\n",
      "number of val data:   380102\n",
      "training_set_score: 0.04459255599912468\n",
      "validation_set_score: 0.02130638994225777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 22:04:42,220] Trial 9 finished with value: 0.02130638994225777 and parameters: {'learning_rate': 0.1923161674780338, 'max_depth': 18, 'num_leaves': 77, 'lambda_l1': 3.3213622863277736, 'lambda_l2': 2.512969583894226, 'max_bin': 1081, 'min_data_in_leaf': 65}. Best is trial 2 with value: 0.02182920394652923.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "Value:  0.02182920394652923\n",
      "Params: \n",
      "    learning_rate: 0.18775525060281287\n",
      "    max_depth: 16\n",
      "    num_leaves: 72\n",
      "    lambda_l1: 3.595672965324523\n",
      "    lambda_l2: 2.6280997468379885\n",
      "    max_bin: 975\n",
      "    min_data_in_leaf: 59\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# CreateFolderForEachTraining(creator)\n",
    "df_asset_details = pd.read_csv(ASSET_DETAILS_CSV)\n",
    "\n",
    "# Start training\n",
    "for asset_id in ASSET_ID_SELECTED_FOR_TRAIN:\n",
    "    asset_name = df_asset_details.loc[df_asset_details['Asset_ID'] == asset_id, 'Asset_Name'].values[0]\n",
    "    print(f\"Training model for {asset_name} (ID={asset_id:<2})\")\n",
    "    folder_name = formatted_time\n",
    "    test_df = test_df.loc[(test_df[f'Target_{asset_id}'] == test_df[f'Target_{asset_id}'])]\n",
    "    # The Bayesian optimiser is applied here\n",
    "    trial_num = 1\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "    # Print the result of optimization\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.678718800Z",
     "start_time": "2023-12-05T21:04:42.291851800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.515465100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:10:44.923399Z",
     "iopub.status.busy": "2023-11-25T16:10:44.922170Z",
     "iopub.status.idle": "2023-11-25T16:10:45.229242Z",
     "shell.execute_reply": "2023-11-25T16:10:45.227877Z",
     "shell.execute_reply.started": "2023-11-25T16:10:44.923355Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.515465100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:10:45.231809Z",
     "iopub.status.busy": "2023-11-25T16:10:45.231408Z",
     "iopub.status.idle": "2023-11-25T16:11:02.260181Z",
     "shell.execute_reply": "2023-11-25T16:11:02.258634Z",
     "shell.execute_reply.started": "2023-11-25T16:10:45.231780Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.515465100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:11:02.262633Z",
     "iopub.status.busy": "2023-11-25T16:11:02.262001Z",
     "iopub.status.idle": "2023-11-25T16:11:02.560731Z",
     "shell.execute_reply": "2023-11-25T16:11:02.559206Z",
     "shell.execute_reply.started": "2023-11-25T16:11:02.262588Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.515465100Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T16:11:02.563217Z",
     "iopub.status.busy": "2023-11-25T16:11:02.562455Z",
     "iopub.status.idle": "2023-11-25T16:11:02.571456Z",
     "shell.execute_reply": "2023-11-25T16:11:02.569984Z",
     "shell.execute_reply.started": "2023-11-25T16:11:02.563179Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.519465600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.519465600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.698710600Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T17:07:13.934200Z",
     "iopub.status.busy": "2023-11-25T17:07:13.933741Z",
     "iopub.status.idle": "2023-11-25T17:07:51.411856Z",
     "shell.execute_reply": "2023-11-25T17:07:51.409042Z",
     "shell.execute_reply.started": "2023-11-25T17:07:13.934165Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.200510Z",
     "iopub.status.idle": "2023-11-25T16:11:36.201522Z",
     "shell.execute_reply": "2023-11-25T16:11:36.201143Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.201078Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.203272Z",
     "iopub.status.idle": "2023-11-25T16:11:36.204052Z",
     "shell.execute_reply": "2023-11-25T16:11:36.203699Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.203664Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.206341Z",
     "iopub.status.idle": "2023-11-25T16:11:36.206988Z",
     "shell.execute_reply": "2023-11-25T16:11:36.206719Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.206686Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.208670Z",
     "iopub.status.idle": "2023-11-25T16:11:36.209339Z",
     "shell.execute_reply": "2023-11-25T16:11:36.209061Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.209030Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.523465300Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.211750Z",
     "iopub.status.idle": "2023-11-25T16:11:36.212317Z",
     "shell.execute_reply": "2023-11-25T16:11:36.212082Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.212059Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.527466700Z"
    },
    "execution": {
     "iopub.status.busy": "2023-11-25T16:11:36.213427Z",
     "iopub.status.idle": "2023-11-25T16:11:36.213883Z",
     "shell.execute_reply": "2023-11-25T16:11:36.213692Z",
     "shell.execute_reply.started": "2023-11-25T16:11:36.213662Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-05T21:04:42.714710800Z",
     "start_time": "2023-12-05T21:04:42.527466700Z"
    },
    "execution": {
     "iopub.execute_input": "2023-11-25T17:06:58.862710Z",
     "iopub.status.busy": "2023-11-25T17:06:58.862210Z",
     "iopub.status.idle": "2023-11-25T17:06:58.875384Z",
     "shell.execute_reply": "2023-11-25T17:06:58.873718Z",
     "shell.execute_reply.started": "2023-11-25T17:06:58.862671Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d490669-15d3-4c35-bad0-ed2652d11f01",
    "_uuid": "2e107c87-fe93-4837-bc49-80ae3c4f2fc2"
   },
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4043536,
     "sourceId": 7030037,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4043626,
     "sourceId": 7030154,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4045593,
     "sourceId": 7033033,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
